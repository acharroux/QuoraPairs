{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Playing with stop words\n",
    "\n",
    "Nb common words seems to be an interesting feature.\n",
    "But isn'it disturbed by common words like do, not, and which may precisely be common to many pairs this without any significance ?\n",
    "So if remove stop words, nb common feature may have more signficance "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ugly incantation to make our 'framework' working\n",
    "import sys\n",
    "sys.path.insert(0, r'/SAPDevelop/QuoraPairs/BruteForce/Tools')\n",
    "\n",
    "#import all our small tools (paths, cache, print,zip,excel, pandas, progress,..)\n",
    "from Tools.all import *\n",
    "\n",
    "# setup the name of our experiment\n",
    "# it will be used to store every result in a unique place\n",
    "EXPERIMENT='more_features_exploration'\n",
    "# Do a bit of checks before actually running code\n",
    "UNITARY_TEST = True\n",
    "print_alert('You will use environment %s' % EXPERIMENT)"
   ]
  },
  {
   "source": [
    "## Build our simple features\n",
    "* Nb words in question 1\n",
    "* Nb words in question 2\n",
    "* **abs(Nb words in question 1 - Nb words in question 2)** suppressed\n",
    "* Nb common words\n",
    "* Nb common words/nb words in question 1\n",
    "* Nb common words/nb words in question 2\n",
    "* Nb non common words in question 1\n",
    "* Nb non common words in question 2\n",
    "* Nb common words/(Nb words in question1 + Nb words in question2)\n",
    "\n",
    "**Notes**\n",
    "* Input data is not processed except for basic unicode clean\n",
    "* Heavy to generate so result is cached\n",
    "* This may be overkill as features will be eliminated but features are also generated for challenge data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_from_columns(dataframe,output_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe.progress_apply(function,axis=1)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "def add_column_from_column(dataframe,output_column_name,input_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe[input_column_name].progress_apply(function)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "def build_all_simple_features(dataframe):\n",
    "    print_warning('Compute common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nb_words_question1','question1',lambda x: len(x.split()))\n",
    "    add_column_from_column(dataframe,'nb_words_question2','question2',lambda x: len(x.split()))\n",
    "\n",
    "    #print_warning('Compute abs(Nb words in question 1 - Nb words in question 2)')\n",
    "    #add_column_from_columns(dataframe,'abs(nb1-nb2)',lambda r: abs(r.nb_words_question1-r.nb_words_question2))\n",
    "\n",
    "    print_warning('Compute Nb common_words between question1 & question2')\n",
    "    add_column_from_columns(dataframe,'common_words',lambda r: list(set(r.question1.split())&set(r.question2.split())))\n",
    "    add_column_from_column(dataframe,'nb_common_words','common_words',len)\n",
    "\n",
    "    print_warning('Compute Nb common words/nb words in question1')\n",
    "    add_column_from_columns(dataframe,'nb_common_words/nb_words_question1',lambda r: r.nb_common_words/max(1,r.nb_words_question1))\n",
    "    print_warning('Compute Nb common words/nb words in question2')\n",
    "    add_column_from_columns(dataframe,'nb_common_words/nb_words_question2',lambda r: r.nb_common_words/max(1,r.nb_words_question2))\n",
    "\n",
    "    print_warning('Compute Nb words in question1,question2 not in common words')\n",
    "    add_column_from_columns(dataframe,'nb_words_question1-common_words',lambda r: len(list(set(r.question1.split())-set(r.common_words))))\n",
    "    add_column_from_columns(dataframe,'nb_words_question2-common_words',lambda r: len(list(set(r.question2.split())-set(r.common_words))))\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_columns(dataframe,'nb_common_words/(nb_words_question1+nb_words_question2)',lambda r: r.nb_common_words/(r.nb_words_question1+r.nb_words_question2))\n",
    "    dataframe=dataframe.drop(columns='common_words')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepare_environnement(EXPERIMENT)\n",
    "train_dataframe=load_dataframe(CLEAN_TRAINING_DATA)\n",
    "challenge_dataframe=load_dataframe(CLEAN_CHALLENGE_DATA)\n",
    "print_section('Untouched input data has been loaded. Training: %d lines Challenge: %d lines' % (len(train_dataframe),len(challenge_dataframe)))\n",
    "\n",
    "train_dataframe=load_or_build_dataframe('Training data + basic features','training_basic_features',build_all_simple_features,train_dataframe)\n",
    "challenge_dataframe=load_or_build_dataframe('Challenge data + basic features','challenge_basic_features',build_all_simple_features,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "## Challenge and training are not equivalent\n",
    "\n",
    "% of duplicate in training is not the same in challenge !\n",
    "We don't have the challenge's answer but by scoring a constant prediction and using logloss definition, we can guess the distribution of duplicates in challenge\n",
    "\n",
    "It's 17.46 %\n",
    "\n",
    "To fix that we can:\n",
    "\n",
    "* duplicate some negative cases in training to match challenge distribution\n",
    "* use weights : each case has a weight !=1 and all algorithms are supposed to be able to deal with that info\n",
    "I choose to use weights (less memory needed ?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the weight for 0 and weight for 1 needed to rebalance dataframe like challenge\n",
    "def balanced_weights(dataframe,expected_positive_ratio):\n",
    "    current_positive_ratio = dataframe['is_duplicate'].sum()/len(dataframe)\n",
    "    weight_for_negative = (1-expected_positive_ratio)/(1-current_positive_ratio)\n",
    "    weight_for_positive = expected_positive_ratio/current_positive_ratio\n",
    "    return weight_for_positive,weight_for_negative\n",
    "    return 1,\n",
    "\n",
    "\n",
    "CHALLENGE_DUPLICATE_PERCENT = 0.1746\n",
    "print_warning('OUPS !! %% of duplicates in train is %.3f. In challenge it is %.3f %%' % (train_dataframe['is_duplicate'].sum()/len(train_dataframe),CHALLENGE_DUPLICATE_PERCENT))\n",
    "\n",
    "# create a new 'weight' column to training dataset\n",
    "# Do not forget to remove this column from features !!!\n",
    "weight_for_1,weight_for_0 = balanced_weights(train_dataframe,CHALLENGE_DUPLICATE_PERCENT)\n",
    "print_info('Weight for positive case %.3f' % weight_for_1)\n",
    "print_info('Weight for negative case %.3f' % weight_for_0)\n",
    "train_dataframe['weight'] = train_dataframe['is_duplicate'].map( {0:weight_for_0, 1:weight_for_1})\n",
    "\n",
    "assert int(train_dataframe['weight'].sum()/len(train_dataframe)) == 1, \"training dataset has not been properly rebalanced\"\n",
    "print_info(\"Training data set has been properly rebalanced\")\n",
    "print_info('Weights distribution:')\n",
    "train_dataframe['weight'].describe()"
   ]
  },
  {
   "source": [
    "## Compute AUC of these basic features and try to figure if there is a bit of information inside each one\n",
    "\n",
    "ie is it helping to separate 1 from 0?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(y_true=dataframe['is_duplicate'],y_score=dataframe[column_name])\n",
    "\n",
    "def simple_weighted_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(y_true=dataframe['is_duplicate'],y_score=dataframe[column_name],sample_weight=dataframe['weight'])\n",
    "\n",
    "def show_AUC(dataframe,column_name):\n",
    "    print_bullet('AUC %s: %f' % (column_name,simple_AUC(dataframe,column_name)))\n",
    "    if 'weight' in dataframe.columns:\n",
    "        print_bullet('Weighted AUC %f' % simple_weighted_AUC(dataframe,column_name))\n",
    "    else:\n",
    "        print_bullet('AUC %f' % simple_AUC(dataframe,column_name))\n",
    "        \n",
    "\n",
    "def display_simple_AUC(dataframe,column_name):\n",
    "    palette = sns.color_palette()\n",
    "    # Let multiplot_generator figure the size\n",
    "    #plot.figure(figsize=(10, 7))\n",
    "    plot.hist(dataframe[column_name][dataframe['is_duplicate']==1],bins=50,color=palette[3],label='Same',histtype='step')\n",
    "    plot.hist(train_dataframe[column_name][dataframe['is_duplicate']==0],bins=50,color=palette[2],label='Different',alpha = 0.75,histtype='step')\n",
    "    plot.title('AUC %s : %f' % (column_name,simple_AUC(dataframe,column_name)) , fontsize=10)\n",
    "    plot.xlabel(column_name)\n",
    "    plot.ylabel('Nb')\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "\n",
    "def show_all_simple_AUC(dataframe):\n",
    "    all =  all_numeric_columns(dataframe)\n",
    "    print_section( 'Show AUC on %d unique features' % len(all))\n",
    "    for name in all:\n",
    "        show_AUC(dataframe,name)\n",
    "        yield\n",
    "        display_simple_AUC(dataframe,name)\n",
    "    print_done('Done')\n",
    "\n",
    "\n",
    "def show_all_simple_AUC_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(show_all_simple_AUC(dataframe), nb_columns)\n",
    "\n",
    "show_all_simple_AUC_in_grid(train_dataframe,nb_columns=2)"
   ]
  },
  {
   "source": [
    "## OK Visualising these AUC is cool\n",
    "Can we have some numbers ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def graph_relative_feature_importance(features_importances,columns):\n",
    "    # Make importances relative to max importance.\n",
    "    features_importances = 100.0 * (features_importances / features_importances.max())\n",
    "    sorted_idx = numpy.argsort(features_importances)\n",
    "    sorted_idx = sorted_idx[-20:-1:1]\n",
    "    pos = numpy.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plot.barh(pos, features_importances[sorted_idx], align='center')\n",
    "    plot.yticks(pos, columns)\n",
    "    plot.xlabel('Relative Importance')\n",
    "    plot.title('Relative Feature Importance', fontsize=30)\n",
    "    plot.tick_params(axis='x', which='major', labelsize=15)\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    plot.show()\n",
    "\n",
    "def graph_feature_importance(features_importances,columns):\n",
    "    # Make importances relative to max importance.\n",
    "    sorted_idx = numpy.argsort(features_importances)\n",
    "    sorted_idx = sorted_idx[-20:]\n",
    "    pos = numpy.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plot.barh(pos, features_importances[sorted_idx], align='center')\n",
    "    plot.yticks(pos, columns)\n",
    "    plot.xlabel('Importance')\n",
    "    plot.title('Feature Importance', fontsize=30)\n",
    "    plot.tick_params(axis='x', which='major', labelsize=15)\n",
    "    #sns.despine(left=True, bottom=True)\n",
    "    plot.show()\n",
    "\n",
    "def independant_features_analysis(name_data,input_df,target_df):\n",
    "    train_df = input_df[all_numeric_columns(input_df)]\n",
    "    target_df = target_df\n",
    "    small = int(len(target_df)*0.05)\n",
    "    small_train = train_df.sample(small,random_state=42)\n",
    "    small_target = target_df.sample(small,random_state=42)\n",
    "    small_weights = input_df[WEIGHT].sample(small,random_state=42)\n",
    "\n",
    "    # be careful all_numeric will include also the target soon\n",
    "    all_numeric = all_numeric_columns(input_df)\n",
    "    print_section('Minimal analysis of numeric features of %s' % name_data)\n",
    "    print_info('Nb features: %d' % len(all_numeric))\n",
    "    print_info(str(all_numeric))\n",
    "    print_info('Is there any null value ? %s' % input_df.isnull().any().any())\n",
    "\n",
    "    compute_variances = feature_selection.VarianceThreshold()\n",
    "    all_numeric.append('is_duplicate')\n",
    "    variances = compute_variances.fit_transform(input_df[all_numeric])\n",
    "    print_info('Is there any low variance feature ? %s' % str(variances.shape[1]!=len(all_numeric)))\n",
    "    print_info('Here are the correlations to the target \"is_duplicate\"')\n",
    "    correlation_matrix = input_df[all_numeric].corr().abs()\n",
    "    print(correlation_matrix['is_duplicate'].nlargest(len(all_numeric)))\n",
    "\n",
    "    correlation_matrix_without_is_duplicate = correlation_matrix['is_duplicate'][correlation_matrix['is_duplicate']<1]\n",
    "    highest_correlated_feature = correlation_matrix_without_is_duplicate.nlargest(1).index[0]\n",
    "    highest_correlation = correlation_matrix_without_is_duplicate.nlargest(1)[0]\n",
    "    if highest_correlation >0.2:\n",
    "        if highest_correlation < 0.5:\n",
    "            print_warning('%s is the most correlated but %f is quite weak' % (highest_correlated_feature,highest_correlation))\n",
    "        else:\n",
    "            print_info('%s is the most correlated but %f is very weak' % (highest_correlated_feature,highest_correlation))\n",
    "    else:\n",
    "        print_alert('%s is the most correlated and %f is quite big' % (highest_correlated_feature,highest_correlation))\n",
    "    plot.figure(figsize=(15, 10))\n",
    "    sns.heatmap(correlation_matrix,annot=True,cbar=True,square=True,cmap='coolwarm',mask = numpy.triu(correlation_matrix))\n",
    "    plot.show()\n",
    "\n",
    "    # we remove is_duplicate\n",
    "    all_numeric = all_numeric_columns(input_df)\n",
    "\n",
    "    print_section('Here are the features that would be selected with a simple univariate analysis')\n",
    "    start = time.time()\n",
    "    for r in tqdm(range(1,len(all_numeric))):\n",
    "        k_best = SelectKBest(score_func=f_classif,k=r)\n",
    "        fit = k_best.fit(small_train,small_target)\n",
    "        best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "        print_info('Nb features to keep %d:%s' %(r,best_columns))\n",
    "        \n",
    "        # Can we graph relative importances\n",
    "        if r>1:\n",
    "            # transform train set so we can compute importances\n",
    "            univariate_features = fit.transform(small_train)\n",
    "            rfc = RandomForestClassifier(n_estimators=100)\n",
    "            rfc_scores = cross_val_score(rfc, univariate_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "            features_importance = rfc.fit(univariate_features, small_target).feature_importances_\n",
    "            graph_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "    \n",
    "    print_section('Here are the features that would be selected a priori from default Multinomial Naive Bayes')\n",
    "    start = time.time()   \n",
    "    mnb = MultinomialNB()\n",
    "    best_from_model = feature_selection.SelectFromModel(mnb)\n",
    "    fit = best_from_model.fit(small_train,small_target,sample_weight=small_weights)\n",
    "    print(fit.get_support(indices=True))\n",
    "    best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "    # transform train set so we can compute importances\n",
    "    mnb_features = fit.transform(small_train)\n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc_scores = cross_val_score(rfc, mnb_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "    features_importance = rfc.fit(mnb_features, small_target).feature_importances_\n",
    "    print(features_importance)\n",
    "    print_info('features to keep :%s logloss %.4f %.4f' %(best_columns,-rfc_scores.mean(),rfc_scores.std()))\n",
    "    graph_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "    \n",
    "\n",
    "    print_info('Try to keep x \\% of variance with a PCA')\n",
    "    print_alert('bug ?')\n",
    "    start = time.time()\n",
    "    for r in tqdm([.8,.9,.95]):\n",
    "        acp = PCA(r)\n",
    "        principal_components = acp.fit_transform(small_train)\n",
    "        principal_df = pandas.DataFrame(principal_components).sample(small,random_state=42)\n",
    "        rfc = RandomForestClassifier(n_estimators=100)\n",
    "        rfc_scores = cross_val_score(rfc, principal_df, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "        features_importance = rfc.fit(principal_df, small_target).feature_importances_\n",
    "        print_info('%% of variance %f:%s' %(r,'aie'))\n",
    "        graph_feature_importance(features_importance,small_train.columns)\n",
    "    print_done('Done',top=start)\n",
    "\n",
    "    print_section('Here are the features that would be selected with a recursive feature elimination')\n",
    "    start = time.time()\n",
    "    print_alert('Doomed to fail : RFE does not support to transmit ,sample_weight=small_weights')\n",
    "    print_alert('ix is waiting since 2016 ?')\n",
    "    for r in tqdm(range(1,len(all_numeric))):\n",
    "        mnb = MultinomialNB()\n",
    "        recursive_best= feature_selection.RFE(mnb, n_features_to_select=r)\n",
    "        fit = recursive_best.fit(small_train,small_target)\n",
    "        best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "        print_info('Nb features to keep %d:%s' %(r,best_columns))\n",
    "        \n",
    "        # Can we graph relative importances\n",
    "        if r>1:\n",
    "            # transform train set so we can compute importances\n",
    "            recursive_features = fit.transform(small_train)\n",
    "            rfc = RandomForestClassifier(n_estimators=100)\n",
    "            rfc_scores = cross_val_score(rfc, recursive_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "            features_importance = rfc.fit(recursive_features, small_target).feature_importances_\n",
    "            graph_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "   \n",
    "\n",
    "\n",
    "independant_features_analysis('train',train_dataframe,train_dataframe['is_duplicate'])\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Here are the specs of algorithms to try"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is our algorithm specification\n",
    "import xgboost \n",
    "\n",
    "def learner_xgboost(algo_spec,input_train,target_train,input_test,target_test,weigth_train,weight_test,show=True):\n",
    "    train_dm = xgboost.DMatrix(input_train, label = target_train, weight = weigth_train)\n",
    "    if target_test is None:\n",
    "        watchlist = [(train_dm, 'train')]\n",
    "    else:\n",
    "        validation_dm = xgboost.DMatrix(input_test, label = target_test,weight = weight_test)\n",
    "        watchlist = [(train_dm, 'train'), (validation_dm, 'valid')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(algo_spec['params'], train_dm, 400, watchlist,early_stopping_rounds=50, verbose_eval=verbose)\n",
    "    duration = time.time()-start\n",
    "    return model\n",
    "\n",
    "\n",
    "def predictor_xgboost(model,input_df,target_df):\n",
    "    final_input = xgboost.DMatrix(input_df)\n",
    "    prediction_proba_df = model.predict(final_input,ntree_limit=model.best_ntree_limit)\n",
    "    prediction_df = numpy.where(prediction_proba_df>0.5,1,0)\n",
    "    return prediction_df,prediction_proba_df\n",
    "\n",
    "\n",
    "\n",
    "HYPER_PARAMETERS_MULTINOMIAL_NAIVE_BAYES_FIRST_TRY = {\n",
    "    'alpha':[1e-10,1e-08,1e-06,1e-04,1e-02,1e-01,1.0]\n",
    "    }\n",
    "\n",
    "HYPER_PARAMETERS_MULTINOMIAL_NAIVE_BAYES_FINE_TUNING = {\n",
    "    'alpha':[0.8e-10,0.9e-10,1.0e-10,1.1e-10,1.2e-10]\n",
    "    }\n",
    "\n",
    "ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH = {\n",
    "    'algorithm': MultinomialNB,\n",
    "    'searcher': RandomizedSearchCV,\n",
    "    'hyper_parameters': {\n",
    "        'alpha':[1e-10,1e-08,1e-06,1e-04,1e-02,1e-01,1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Not very efficient ....\n",
    "ALGORITHM_SPEC_COMPLEMENT_NB = {\n",
    "    'algorithm': ComplementNB\n",
    "}\n",
    "\n",
    "ALGORITHM_SPEC_XGBOOST = {\n",
    "    'learner': learner_xgboost,\n",
    "    'predicter': predictor_xgboost,\n",
    "    'params': {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'eta' : 0.02,\n",
    "        'max_depth':3 \n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test: one default Multinomial Naive Bayes model (80,20) & (100,0) using feature nb_words_question1')\n",
    "    res = build_model_80_20_and_100_0(ALGORITHM_SPEC_COMPLEMENT_NB,train_dataframe,['nb_common_words/nb_words_question1', 'nb_words_question1-common_words', 'nb_words_question2-common_words', 'nb_common_words/(nb_words_question1+nb_words_question2)'],train_dataframe['is_duplicate'])\n",
    "    #print(res)\n",
    "\n",
    "# Unitary test\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : optimize Multinomial Naive Bayes model with hyper_parameters (80,20) & (100,0) using feature nb_words_question1')\n",
    "    res = build_model_80_20_and_100_0_hyper(ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH,train_dataframe,['nb_common_words/nb_words_question1', 'nb_words_question1-common_words', 'nb_words_question2-common_words', 'nb_common_words/(nb_words_question1+nb_words_question2)'],train_dataframe['is_duplicate'])\n",
    "    #print(res)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : explore all features with default Multinomial Naive Bayes model on (80,20) & (100,0)')\n",
    "    small_train = train_dataframe.sample(100, random_state=42)\n",
    "    res = build_default_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH,small_train,small_train['is_duplicate'])\n",
    "    #print(res)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : explore all features with default Multinomial Naive Bayes model on  (80,20) & (100,0) with cross validation')\n",
    "    small_train = train_dataframe.sample(100,random_state=42)\n",
    "    res = build_cross_validation_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH,small_train,small_train['is_duplicate'])\n",
    "    #print(res)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : explore all features with Multinomial Naive Bayes model & hyper parameters on (80,20) & (100,0) with hyper parameters')\n",
    "    small_train = train_dataframe.sample(100,random_state=42)\n",
    "    res = build_hyper_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH,small_train,small_train['is_duplicate'])\n",
    "    #print(res)\n",
    "\n",
    "# XGBoost \n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : explore all features with default xgboost model on (80,20) & (100,0)')\n",
    "    small_train = train_dataframe.sample(100,random_state=42)\n",
    "    res = build_default_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_XGBOOST,small_train,small_train['is_duplicate'])\n",
    "    #print(res)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : explore all features with xgboost model on  (80,20) & (100,0) with cross validation')\n",
    "    small_train = train_dataframe.sample(100,random_state=42)\n",
    "    res = build_cross_validation_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_XGBOOST,small_train,small_train['is_duplicate'])\n",
    "    #print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.describe()"
   ]
  },
  {
   "source": [
    "## Everything ready for exploration\n",
    "\n",
    "Algorithms available now are:\n",
    "* Multinomial Naive Bayes : good results facing its simplicity\n",
    "* Complement Naive Bayes (poor results)\n",
    "\n",
    "Strategies available are\n",
    "* explore all combinations with an algorithm using fixed parameters\n",
    "* explore all combinations with an algorithm using fixed parameters and with cross validation\n",
    "* explore all combinations with an algorithm with hyper parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HYPER_PARAMETERS_MULTINOMIAL_NAIVE_BAYES_FIRST_TRY = [1e-10,1e-08,1e-06,1e-04,1e-02,1e-01,1.0]\n",
    "\n",
    "HYPER_PARAMETERS_MULTINOMIAL_NAIVE_BAYES_FINE_TUNING = [0.9e-10,0.95e-10,1.0e-10,1.05e-10,1.1e-10]\n",
    "\n",
    "ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH = {\n",
    "    'algorithm': MultinomialNB,\n",
    "    'searcher': RandomizedSearchCV,\n",
    "    'hyper_parameters': {\n",
    "        'alpha':HYPER_PARAMETERS_MULTINOMIAL_NAIVE_BAYES_FINE_TUNING\n",
    "    }\n",
    "}\n",
    "\n",
    "# Explore by building default models\n",
    "default_multinomial_results = build_default_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH,train_dataframe,train_dataframe['is_duplicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_validation_multinomial_results = build_cross_validation_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH,train_dataframe,train_dataframe['is_duplicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyper_multinomial_results = build_hyper_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_MULTINOMIAL_NB_RANDOMIZED_SEARCH,train_dataframe,train_dataframe['is_duplicate'])"
   ]
  },
  {
   "source": [
    "## Explore a little bit our database"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_model(results,model_key,kind):\n",
    "    model = results['model_'+kind][model_key]\n",
    "    column_names = results['columns'][model_key]\n",
    "    return model,numpy.asarray(column_names)\n",
    "    \n",
    "def find_best_models(results,top,criteria,kind):\n",
    "    if 'logloss' in criteria:\n",
    "        return results.nsmallest(top,criteria+'_'+kind)\n",
    "    else:\n",
    "        return results.nlargest(top,criteria+'_'+kind)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    model,columns = retrieve_model(default_multinomial_results,'nb_common_words/nb_words_question1+nb_words_question1-common_words+nb_words_question2-common_words+nb_common_words/(nb_words_question1+nb_words_question2)','100_0')\n",
    "    assert model is not None and len(columns)==4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('default Multinomial Naive Bayes')\n",
    "display(default_multinomial_results.head())\n",
    "print_section('Cross Validation Multinomial Naive Bayes')\n",
    "display(cross_validation_multinomial_results.head())\n",
    "print_section('Hyper parameters Multinomial Naive Bayes')\n",
    "display(hyper_multinomial_results.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#find_best_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('3 Best defaults')\n",
    "display(find_best_models(default_multinomial_results,3,'logloss_proba','80_20'))\n",
    "print_section('3 Best cross validation')\n",
    "display(find_best_models(cross_validation_multinomial_results,3,'logloss_proba','min'))\n",
    "print_section('3 Best hyper parameters')\n",
    "display(find_best_models(hyper_multinomial_results,3,'logloss_proba','80_20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_dict_to_excel(default_multinomial_results,file_name='default')\n",
    "save_models_dict_to_excel(default_multinomial_results,file_name='cross')\n",
    "save_models_dict_to_excel(default_multinomial_results,file_name='hyper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_docker_cp_command(absolute_file_name):\n",
    "    return 'docker cp '+ DOCKER_IMAGE_NAME+':'+zip_file_name(absolute_file_name)+ ' c:\\\\temp\\\\outputs'\n",
    "\n",
    "def show_kaggle_command(absolute_file_name):\n",
    "    return 'kaggle competitions submit quora-question-pairs -f \"' + zip_file_name(absolute_file_name) +'\" -m \"' + absolute_file_name +'\"'\n",
    "\n",
    "def show_docker_cp_commands(best_results):\n",
    "    print_section('Use these commands to transfer apply results to windows host')\n",
    "    for c in best_results['file_name'].apply(show_docker_cp_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "\n",
    "def show_kaggle_commands(best_results):\n",
    "    print_section('Use these commands to submit apply results to kaggle')\n",
    "    for c in best_results['file_name'].apply(show_kaggle_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "    \n",
    "# return a dataframe fully ready to be converted in csv and published into kaggle\n",
    "def simple_apply(results,model_key,input_dataframe,kind,proba=True):\n",
    "    model,column_names=retrieve_model(results,model_key,kind)\n",
    "    input_for_prediction=input_dataframe[column_names]\n",
    "    res = pandas.DataFrame()\n",
    "    if 'test_id' in input_dataframe.columns:\n",
    "        res['test_id']=input_dataframe['test_id']\n",
    "    if proba:\n",
    "        res['is_duplicate']=pandas.Series(model.predict_proba(input_for_prediction)[:,1],name='is_duplicate')\n",
    "    else:\n",
    "        res['is_duplicate']=pandas.Series(model.predict(input_for_prediction),name='is_duplicate')\n",
    "    return res  \n",
    "\n",
    "def submit_model(results,criteria,kind,input_dataframe,model_key,label=None,proba=True,show_how_to_publish=True,kaggle=False):\n",
    "    if label is None:\n",
    "        absolute_file_name_csv = apply_absolute_file_name(criteria,kind,model_key)\n",
    "    else:\n",
    "        absolute_file_name_csv = apply_absolute_file_name(label+'_'+criteria,kind,model_key)\n",
    "    print_info('Doing apply')\n",
    "    prediction = simple_apply(results,model_key,input_dataframe,kind,proba)\n",
    "    print_info('Generating CSV file')\n",
    "    prediction.to_csv(absolute_file_name_csv,index=False)\n",
    "    print_info('Zipping file')\n",
    "    absolute_file_name_zip = zip_file_and_delete(absolute_file_name_csv)\n",
    "    print_info('%s is ready' % absolute_file_name_csv)\n",
    "    if show_how_to_publish:\n",
    "        if kaggle:\n",
    "            print_warning('Use this commands to submit apply results to kaggle')\n",
    "            print_warning(show_kaggle_command(absolute_file_name_zip))\n",
    "        else:\n",
    "            print_warning('Use this command to transfer apply _results to Windows host')\n",
    "            print_warning(show_docker_cp_command(absolute_file_name_csv))\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    res = simple_apply(default_multinomial_results,'nb_common_words/nb_words_question1+nb_words_question1-common_words+nb_words_question2-common_words+nb_common_words/(nb_words_question1+nb_words_question2)',challenge_dataframe,'80_20')\n",
    "    assert len(res)==len(challenge_dataframe)\n",
    "    display(res.head())\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    submit_model(\n",
    "        default_multinomial_results,\n",
    "        'logloss_proba',\n",
    "        '80_20',\n",
    "        challenge_dataframe.sample(1000,random_state=42),\n",
    "        'nb_common_words/nb_words_question1+nb_words_question1-common_words+nb_words_question2-common_words+nb_common_words/(nb_words_question1+nb_words_question2)',\n",
    "        label='_TODEL_')\n",
    "    "
   ]
  },
  {
   "source": [
    "### Let's try XGBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_xgboost_results = build_default_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_XGBOOST,train_dataframe,train_dataframe['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_validation_xgboost_results = build_cross_validation_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_XGBOOST,train_dataframe,train_dataframe['is_duplicate'])"
   ]
  },
  {
   "source": [
    "Manual code for sandboxing before launching big stuff"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost \n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta' : 0.02,\n",
    "    'max_depth':3 \n",
    "    }\n",
    "\n",
    "def build_XGBoost_model_80_20(training,target,column_names,show=True):\n",
    "    suffix='80_20'\n",
    "    input_train,input_test,target_train,target_test = train_test_split(training,target,random_state=42,test_size=0.2)\n",
    "    final_input_train = input_train[list(column_names)]\n",
    "    final_input_test = input_test[list(column_names)]\n",
    "    train_dm = xgboost.DMatrix(final_input_train, label = target_train, weight = input_train[WEIGHT])\n",
    "    validation_dm = xgboost.DMatrix(final_input_test, label = target_test,weight = input_test[WEIGHT])\n",
    "    watchlist = [(train_dm, 'train'), (validation_dm, 'valid')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(params, train_dm, 400, watchlist,early_stopping_rounds=50, verbose_eval=verbose)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model_xgboost(model,final_input_test,target_test,suffix,sample_weight = input_test[WEIGHT],show=show)\n",
    "    infos.update({add_suffix('time',suffix):duration})\n",
    "    if show:      \n",
    "        print_model_infos(suffix,PRINT_INFOS_ON_MODEL,infos,suffix)\n",
    "    return  infos\n",
    "\n",
    "def build_XGBoost_model_100_0(training,target,column_names,show=True):\n",
    "    suffix='100_0'\n",
    "    final_train = training[list(column_names)]\n",
    "    final_target = target\n",
    "    final_weight = training[WEIGHT]\n",
    "\n",
    "    train_dm = xgboost.DMatrix(final_train, label = final_target, weight = final_weight)\n",
    "    watchlist = [(train_dm, 'train')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(params, train_dm, 400, watchlist,early_stopping_rounds=50, verbose_eval=verbose)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model_xgboost(model,final_train,final_target,suffix,sample_weight = final_weight,show=show)\n",
    "    infos.update({add_suffix('time',suffix):duration})\n",
    "    if show:      \n",
    "        print_model_infos(suffix,PRINT_INFOS_ON_MODEL,infos,suffix)\n",
    "    return  infos\n",
    "\n",
    "def compute_metrics_model_xgboost(model,input_df,target_df,suffix,sample_weight = None,show = True):\n",
    "    final_input = xgboost.DMatrix(input_df)\n",
    "    prediction_proba_df = model.predict(final_input,ntree_limit=model.best_ntree_limit)\n",
    "    prediction_df = numpy.where(prediction_proba_df>0.5,1,0)\n",
    "    #prediction_proba_df = model.predict_proba(final_input,ntree_limit=model.best_ntree_limit)\n",
    "    res = metrics.classification_report(target_df,prediction_df,sample_weight = sample_weight,output_dict=True)\n",
    "    accuracy = res['accuracy']\n",
    "    score = res['weighted avg']['f1-score']\n",
    "    logloss_proba = metrics.log_loss(target_df,prediction_proba_df,sample_weight = sample_weight)\n",
    "    if show:\n",
    "        print('Classification report on %s' % suffix)\n",
    "        print(metrics.classification_report(target_df,prediction_df,sample_weight = sample_weight))\n",
    "    return add_suffix_to_keys(\n",
    "            {\n",
    "             'accuracy':accuracy,\n",
    "             'score':score,\n",
    "             'logloss_proba':logloss_proba,\n",
    "             'model':model\n",
    "           },\n",
    "           suffix)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    small_train = train_dataframe.sample(1000,random_state=42)\n",
    "    print_section('Unitary test : playbox XGBoost 100_0')\n",
    "    res = build_XGBoost_model_100_0(small_train,small_train['is_duplicate'],all_numeric_columns(small_train))\n",
    "    print(res)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    small_train = train_dataframe.sample(1000,random_state=42)\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "    res = build_XGBoost_model_80_20(small_train,small_train['is_duplicate'],all_numeric_columns(small_train))\n",
    "    print(res)\n",
    "    #xgboost.plot_importance(res['model'])\n",
    "    #xgboost.to_graphviz(model,num_trees=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost \n",
    "\n",
    "def learner_xgboost(algo_spec,input_train,target_train,input_test,target_test,weigth_train,weight_test,show=True):\n",
    "    train_dm = xgboost.DMatrix(input_train, label = target_train, weight = weigth_train)\n",
    "    if target_test is None:\n",
    "        watchlist = [(train_dm, 'train')]\n",
    "    else:\n",
    "        validation_dm = xgboost.DMatrix(input_test, label = target_test,weight = weight_test)\n",
    "        watchlist = [(train_dm, 'train'), (validation_dm, 'valid')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(algo_spec['params'], train_dm, 400, watchlist,early_stopping_rounds=50, verbose_eval=verbose)\n",
    "    duration = time.time()-start\n",
    "    return model\n",
    "\n",
    "\n",
    "def predictor_xgboost(model,input_df,target_df):\n",
    "    final_input = xgboost.DMatrix(input_df)\n",
    "    prediction_proba_df = model.predict(final_input,ntree_limit=model.best_ntree_limit)\n",
    "    prediction_df = numpy.where(prediction_proba_df>0.5,1,0)\n",
    "    return prediction_df,prediction_proba_df\n",
    "\n",
    "\n",
    "ALGORITHM_SPEC_XGBOOST = {\n",
    "    'learner': learner_xgboost,\n",
    "    'predicter': predictor_xgboost,\n",
    "    'params': {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'eta' : 0.02,\n",
    "        'max_depth':3 \n",
    "        }\n",
    "}\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : explore all features with default xgboost model on (80,20) & (100,0)')\n",
    "    small_train = train_dataframe.sample(100,random_state=42)\n",
    "    res = build_default_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_XGBOOST,small_train,small_train['is_duplicate'])\n",
    "    res\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : explore all features with xgboost model on  (80,20) & (100,0) with cross validation')\n",
    "    small_train = train_dataframe.sample(100,random_state=42)\n",
    "    res = build_cross_validation_model_on_all_subset_of_simple_features(ALGORITHM_SPEC_XGBOOST,small_train,small_train['is_duplicate'])\n",
    "    res"
   ]
  }
 ]
}