{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600379078859",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Compute full spacy analysis of all questions and pickle it for further usage\n",
    "* modele small : en_core_web_sm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>You will work on experiment compute_full_spacy_small_analysis</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "# Ugly incantation to make our framework working\n",
    "import sys\n",
    "sys.path.insert(0, r'/SAPDevelop/QuoraPairs/BruteForce/Tools')\n",
    "\n",
    "#import all our small tools (paths, cache, print,zip,excel, pandas, progress,..)\n",
    "from Tools.all import *\n",
    "\n",
    "# setup the name of our experiment\n",
    "# it will be used to store every result in a unique place\n",
    "EXPERIMENT='compute_full_spacy_small_analysis'\n",
    "print_warning('You will work on experiment %s' %EXPERIMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy stuff\n",
    "import spacy\n",
    "\n",
    "def load_spacy_model(spacy_model_name):\n",
    "     return spacy.load(spacy_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_from_columns(dataframe,output_column_name,function):\n",
    "    dataframe[output_column_name] = dataframe.progress_apply(function,axis=1)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "def add_column_from_column(dataframe,output_column_name,input_column_name,function):\n",
    "    dataframe[output_column_name] = dataframe[input_column_name].progress_apply(function)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "\n",
    "def build_spacy_analysis(dataframe,spacy_model_name):\n",
    "    start_small()\n",
    "    spacy_nlp = load_spacy_model(spacy_model_name)\n",
    "    print_warning('Compute full analysis of question1 using %s spacy model' % spacy_model_name)\n",
    "    add_column_from_column(dataframe,spacy_model_name,'question1',lambda x: spacy_nlp(x))\n",
    "    print_warning('Compute full analysis of question2 using %s spacy model' % spacy_model_name)\n",
    "    add_column_from_column(dataframe,spacy_model_name,'question2',lambda x: spacy_nlp(x))\n",
    "    end_small()\n",
    "    return dataframe\n",
    "\n",
    "def build_spacy_analysis_multithread(dataframe,spacy_model_name):\n",
    "    start_small()\n",
    "    spacy_nlp = load_spacy_model(spacy_model_name)\n",
    "    nb_threads = 6\n",
    "    print_warning('%d threads !! Compute full analysis of question1 using %s spacy model' % (nb_threads,spacy_model_name))\n",
    "    dataframe[spacy_model_name+'_q1']= list(spacy_nlp.pipe(dataframe['question1'],batch_size=1000,n_process=6))\n",
    "    print_warning('%d threads !! Compute full analysis of question2 using %s spacy model' % (nb_threads,spacy_model_name))\n",
    "    dataframe[spacy_model_name+'_q2']= list(spacy_nlp.pipe(dataframe['question2'],batch_size=1000,n_process=6))\n",
    "    end_small()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "France is in Europe <class 'spacy.tokens.doc.Doc'>\nFrance\nis\nin\nEurope\nTaiwan is not part of China <class 'spacy.tokens.doc.Doc'>\nTaiwan\nis\nnot\npart\nof\nChina\n--\nFrance <class 'spacy.tokens.token.Token'>\nis <class 'spacy.tokens.token.Token'>\nin <class 'spacy.tokens.token.Token'>\nEurope <class 'spacy.tokens.token.Token'>\n"
    }
   ],
   "source": [
    "spacy_nlp = load_spacy_model('en_core_web_sm')\n",
    "for d in spacy_nlp.pipe(['France is in Europe','Taiwan is not part of China']):\n",
    "\n",
    "\n",
    "print('--')\n",
    "s = spacy_nlp('France is in Europe')\n",
    "for i in s:\n",
    "    print(i,type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Prepare compute_full_spacy_small_analysis environment in ../compute_full_spacy_small_analysis</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Make local copy of ../PandasStore/clean_training.pkl</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Make local copy of ../PandasStore/clean_challenge.pkl</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Untouched input data has been loaded. Training: 404290 lines Challenge: 2345796 lines</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Spacy analysis with en_core_web_sm: Load or rebuild spacy_small_analysis</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>!!!!! ../compute_full_spacy_small_analysis/spacy_small_analysis.pkl does not exists!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Rebuild and save it</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span><small>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>6 threads !! Compute full analysis of question1 using en_core_web_md spacy model</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>6 threads !! Compute full analysis of question2 using en_core_web_md spacy model</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Save spacy_small_analysis</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:spacy_small_analysis contains 404290 lines in 571.1 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "prepare_environnement(EXPERIMENT)\n",
    "train_dataframe=load_dataframe(CLEAN_TRAINING_DATA)\n",
    "challenge_dataframe=load_dataframe(CLEAN_CHALLENGE_DATA)\n",
    "print_section('Untouched input data has been loaded. Training: %d lines Challenge: %d lines' % (len(train_dataframe),len(challenge_dataframe)))\n",
    "\n",
    "train_spacy_analysis = load_or_build_dataframe('Spacy analysis with en_core_web_sm','train_spacy_analysis',build_spacy_analysis_multithread,train_dataframe,param1='en_core_web_md')\n",
    "challenge_spacy_analysis = load_or_build_dataframe('Spacy analysis with en_core_web_sm','challenge_spacy_analysis',build_spacy_analysis_multithread,train_dataframe,param1='en_core_web_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id  qid1  qid2                                                                     question1                                                                                 question2  \\\n0   0     1     2            What is the step by step guide to invest in share market in india?                                 What is the step by step guide to invest in share market?   \n1   1     3     4                           What is the story of Kohinoor (Koh-i-Noor) Diamond?  What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?   \n2   2     5     6     How can I increase the speed of my internet connection while using a VPN?                               How can Internet speed be increased by hacking through DNS?   \n3   3     7     8                            Why am I mentally very lonely? How can I solve it?                         Find the remainder when [math]23^{24}[/math] is divided by 24,23?   \n4   4     9    10  Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?                                                   Which fish would survive in salt water?   \n\n   is_duplicate                                                                                 en_core_web_md_q1  \\\n0             0               (What, is, the, step, by, step, guide, to, invest, in, share, market, in, india, ?)   \n1             0                        (What, is, the, story, of, Kohinoor, (, Koh, -, i, -, Noor, ), Diamond, ?)   \n2             0        (How, can, I, increase, the, speed, of, my, internet, connection, while, using, a, VPN, ?)   \n3             0                                (Why, am, I, mentally, very, lonely, ?, How, can, I, solve, it, ?)   \n4             0  (Which, one, dissolve, in, water, quikly, sugar, ,, salt, ,, methane, and, carbon, di, oxide, ?)   \n\n                                                                                                      en_core_web_md_q2  \n0                                              (What, is, the, step, by, step, guide, to, invest, in, share, market, ?)  \n1  (What, would, happen, if, the, Indian, government, stole, the, Kohinoor, (, Koh, -, i, -, Noor, ), diamond, back, ?)  \n2                                              (How, can, Internet, speed, be, increased, by, hacking, through, DNS, ?)  \n3                                     (Find, the, remainder, when, [, math]23^{24}[/math, ], is, divided, by, 24,23, ?)  \n4                                                                     (Which, fish, would, survive, in, salt, water, ?)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>en_core_web_md_q1</th>\n      <th>en_core_web_md_q2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>What is the step by step guide to invest in share market in india?</td>\n      <td>What is the step by step guide to invest in share market?</td>\n      <td>0</td>\n      <td>(What, is, the, step, by, step, guide, to, invest, in, share, market, in, india, ?)</td>\n      <td>(What, is, the, step, by, step, guide, to, invest, in, share, market, ?)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?</td>\n      <td>What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?</td>\n      <td>0</td>\n      <td>(What, is, the, story, of, Kohinoor, (, Koh, -, i, -, Noor, ), Diamond, ?)</td>\n      <td>(What, would, happen, if, the, Indian, government, stole, the, Kohinoor, (, Koh, -, i, -, Noor, ), diamond, back, ?)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>How can I increase the speed of my internet connection while using a VPN?</td>\n      <td>How can Internet speed be increased by hacking through DNS?</td>\n      <td>0</td>\n      <td>(How, can, I, increase, the, speed, of, my, internet, connection, while, using, a, VPN, ?)</td>\n      <td>(How, can, Internet, speed, be, increased, by, hacking, through, DNS, ?)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>8</td>\n      <td>Why am I mentally very lonely? How can I solve it?</td>\n      <td>Find the remainder when [math]23^{24}[/math] is divided by 24,23?</td>\n      <td>0</td>\n      <td>(Why, am, I, mentally, very, lonely, ?, How, can, I, solve, it, ?)</td>\n      <td>(Find, the, remainder, when, [, math]23^{24}[/math, ], is, divided, by, 24,23, ?)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n      <td>10</td>\n      <td>Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0</td>\n      <td>(Which, one, dissolve, in, water, quikly, sugar, ,, salt, ,, methane, and, carbon, di, oxide, ?)</td>\n      <td>(Which, fish, would, survive, in, salt, water, ?)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "spacy_small_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataframe.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_dataframe.head().transpose()"
   ]
  },
  {
   "source": [
    "## Compute AUC of these basic features and try to figure if there is a bit of information inside each one\n",
    "ie is it helping to separate 1 from 0?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(dataframe['is_duplicate'],dataframe[column_name])\n",
    "\n",
    "def show_AUC(dataframe,column_name):\n",
    "    print_bullet('AUC %s: %f' % (column_name,simple_AUC(dataframe,column_name)))\n",
    "\n",
    "def display_simple_AUC(dataframe,column_name):\n",
    "    palette = sns.color_palette()\n",
    "    # Let multiplot_generator figure the size\n",
    "    #plot.figure(figsize=(10, 7))\n",
    "    plot.hist(dataframe[column_name][dataframe['is_duplicate']==1],bins=50,color=palette[3],label='Same',histtype='step')\n",
    "    plot.hist(train_dataframe[column_name][dataframe['is_duplicate']==0],bins=50,color=palette[2],label='Different',alpha = 0.75,histtype='step')\n",
    "    plot.title('AUC %s : %f' % (column_name,simple_AUC(dataframe,column_name)) , fontsize=10)\n",
    "    plot.xlabel(column_name)\n",
    "    plot.ylabel('Nb')\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "def show_all_simple_AUC(dataframe):\n",
    "    all =  all_numeric_columns(dataframe)\n",
    "    print_section( 'Show AUC on %d unique features' % len(all))\n",
    "    for name in all:\n",
    "        show_AUC(dataframe,name)\n",
    "        yield\n",
    "        display_simple_AUC(dataframe,name)\n",
    "    print_done('Done')\n",
    "\n",
    "\n",
    "def show_all_simple_AUC_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(show_all_simple_AUC(dataframe), nb_columns)\n",
    "\n",
    "show_all_simple_AUC_in_grid(train_dataframe,nb_columns=2)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## As a quick check, Build simple naive bayes models (full & with test) on a fixed set of features\n",
    "For both models, we return a performance record:\n",
    "* time to train\n",
    "* accuracy\n",
    "* score (a ponderation on f1-score)\n",
    "* logloss\n",
    "* list of features\n",
    "* model\n",
    "* Nb features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# This will add a suffix to all keys of a dict\n",
    "# Used to add _test,_train,_full to keys of infos about a model\n",
    "\n",
    "FAKE_INFOS_ON_MODEL_TEST_TRAIN = {\n",
    "    'accuracy_train': 0,\n",
    "    'score_train': 0,\n",
    "    'logloss_proba_train': 0,\n",
    "    'model_train': None,\n",
    "    'accuracy_test': 0,\n",
    "    'score_test': 0,\n",
    "    'logloss_proba_test': 0,\n",
    "    'model_test': None,\n",
    "    'time_test':0\n",
    "}\n",
    "\n",
    "FAKE_INFOS_ON_MODEL_FULL = {\n",
    "    'accuracy_full': 0,\n",
    "    'score_full': 0,\n",
    "    'logloss_proba_full': 0,\n",
    "    'model_full': None,\n",
    "    'time_full': 0\n",
    "}\n",
    "\n",
    "\n",
    "PRINT_INFOS_ON_2_MODELS = {\n",
    "    'accuracy_test': '%.4f',\n",
    "    'score_test': '%.4f',\n",
    "    'logloss_proba_test': '%.4f',\n",
    "    'time_test': '%.1f',\n",
    "\n",
    "    'accuracy_train': '%.4f',\n",
    "    'score_train': '%.4f',\n",
    "    'logloss_proba_train': '%.4f',\n",
    "\n",
    "    'accuracy_full': '%.4f',\n",
    "    'score_full': '%.4f',\n",
    "    'logloss_proba_full': '%.4f',\n",
    "    'time_full': '%.1f',\n",
    "\n",
    "    'nb_features': '%d',\n",
    "    'column_names': '%s'\n",
    "}\n",
    "\n",
    "PRINT_INFOS_ON_MODEL_TRAIN = {\n",
    "    'accuracy_train': '%.4f',\n",
    "    'score_train': '%.4f',\n",
    "    'logloss_proba_train': '%.4f',\n",
    "}\n",
    "\n",
    "PRINT_INFOS_ON_MODEL_TEST = {\n",
    "    'accuracy_test': '%.4f',\n",
    "    'score_test': '%.4f',\n",
    "    'logloss_proba_test': '%.4f',\n",
    "    'time_test': '%.1f',\n",
    "}\n",
    "\n",
    "PRINT_INFOS_ON_MODEL_FULL = {\n",
    "    'accuracy_full': '%.4f',\n",
    "    'score_full': '%.4f',\n",
    "    'logloss_proba_full': '%.4f',\n",
    "    'time_full': '%.1f',\n",
    "\n",
    "}\n",
    "\n",
    "def add_suffix_to_keys(d,s):\n",
    "    return dict(zip([k+s for k in d.keys()],d.values()))\n",
    "\n",
    "def format_model_infos(message,keys_formats,infos):\n",
    "    values = list()\n",
    "    for k,f in keys_formats.items():\n",
    "        values.append( f % infos[k])\n",
    "    #return print_info( '%s %s' %(message,'&nbsp;|&nbsp;'.join(values)))\n",
    "    return '%s %s' %(message,' | '.join(values))\n",
    "\n",
    "def print_model_infos(message,keys_formats,infos):\n",
    "    print_info(format_model_infos(message,keys_formats,infos))\n",
    "def print_header_infos_model(key_formats):\n",
    "    print_info('|'.join(key_formats.keys()))\n",
    "\n",
    "## Use & trick FAKE_XXXXX to mock the big loop of big trainings \n",
    "def FAKE_build_naivebayes_model_full(input,column_names,target,show=True):\n",
    "    return FAKE_INFOS_ON_MODEL_FULL\n",
    "\n",
    "def FAKE_build_naivebayes_model_with_test(input,column_names,target,show=True):\n",
    "    return FAKE_INFOS_ON_MODEL_TEST_TRAIN\n",
    "\n",
    "def FAKE_build_model(train_dataframe,column_names,target,show=True):\n",
    "    infos = FAKE_build_naivebayes_model_with_test(train_dataframe,column_names,target,show=show)\n",
    "    infos.update(FAKE_build_naivebayes_model_full(train_dataframe,column_names,target,show=show))\n",
    "    infos.update(\n",
    "        {\n",
    "            'nb_features':len(column_names),\n",
    "            'column_names':clean_combination_name(column_names),\n",
    "            'columns': column_names\n",
    "        })\n",
    "    return infos\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics_model(model,input_df,target_df,suffix,show=True):\n",
    "    prediction_df = model.predict(input_df)\n",
    "    prediction_proba_df = model.predict_proba(input_df)\n",
    "    res = metrics.classification_report(target_df,prediction_df,output_dict=True)\n",
    "    accuracy = res['accuracy']\n",
    "    score = res['0']['f1-score']*(1-0.17)+res['1']['f1-score']*.17\n",
    "    logloss_proba = metrics.log_loss(target_df,prediction_proba_df)\n",
    "    if show:\n",
    "        print('Classification report on %s' % suffix)\n",
    "        print(metrics.classification_report(target_df,prediction_df))\n",
    "    return add_suffix_to_keys(\n",
    "            {\n",
    "             'accuracy':accuracy,\n",
    "             'score':score,\n",
    "             'logloss_proba':logloss_proba,\n",
    "             'model':model\n",
    "           },\n",
    "           suffix)\n",
    "\n",
    "\n",
    "def build_naivebayes_model_with_test(input,column_names,target,show=True):\n",
    "    # print_bullet('Multinomial Naive Bayes with test on %s' % clean_combination_name(column_names))\n",
    "    input_train = pandas.DataFrame()\n",
    "    for column_name in column_names:\n",
    "        input_train[column_name]=input[column_name]\n",
    "    target_train = target\n",
    "    input_train,input_test,target_train,target_test = train_test_split(input_train,target_train,random_state=42,test_size=0.2)\n",
    "    if show:\n",
    "        print_info( 'Training:%d lines Test:%d Nb Features: %d' % (len(input_train),len(input_test),len(input_train.columns)))\n",
    "    model = MultinomialNB()\n",
    "    #naive_bayes_classifier_with_test=ComplementNB()\n",
    "    start = time.time()\n",
    "    model.fit(input_train,target_train)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model(model,input_test,target_test,'_test',show=show)\n",
    "    infos.update(compute_metrics_model(model,input_train,target_train,'_train',show=show))\n",
    "    infos.update({'time_test':duration})\n",
    "    if show:      \n",
    "        print_model_infos('Test ',PRINT_INFOS_ON_MODEL_TEST,infos)\n",
    "        print_model_infos('Train ',PRINT_INFOS_ON_MODEL_TRAIN,infos)\n",
    "    return  infos\n",
    "    \n",
    "def build_naivebayes_model_full(input,column_names,target,show=True):\n",
    "    input_full = pandas.DataFrame()\n",
    "    for column_name in column_names:\n",
    "        input_full[column_name] = input[column_name]\n",
    "    if show:\n",
    "        print_info( 'Training on %dx%d' % (len(input_full),len(input_full.columns)))\n",
    "    target_full = target\n",
    "    model = MultinomialNB()\n",
    "    start = time.time()\n",
    "    model.fit(input_full,target_full)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model(model,input_full,target_full,'_full',show=show)\n",
    "    infos.update({'time_full':duration})\n",
    "    if show:\n",
    "        print_model_infos('Full ',PRINT_INFOS_ON_MODEL_FULL,infos)\n",
    "    return infos\n",
    "\n",
    "\n",
    "def build_model(train_dataframe,column_names,target,show=True):\n",
    "    infos = build_naivebayes_model_with_test(train_dataframe,column_names,target,show=show)\n",
    "    infos.update(build_naivebayes_model_full(train_dataframe,column_names,target,show=show))\n",
    "    infos.update(\n",
    "        {\n",
    "            'nb_features':len(column_names),\n",
    "            'column_names':clean_combination_name(column_names),\n",
    "            'columns': column_names\n",
    "        })\n",
    "    return infos\n",
    "\n",
    "print_section('Quick check: Basic model using feature nb_words_question1')\n",
    "build_model(train_dataframe,['nb_words_question1'],train_dataframe['is_duplicate'])\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Build models (test & full) on all combinations of features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bad design choice : a DataFrame can be more convenient than a dict\n",
    "# But then, it is convenient to suppress all non numeric/string columns\n",
    "def models_dict_to_df(models_dict):\n",
    "    # reorder also the columns in a way I use more convenient\n",
    "    return pandas.DataFrame.from_dict(models_dict, orient='index').reindex(columns=['logloss_proba_test','logloss_proba_train','logloss_proba_full','nb_features','column_names','accuracy_test','accuracy_train','accuracy_full','score_test','score_train','score_full','model_test','model_train','model_full','columns','time_test','time_full'])\n",
    "\n",
    "def build_model_on_all_subset_of_simple_features(dataframe,target):\n",
    "    start = time.time()\n",
    "    all_combinations = list(all_subsets(all_numeric_columns(dataframe)))\n",
    "    steps_for_progress = int(len(all_combinations)/20)\n",
    "    print_section('%s : Build all models (with test+full) on every combination of simple features - %d lines' % (EXPERIMENT,len(dataframe)))\n",
    "    print_warning('%d*2 models built - only %d logged here' % (len(all_combinations),(int(len(all_combinations)/steps_for_progress))))\n",
    "    models_dict = dict()\n",
    "    print_header_infos_model(PRINT_INFOS_ON_2_MODELS)\n",
    "    progress = tqdm(all_combinations)\n",
    "    num_model = 0\n",
    "    for c in progress:\n",
    "        #if (len(c)) ==1:\n",
    "        if (len(c)) >0:\n",
    "            infos = build_model(dataframe,c,target,show=False)\n",
    "            models_dict[clean_combination_name(c)] = infos\n",
    "            # There is a smart panda progress bar but invisible in pdf\n",
    "            # So try to minimize logs and still have some progress info\n",
    "            if (num_model % steps_for_progress) == 0:\n",
    "                  print_info(format_model_infos('',PRINT_INFOS_ON_2_MODELS,infos))\n",
    "            num_model += 1\n",
    "            progress.refresh()\n",
    "    print_done('Done',top=start)\n",
    "    # Design mistake : need to convert dict to dataframe :(\n",
    "    return models_dict_to_df(models_dict)\n",
    "\n",
    "\n",
    "def FAKE_build_model_on_all_subset_of_simple_features(dataframe,target):\n",
    "    start = time.time()\n",
    "    print_section('%s : Build all models (with test+full) on every combination of simple features - %d rows' % (EXPERIMENT,len(dataframe)))\n",
    "    models_dict = dict()\n",
    "    print_header_infos_model(PRINT_INFOS_ON_2_MODELS)\n",
    "    all_combinations = list(all_subsets(all_numeric_columns(dataframe)))\n",
    "    progress = tqdm(all_combinations)\n",
    "    for c in progress:\n",
    "        if (len(c)) >0:\n",
    "            infos = FAKE_build_model(dataframe,c,target,show=False)\n",
    "            models_dict[clean_combination_name(c)] = infos\n",
    "            print_info(format_model_infos('',PRINT_INFOS_ON_2_MODELS,infos))\n",
    "            progress.refresh()\n",
    "    print_done('Done',top=start)\n",
    "    # Design mistake : need to convert dict to dataframe :(\n",
    "    return models_dict_to_df(models_dict)"
   ]
  },
  {
   "source": [
    "## If you want to quickly validate some code, activate this massive subsampling \n",
    "Results will be useless but code coverage will be ok\n",
    "This is also the good place to rebalance training data (cf next experiments)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small_train_dataframe = train_dataframe.sample(20000)\n",
    "# small_models = build_model_on_all_subset_of_simple_features(small_train_dataframe,small_train_dataframe['is_duplicate'])\n",
    "# graph_all_metrics_all_models_in_grid(small_models,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build ALL models\n",
    "# Do not forget to keep the result !\n",
    "# it's a dataframe with all infos on all models\n",
    "\n",
    "# Use this line to mock up the loop\n",
    "#FAKE_build_model_on_all_subset_of_simple_features(train_dataframe,train_dataframe['is_duplicate'])\n",
    "\n",
    "model_results = build_model_on_all_subset_of_simple_features(train_dataframe,train_dataframe['is_duplicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.head().transpose()"
   ]
  },
  {
   "source": [
    "## Graph metrics on all models generated"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_all_metric_all_models(results,y_label,metric_sort,metrics):\n",
    "    palette = sns.color_palette()\n",
    "    #data = results.sort_values(metric_sort+'_test',ascending=(metric_sort!='logloss_proba'))\n",
    "    data = results.sort_values(metric_sort+'_test',ascending=(metric_sort=='logloss_proba'))\n",
    "    plot.xlim(len(data),0)\n",
    "    #data = results.sort_values('logloss_proba_test',ascending=False)\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    x = numpy.arange(len(data))\n",
    "    num_col=1\n",
    "    if len(metrics)>1:\n",
    "        kinds =['_test']\n",
    "    else:\n",
    "        kinds = ['_test','_train','_full']\n",
    "    for k in kinds:\n",
    "        for m in metrics:\n",
    "                plot.plot(x,data[m+k],color = palette[num_col],label = m+k)\n",
    "                num_col = num_col+1\n",
    "    plot.ylabel(y_label)\n",
    "    plot.xlabel('Rank of logloss_proba_test')\n",
    "    #plot.plot(x,data['nb_features'],color=palette[num_col],label='Nb features')\n",
    "    #plot.plot(x,data['time_full'],color = palette[num_col+1],label = 'time_full')\n",
    "    plot.title(EXPERIMENT+':'+ y_label,fontsize=8)\n",
    "    #plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.grid()\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "def graph_all_metrics_all_models(results):\n",
    "    for m in ['accuracy','score','logloss_proba']:\n",
    "        yield\n",
    "        graph_all_metric_all_models(results,m,'logloss_proba',[m])\n",
    "    yield\n",
    "    graph_all_metric_all_models(results,'all metrics','logloss_proba',['accuracy','score','logloss_proba'])\n",
    "\n",
    "def graph_all_metrics_all_models_in_grid(results,nb_columns=2):\n",
    "    multiplot_from_generator(graph_all_metrics_all_models(results), nb_columns)\n",
    "    \n",
    "graph_all_metrics_all_models_in_grid(model_results,2)"
   ]
  },
  {
   "source": [
    "* When submitting to kaggle, any digit may be important but visually, logloss graph on test,train, full are quite identical\n",
    "* More convenient graph is the last one"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot.figure(figsize=(15, 10))\n",
    "graph_all_metric_all_models(model_results,'all metrics','logloss_proba',['accuracy','score','logloss_proba'])"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Query our 'database' of models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euark Suppose global variable model_results is available !\n",
    "\n",
    "# kind is 'test or 'full'\n",
    "def retrieve_model(model_key,kind):\n",
    "    model = model_results['model_'+kind][model_key]\n",
    "    column_names = model_results['columns'][model_key]\n",
    "    return model,numpy.asarray(column_names)\n",
    "    \n",
    "# suppose global variable model_results is available\n",
    "def find_best_models(top,criteria,kind):\n",
    "    if 'logloss' in criteria:\n",
    "        return model_results.nsmallest(top,criteria+'_'+kind)\n",
    "    else:\n",
    "        return model_results.nlargest(top,criteria+'_'+kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_models(4,'logloss_proba','full')"
   ]
  },
  {
   "source": [
    "## Save our database of performances in a convenient Excel file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_dict_to_excel(model_results)"
   ]
  },
  {
   "source": [
    "## Do some scorings with our selection of models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to rettrieve a specific model record\n",
    "# retrieve_model('nb_common_words+nb_common_words/(nb_words_question1+nb_words_question2)','test')\n",
    "\n",
    "# find best 4 full models based on logloss_proba\n",
    "find_best_models(4,'logloss_proba','full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_docker_cp_command(absolute_file_name):\n",
    "    return 'docker cp '+ DOCKER_IMAGE_NAME+':'+zip_file_name(absolute_file_name)+ ' c:\\\\temp\\\\outputs'\n",
    "\n",
    "def show_kaggle_command(absolute_file_name):\n",
    "    return 'kaggle competitions submit quora-question-pairs -f \"' + zip_file_name(absolute_file_name) +'\" -m \"' + absolute_file_name +'\"'\n",
    "\n",
    "def show_docker_cp_commands(best_results):\n",
    "    print_section('Use these commands to transfer apply results to windows host')\n",
    "    for c in best_results['file_name'].apply(show_docker_cp_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "\n",
    "def show_kaggle_commands(best_results):\n",
    "    print_section('Use these commands to submit apply results to kaggle')\n",
    "    for c in best_results['file_name'].apply(show_kaggle_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "    \n",
    "# return a dataframe fully ready to be converted in csv and published into kaggle\n",
    "def simple_apply(model_key,input_dataframe,kind,proba=True):\n",
    "    model,column_names=retrieve_model(model_key,kind)\n",
    "    input_for_prediction=input_dataframe[column_names]\n",
    "    res = pandas.DataFrame()\n",
    "    if 'test_id' in input_dataframe.columns:\n",
    "        res['test_id']=input_dataframe['test_id']\n",
    "    if proba:\n",
    "        res['is_duplicate']=pandas.Series(model.predict_proba(input_for_prediction)[:,1],name='is_duplicate')\n",
    "    else:\n",
    "        res['is_duplicate']=pandas.Series(model.predict(input_for_prediction),name='is_duplicate')\n",
    "    return res  \n",
    "\n",
    "def submit_model(criteria,kind,input_dataframe,model_key,proba=True,show_how_to_publish=True,kaggle=False):\n",
    "    absolute_file_name_csv = apply_absolute_file_name(criteria,kind,model_key)\n",
    "    print_info('Doing apply')\n",
    "    prediction = simple_apply(model_key,input_dataframe,kind,proba)\n",
    "    print_info('Generating CSV file')\n",
    "    prediction.to_csv(absolute_file_name_csv,index=False)\n",
    "    print_info('Zipping file')\n",
    "    absolute_file_name_zip = zip_file_and_delete(absolute_file_name_csv)\n",
    "    print_info('%s is ready' % absolute_file_name_csv)\n",
    "    if show_how_to_publish:\n",
    "        if kaggle:\n",
    "            print_warning('Use this commands to submit apply results to kaggle')\n",
    "            print_warning(show_kaggle_command(absolute_file_name_zip))\n",
    "        else:\n",
    "            print_warning('Use this command to transfer apply _results to Windows host')\n",
    "            print_warning(show_docker_cp_command(absolute_file_name_csv))\n",
    "\n",
    "\n",
    "\n",
    "def submit_best_models(top,criteria,kind,input_dataframe,proba=True,kaggle=False):\n",
    "    print_section('Submit best %d %s models by %s' % (top,kind,criteria))\n",
    "    best_models = find_best_models(top,criteria,kind)\n",
    "    best_models['model_key']=numpy.asarray(best_models.index)\n",
    "    #best_models['file_name']=numpy.array([apply_file_name(criteria,kind,n) for n in best_models.index])\n",
    "    best_models['file_name']=best_models['model_key'].apply(lambda mk: apply_absolute_file_name(criteria,kind,mk))\n",
    "    best_models['model_key'].apply(lambda mk: submit_model(criteria,kind,input_dataframe,mk,proba=proba,show_how_to_publish=False,kaggle=kaggle))\n",
    "    best_models['docker']=best_models['file_name'].apply(show_docker_cp_command)\n",
    "    best_models['kaggle']=best_models['file_name'].apply(show_kaggle_command)\n",
    "    print_done('Done')\n",
    "    if kaggle:\n",
    "        show_kaggle_commands(best_models)\n",
    "    else:\n",
    "        show_docker_cp_commands(best_models)\n",
    "    return best_models"
   ]
  },
  {
   "source": [
    "## A basic apply"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_apply('nb_common_words+nb_common_words/(nb_words_question1+nb_words_question2)',challenge_dataframe,'full')\n"
   ]
  },
  {
   "source": [
    "## Full apply\n",
    "* Retrieve a set of best models\n",
    "* for each model\n",
    "    * score the model \n",
    "    * save the result in csv\n",
    "    * zip the file\n",
    "    * display the command to submit on kaggle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logloss_proba_test = submit_best_models(3,'logloss_proba','test',challenge_dataframe,kaggle=True)"
   ]
  },
  {
   "source": [
    "# What Kaggle think about our submissions ?\n",
    "## Get back all our submissions to kaggle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_submissions(all_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_submissions(all_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_submissions(all_submissions,metric='publicScore',n=5)"
   ]
  },
  {
   "source": [
    "# Graph our submissions (WIP)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_submissions(dataframe):\n",
    "    print_section( 'Show history of submissions')\n",
    "    for name in ['publicScore']: #all_numeric_columns(dataframe):\n",
    "        yield\n",
    "        display_submission(dataframe)\n",
    "    print_done('Done')\n",
    "\n",
    "def display_submissions_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(display_submissions(dataframe), nb_columns)\n",
    "\n",
    "def auto_label(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        plot.annotate('{}'.format(height),xy=(rect.get_x()+rect.get_width()/2,height),xytext=(0,3),textcoords='offset points',ha='center',va='bottom')\n",
    "\n",
    "\n",
    "# Only useful to show private/public is almost the same\n",
    "# at least now\n",
    "def display_submission_private_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    labels = dataframe['description']\n",
    "    public_scores = dataframe['publicScore']\n",
    "    private_scores = dataframe['privateScore']\n",
    "    x = numpy.arange(len(labels))\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(20, 14))\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    rects_public_score = plot.bar(x-width/2,public_scores,width,label = 'publicScore')\n",
    "    rects_private_score = plot.bar(x+width/2,private_scores,width,label = 'privateScore')\n",
    "    plot.ylabel('Scores')\n",
    "    plot.title('History of scores')\n",
    "    plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.legend()\n",
    "    #auto_label(rects_public_score)\n",
    "    fig.tight_layout()\n",
    "    plot.show()\n",
    "\n",
    "def find_description_test_and_full(dataframe):\n",
    "    list_test = list()\n",
    "    list_full = list()\n",
    "    for i,d in dataframe.iterrows():\n",
    "        #print(d)\n",
    "        if d.description.find('!test!')>=0:\n",
    "            list_test.append(d.description.replace('!test!','!x!'))\n",
    "        if d.description.find('!full!')>=0:\n",
    "            list_full.append(d.description.replace('!full!','!x!'))\n",
    "    list_test_and_full = list(set(list_test)&set(list_full))\n",
    "    list_test = [i.replace('!x!','test') for i in list_test_and_full]\n",
    "    list_full = [i.replace('!x!','full') for i in list_test_and_full]\n",
    "    return [i.replace('!x!','!test!') for i in list_test_and_full]+[i.replace('!x!','!full!') for i in list_test_and_full]\n",
    "    \n",
    "        \n",
    "\n",
    "def display_submission_full_test_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    test_and_full_scores = dataframe\n",
    "    test_scores = test_and_full_scores[test_and_full_scores['description'].str.find('!test!')>=0]['publicScore']\n",
    "    full_scores = test_and_full_scores[test_and_full_scores['description'].str.find('!full!')>=0]['publicScore']\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(15, 10))\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    x = numpy.arange(len(test_scores))\n",
    "    rects_test_score = plot.bar(x-width/2,test_scores,width,label = 'Score on test')\n",
    "    x = numpy.arange(len(full_scores))\n",
    "    rects_full_score = plot.bar(x+width/2,full_scores,width,label = 'Score on full')\n",
    "    plot.ylabel('Scores')\n",
    "    plot.title('History of scores')\n",
    "    plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.legend()\n",
    "    #auto_label(rects_public_score)\n",
    "    fig.tight_layout()\n",
    "    plot.show()\n",
    "\n",
    "# to check full and test models have been properly separated for further usage\n",
    "# res = find_description_test_and_full(all_submissions)\n",
    "\n",
    "# Only useful to show private/public is almost the same\n",
    "# at least now\n",
    "def display_submission_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    x = numpy.arange(len(dataframe['description']))\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(10, 10))\n",
    "    #ax = plot\n",
    "    #fig,ax = plot.subplots()\n",
    "    plot.plot(x,dataframe['publicScore'],label='Public score')\n",
    "    plot.plot(x,dataframe['privateScore'],label='Private score')\n",
    "    plot.ylabel('Score')\n",
    "    plot.xlabel('# Submission')\n",
    "    plot.title('History of scores')\n",
    "    plot.legend()\n",
    "    plot.show()"
   ]
  },
  {
   "source": [
    "### First scores were quite bad : logloss .... *28* !!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Worst submissions based on publicScore')\n",
    "all_submissions.nlargest(2,'publicScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Best submissions based on publicScore')\n",
    "all_submissions.nsmallest(2,'publicScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_submission_public(all_submissions)"
   ]
  }
 ]
}