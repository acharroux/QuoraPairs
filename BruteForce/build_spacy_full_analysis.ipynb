{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600525798913",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Compute full spacy analysis of all questions and pickle it for further usage\n",
    "* nlp model : en_core_web_md  (medium)\n",
    "* nlp pipeline is partially deactivated: tagger,parser,textcat are unused\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:RED\">You will work on experiment compute_spacy_medium_analysis </span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "# Ugly incantation to make our framework working\n",
    "import sys\n",
    "sys.path.insert(0, r'/SAPDevelop/QuoraPairs/BruteForce/Tools')\n",
    "\n",
    "#import all our small tools (paths, cache, print,zip,excel, pandas, progress,..)\n",
    "from Tools.all import *\n",
    "\n",
    "# setup the name of our experiment\n",
    "# it will be used to store every result in a unique place\n",
    "EXPERIMENT='compute_spacy_medium_analysis'\n",
    "print_alert('You will work on experiment %s' %EXPERIMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy stuff\n",
    "import spacy\n",
    "\n",
    "def load_spacy_model(spacy_model_name):\n",
    "     return spacy.load(spacy_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_from_columns(dataframe,output_column_name,function):\n",
    "    dataframe[output_column_name] = dataframe.progress_apply(function,axis=1)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "def add_column_from_column(dataframe,output_column_name,input_column_name,function):\n",
    "    dataframe[output_column_name] = dataframe[input_column_name].progress_apply(function)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "\n",
    "def build_spacy_analysis(dataframe,spacy_model_name):\n",
    "    start_small()\n",
    "    spacy_nlp = load_spacy_model(spacy_model_name)\n",
    "    print_warning('Compute full analysis of question1 using %s spacy model' % spacy_model_name)\n",
    "    add_column_from_column(dataframe,spacy_model_name,'question1',lambda x: spacy_nlp(x))\n",
    "    print_warning('Compute full analysis of question2 using %s spacy model' % spacy_model_name)\n",
    "    add_column_from_column(dataframe,spacy_model_name,'question2',lambda x: spacy_nlp(x))\n",
    "    end_small()\n",
    "    return dataframe\n",
    "\n",
    "def build_spacy_analysis_multithread(dataframe,spacy_model_name):\n",
    "    start_small()\n",
    "    spacy_nlp = load_spacy_model(spacy_model_name)\n",
    "    nb_threads = 6\n",
    "    print_warning('%d threads !! Compute full analysis of question1 using %s spacy model' % (nb_threads,spacy_model_name))\n",
    "    dataframe[spacy_model_name+'_q1']= list(spacy_nlp.pipe(dataframe['question1'],batch_size=1000,n_process=6))\n",
    "    print_warning('%d threads !! Compute full analysis of question2 using %s spacy model' % (nb_threads,spacy_model_name))\n",
    "    dataframe[spacy_model_name+'_q2']= list(spacy_nlp.pipe(dataframe['question2'],batch_size=1000,n_process=6))\n",
    "    end_small()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Prepare compute_full_spacy_full_analysis environment in ../compute_full_spacy_full_analysis</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Untouched input data has been loaded. Training: 404290 lines Challenge: 2345796 lines</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "prepare_environnement(EXPERIMENT)\n",
    "train_dataframe=load_dataframe(CLEAN_TRAINING_DATA)\n",
    "challenge_dataframe=load_dataframe(CLEAN_CHALLENGE_DATA)\n",
    "print_section('Untouched input data has been loaded. Training: %d lines Challenge: %d lines' % (len(train_dataframe),len(challenge_dataframe)))\n",
    "\n",
    "#train_spacy_analysis = load_or_build_dataframe('Spacy analysis with en_core_web_sm','train_spacy_analysis',build_spacy_analysis_multithread,train_dataframe,param1='en_core_web_md')\n",
    "#challenge_spacy_analysis = load_or_build_dataframe('Spacy analysis with en_core_web_sm','challenge_spacy_analysis',build_spacy_analysis_multithread,train_dataframe,param1='en_core_web_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = load_spacy_model('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "TEMPLATE_ALL_ENTITIES = [\n",
    "            'GPE',\n",
    "            'PERSON',\n",
    "            'PRODUCT',\n",
    "            'CARDINAL',\n",
    "            'ORG',\n",
    "            'ORDINAL',\n",
    "            'TIME',\n",
    "            'DATE',\n",
    "            'NORP',\n",
    "            'WORK_OF_ART',\n",
    "            'LANGUAGE',\n",
    "            'EVENT',\n",
    "            'FAC',\n",
    "            'MONEY',\n",
    "            'LAW',\n",
    "            'LOC',\n",
    "            'QUANTITY',\n",
    "            'PERCENT'\n",
    "            ]\n",
    "\n",
    "ENTITIES_TO_KEEP = [\n",
    "            'GPE',\n",
    "            'PERSON',\n",
    "            'PRODUCT']\n",
    "\n",
    "# browse all the dataframe and build a dict of occurrences by ent type\n",
    "# We loose order\n",
    "# This would be useful to sniff the dataset by grabbing values of most common entities and search it inside dataset\n",
    "def build_all_ent_types_values(dataframe,spacy_nlp,disable=None):\n",
    "    ent_values = dict(map(lambda x: (x,collections.Counter()),TEMPLATE_ALL_ENTITIES))\n",
    "    start = time.time()\n",
    "    for d in tqdm(spacy_nlp.pipe(dataframe['question1'],batch_size=100,n_process=os.cpu_count()-1),total=len(dataframe)):\n",
    "        for t in d:\n",
    "            if t.ent_type_ !='':\n",
    "                ent_values[t.ent_type_][t.text] += 1  \n",
    "    print_done('Build entities Done',top=start)\n",
    "    return ent_values\n",
    "\n",
    "def sniff_all_ent_types_values(ents):\n",
    "    for i in ents.keys():\n",
    "        print(i,ents[i].most_common(10))\n",
    "\n",
    "\n",
    "def build_ent_types_values(dataframe,question_name,spacy_nlp_name,disable=None):\n",
    "    if disable is None:\n",
    "        print_info('Build entities on %s using %s' % (question_name,spacy_nlp_name))\n",
    "    else:\n",
    "        print_info('Build entities on %s using %s in fast mode' % (question_name,spacy_nlp_name))\n",
    "    spacy_nlp = load_spacy_model(spacy_nlp_name,disable=disable)\n",
    "    start = time.time()\n",
    "    ent_type_values_in_all_rows = list()\n",
    "    entities_to_keep = dict(map(lambda x: (x,None),ENTITIES_TO_KEEP))\n",
    "    for d in tqdm(spacy_nlp.pipe(dataframe[question_name],batch_size=100,n_process=os.cpu_count()-1),total=len(dataframe)):\n",
    "        ent_type_values_row = dict(map(lambda x: (x,list()),ENTITIES_TO_KEEP))\n",
    "        # keep only the ones non empty and we are interested in\n",
    "        [ent_type_values_row[t.ent_type_].append(t.text) for t in d if t.ent_type_!= '' and t.ent_type_ in ent_type_values_row]\n",
    "        # rename keys like question1_PERSON:[Donald Trump]\n",
    "        ent_type_values_row = dict(map(lambda x: ('_'.join([question_name,x]),ent_type_values_row[x]),ent_type_values_row))\n",
    "        # generate len for each category nb_question1_PERSON:2\n",
    "        nb_type_values = dict(map(lambda x: ('_'.join([x,'nb']),len(ent_type_values_row[x])),ent_type_values_row))\n",
    "        # Merge 2 dicts\n",
    "        nb_type_values.update(ent_type_values_row)\n",
    "        # save it. Ouf !!\n",
    "        ent_type_values_in_all_rows.append(nb_type_values)\n",
    "    print_done('Build entities Done',top=start)\n",
    "    return pandas.DataFrame(ent_type_values_in_all_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Build entities using en_core_web_md in fast mode</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=404290.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba8cab3e83ae46ae84284f0428b1b15f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Build entities Done in 113.1 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Build entities using en_core_web_md in fast mode</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2345796.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff2e22a58db3466c87f9e3c3e073ac05"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Build entities Done in 687.9 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     question1_GPE_nb  question1_PERSON_nb  question1_PRODUCT_nb question1_GPE                      question1_PERSON question1_PRODUCT\n0                   1                    0                     0       [Korea]                                    []                []\n1                   0                    0                     0            []                                    []                []\n2                   0                    0                     0            []                                    []                []\n3                   0                    0                     0            []                                    []                []\n4                   0                    0                     0            []                                    []                []\n..                ...                  ...                   ...           ...                                   ...               ...\n195                 0                    4                     0            []  [Beethoven, Mozart, Vivaldi, Brahms]                []\n196                 0                    0                     0            []                                    []                []\n197                 0                    0                     0            []                                    []                []\n198                 0                    0                     0            []                                    []                []\n199                 0                    0                     0            []                                    []                []\n\n[200 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1_GPE_nb</th>\n      <th>question1_PERSON_nb</th>\n      <th>question1_PRODUCT_nb</th>\n      <th>question1_GPE</th>\n      <th>question1_PERSON</th>\n      <th>question1_PRODUCT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[Korea]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[Beethoven, Mozart, Vivaldi, Brahms]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "\n",
    "mini_train_dataframe = train_dataframe.sample(200,random_state=42)   \n",
    "# mini_small = build_ent_types_values(mini_train_dataframe,'en_core_web_sm')\n",
    "# mini_small_speed = build_ent_types_values(mini_train_dataframe,'en_core_web_sm',disable=['tagger','parser','textcat'])\n",
    "# mini_medium = build_ent_types_values(mini_train_dataframe,'en_core_web_md')\n",
    "training_entities_question1 = build_ent_types_values(train_dataframe,'question1','en_core_web_md',disable=['tagger','parser','textcat'])\n",
    "challenge_entities_question1 = build_ent_types_values(challenge_dataframe,'question1','en_core_web_md',disable=['tagger','parser','textcat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_entities_question1.to_pickle('/SAPDevelop/tmp/challenge_entities_question1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_speed_df = pandas.DataFrame.from_dict(medium_speed)\n",
    "medium_df = pandas.DataFrame.from_dict(medium)\n",
    "small_speed_df = pandas.DataFrame.from_dict(small_speed)\n",
    "small_df = pandas.DataFrame.from_dict(small)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(36230, 18) (36332, 18)\n(38213, 18) (38305, 18)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "print(small_df.shape,small_speed_df.shape)\n",
    "print(medium_df.shape,medium_speed_df.shape)\n",
    "os.cpu_count()"
   ]
  },
  {
   "source": [
    "## Compute AUC of these basic features and try to figure if there is a bit of information inside each one\n",
    "ie is it helping to separate 1 from 0?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(dataframe['is_duplicate'],dataframe[column_name])\n",
    "\n",
    "def show_AUC(dataframe,column_name):\n",
    "    print_bullet('AUC %s: %f' % (column_name,simple_AUC(dataframe,column_name)))\n",
    "\n",
    "def display_simple_AUC(dataframe,column_name):\n",
    "    palette = sns.color_palette()\n",
    "    # Let multiplot_generator figure the size\n",
    "    #plot.figure(figsize=(10, 7))\n",
    "    plot.hist(dataframe[column_name][dataframe['is_duplicate']==1],bins=50,color=palette[3],label='Same',histtype='step')\n",
    "    plot.hist(train_dataframe[column_name][dataframe['is_duplicate']==0],bins=50,color=palette[2],label='Different',alpha = 0.75,histtype='step')\n",
    "    plot.title('AUC %s : %f' % (column_name,simple_AUC(dataframe,column_name)) , fontsize=10)\n",
    "    plot.xlabel(column_name)\n",
    "    plot.ylabel('Nb')\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "def show_all_simple_AUC(dataframe):\n",
    "    all =  all_numeric_columns(dataframe)\n",
    "    print_section( 'Show AUC on %d unique features' % len(all))\n",
    "    for name in all:\n",
    "        show_AUC(dataframe,name)\n",
    "        yield\n",
    "        display_simple_AUC(dataframe,name)\n",
    "    print_done('Done')\n",
    "\n",
    "\n",
    "def show_all_simple_AUC_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(show_all_simple_AUC(dataframe), nb_columns)\n",
    "\n",
    "show_all_simple_AUC_in_grid(train_dataframe,nb_columns=2)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## As a quick check, Build simple naive bayes models (full & with test) on a fixed set of features\n",
    "For both models, we return a performance record:\n",
    "* time to train\n",
    "* accuracy\n",
    "* score (a ponderation on f1-score)\n",
    "* logloss\n",
    "* list of features\n",
    "* model\n",
    "* Nb features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# This will add a suffix to all keys of a dict\n",
    "# Used to add _test,_train,_full to keys of infos about a model\n",
    "\n",
    "FAKE_INFOS_ON_MODEL_TEST_TRAIN = {\n",
    "    'accuracy_train': 0,\n",
    "    'score_train': 0,\n",
    "    'logloss_proba_train': 0,\n",
    "    'model_train': None,\n",
    "    'accuracy_test': 0,\n",
    "    'score_test': 0,\n",
    "    'logloss_proba_test': 0,\n",
    "    'model_test': None,\n",
    "    'time_test':0\n",
    "}\n",
    "\n",
    "FAKE_INFOS_ON_MODEL_FULL = {\n",
    "    'accuracy_full': 0,\n",
    "    'score_full': 0,\n",
    "    'logloss_proba_full': 0,\n",
    "    'model_full': None,\n",
    "    'time_full': 0\n",
    "}\n",
    "\n",
    "\n",
    "PRINT_INFOS_ON_2_MODELS = {\n",
    "    'accuracy_test': '%.4f',\n",
    "    'score_test': '%.4f',\n",
    "    'logloss_proba_test': '%.4f',\n",
    "    'time_test': '%.1f',\n",
    "\n",
    "    'accuracy_train': '%.4f',\n",
    "    'score_train': '%.4f',\n",
    "    'logloss_proba_train': '%.4f',\n",
    "\n",
    "    'accuracy_full': '%.4f',\n",
    "    'score_full': '%.4f',\n",
    "    'logloss_proba_full': '%.4f',\n",
    "    'time_full': '%.1f',\n",
    "\n",
    "    'nb_features': '%d',\n",
    "    'column_names': '%s'\n",
    "}\n",
    "\n",
    "PRINT_INFOS_ON_MODEL_TRAIN = {\n",
    "    'accuracy_train': '%.4f',\n",
    "    'score_train': '%.4f',\n",
    "    'logloss_proba_train': '%.4f',\n",
    "}\n",
    "\n",
    "PRINT_INFOS_ON_MODEL_TEST = {\n",
    "    'accuracy_test': '%.4f',\n",
    "    'score_test': '%.4f',\n",
    "    'logloss_proba_test': '%.4f',\n",
    "    'time_test': '%.1f',\n",
    "}\n",
    "\n",
    "PRINT_INFOS_ON_MODEL_FULL = {\n",
    "    'accuracy_full': '%.4f',\n",
    "    'score_full': '%.4f',\n",
    "    'logloss_proba_full': '%.4f',\n",
    "    'time_full': '%.1f',\n",
    "\n",
    "}\n",
    "\n",
    "def add_suffix_to_keys(d,s):\n",
    "    return dict(zip([k+s for k in d.keys()],d.values()))\n",
    "\n",
    "def format_model_infos(message,keys_formats,infos):\n",
    "    values = list()\n",
    "    for k,f in keys_formats.items():\n",
    "        values.append( f % infos[k])\n",
    "    #return print_info( '%s %s' %(message,'&nbsp;|&nbsp;'.join(values)))\n",
    "    return '%s %s' %(message,' | '.join(values))\n",
    "\n",
    "def print_model_infos(message,keys_formats,infos):\n",
    "    print_info(format_model_infos(message,keys_formats,infos))\n",
    "def print_header_infos_model(key_formats):\n",
    "    print_info('|'.join(key_formats.keys()))\n",
    "\n",
    "## Use & trick FAKE_XXXXX to mock the big loop of big trainings \n",
    "def FAKE_build_naivebayes_model_full(input,column_names,target,show=True):\n",
    "    return FAKE_INFOS_ON_MODEL_FULL\n",
    "\n",
    "def FAKE_build_naivebayes_model_with_test(input,column_names,target,show=True):\n",
    "    return FAKE_INFOS_ON_MODEL_TEST_TRAIN\n",
    "\n",
    "def FAKE_build_model(train_dataframe,column_names,target,show=True):\n",
    "    infos = FAKE_build_naivebayes_model_with_test(train_dataframe,column_names,target,show=show)\n",
    "    infos.update(FAKE_build_naivebayes_model_full(train_dataframe,column_names,target,show=show))\n",
    "    infos.update(\n",
    "        {\n",
    "            'nb_features':len(column_names),\n",
    "            'column_names':clean_combination_name(column_names),\n",
    "            'columns': column_names\n",
    "        })\n",
    "    return infos\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics_model(model,input_df,target_df,suffix,show=True):\n",
    "    prediction_df = model.predict(input_df)\n",
    "    prediction_proba_df = model.predict_proba(input_df)\n",
    "    res = metrics.classification_report(target_df,prediction_df,output_dict=True)\n",
    "    accuracy = res['accuracy']\n",
    "    score = res['0']['f1-score']*(1-0.17)+res['1']['f1-score']*.17\n",
    "    logloss_proba = metrics.log_loss(target_df,prediction_proba_df)\n",
    "    if show:\n",
    "        print('Classification report on %s' % suffix)\n",
    "        print(metrics.classification_report(target_df,prediction_df))\n",
    "    return add_suffix_to_keys(\n",
    "            {\n",
    "             'accuracy':accuracy,\n",
    "             'score':score,\n",
    "             'logloss_proba':logloss_proba,\n",
    "             'model':model\n",
    "           },\n",
    "           suffix)\n",
    "\n",
    "\n",
    "def build_naivebayes_model_with_test(input,column_names,target,show=True):\n",
    "    # print_bullet('Multinomial Naive Bayes with test on %s' % clean_combination_name(column_names))\n",
    "    input_train = pandas.DataFrame()\n",
    "    for column_name in column_names:\n",
    "        input_train[column_name]=input[column_name]\n",
    "    target_train = target\n",
    "    input_train,input_test,target_train,target_test = train_test_split(input_train,target_train,random_state=42,test_size=0.2)\n",
    "    if show:\n",
    "        print_info( 'Training:%d lines Test:%d Nb Features: %d' % (len(input_train),len(input_test),len(input_train.columns)))\n",
    "    model = MultinomialNB()\n",
    "    #naive_bayes_classifier_with_test=ComplementNB()\n",
    "    start = time.time()\n",
    "    model.fit(input_train,target_train)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model(model,input_test,target_test,'_test',show=show)\n",
    "    infos.update(compute_metrics_model(model,input_train,target_train,'_train',show=show))\n",
    "    infos.update({'time_test':duration})\n",
    "    if show:      \n",
    "        print_model_infos('Test ',PRINT_INFOS_ON_MODEL_TEST,infos)\n",
    "        print_model_infos('Train ',PRINT_INFOS_ON_MODEL_TRAIN,infos)\n",
    "    return  infos\n",
    "    \n",
    "def build_naivebayes_model_full(input,column_names,target,show=True):\n",
    "    input_full = pandas.DataFrame()\n",
    "    for column_name in column_names:\n",
    "        input_full[column_name] = input[column_name]\n",
    "    if show:\n",
    "        print_info( 'Training on %dx%d' % (len(input_full),len(input_full.columns)))\n",
    "    target_full = target\n",
    "    model = MultinomialNB()\n",
    "    start = time.time()\n",
    "    model.fit(input_full,target_full)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model(model,input_full,target_full,'_full',show=show)\n",
    "    infos.update({'time_full':duration})\n",
    "    if show:\n",
    "        print_model_infos('Full ',PRINT_INFOS_ON_MODEL_FULL,infos)\n",
    "    return infos\n",
    "\n",
    "\n",
    "def build_model(train_dataframe,column_names,target,show=True):\n",
    "    infos = build_naivebayes_model_with_test(train_dataframe,column_names,target,show=show)\n",
    "    infos.update(build_naivebayes_model_full(train_dataframe,column_names,target,show=show))\n",
    "    infos.update(\n",
    "        {\n",
    "            'nb_features':len(column_names),\n",
    "            'column_names':clean_combination_name(column_names),\n",
    "            'columns': column_names\n",
    "        })\n",
    "    return infos\n",
    "\n",
    "print_section('Quick check: Basic model using feature nb_words_question1')\n",
    "build_model(train_dataframe,['nb_words_question1'],train_dataframe['is_duplicate'])\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Build models (test & full) on all combinations of features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bad design choice : a DataFrame can be more convenient than a dict\n",
    "# But then, it is convenient to suppress all non numeric/string columns\n",
    "def models_dict_to_df(models_dict):\n",
    "    # reorder also the columns in a way I use more convenient\n",
    "    return pandas.DataFrame.from_dict(models_dict, orient='index').reindex(columns=['logloss_proba_test','logloss_proba_train','logloss_proba_full','nb_features','column_names','accuracy_test','accuracy_train','accuracy_full','score_test','score_train','score_full','model_test','model_train','model_full','columns','time_test','time_full'])\n",
    "\n",
    "def build_model_on_all_subset_of_simple_features(dataframe,target):\n",
    "    start = time.time()\n",
    "    all_combinations = list(all_subsets(all_numeric_columns(dataframe)))\n",
    "    steps_for_progress = int(len(all_combinations)/20)\n",
    "    print_section('%s : Build all models (with test+full) on every combination of simple features - %d lines' % (EXPERIMENT,len(dataframe)))\n",
    "    print_warning('%d*2 models built - only %d logged here' % (len(all_combinations),(int(len(all_combinations)/steps_for_progress))))\n",
    "    models_dict = dict()\n",
    "    print_header_infos_model(PRINT_INFOS_ON_2_MODELS)\n",
    "    progress = tqdm(all_combinations)\n",
    "    num_model = 0\n",
    "    for c in progress:\n",
    "        #if (len(c)) ==1:\n",
    "        if (len(c)) >0:\n",
    "            infos = build_model(dataframe,c,target,show=False)\n",
    "            models_dict[clean_combination_name(c)] = infos\n",
    "            # There is a smart panda progress bar but invisible in pdf\n",
    "            # So try to minimize logs and still have some progress info\n",
    "            if (num_model % steps_for_progress) == 0:\n",
    "                  print_info(format_model_infos('',PRINT_INFOS_ON_2_MODELS,infos))\n",
    "            num_model += 1\n",
    "            progress.refresh()\n",
    "    print_done('Done',top=start)\n",
    "    # Design mistake : need to convert dict to dataframe :(\n",
    "    return models_dict_to_df(models_dict)\n",
    "\n",
    "\n",
    "def FAKE_build_model_on_all_subset_of_simple_features(dataframe,target):\n",
    "    start = time.time()\n",
    "    print_section('%s : Build all models (with test+full) on every combination of simple features - %d rows' % (EXPERIMENT,len(dataframe)))\n",
    "    models_dict = dict()\n",
    "    print_header_infos_model(PRINT_INFOS_ON_2_MODELS)\n",
    "    all_combinations = list(all_subsets(all_numeric_columns(dataframe)))\n",
    "    progress = tqdm(all_combinations)\n",
    "    for c in progress:\n",
    "        if (len(c)) >0:\n",
    "            infos = FAKE_build_model(dataframe,c,target,show=False)\n",
    "            models_dict[clean_combination_name(c)] = infos\n",
    "            print_info(format_model_infos('',PRINT_INFOS_ON_2_MODELS,infos))\n",
    "            progress.refresh()\n",
    "    print_done('Done',top=start)\n",
    "    # Design mistake : need to convert dict to dataframe :(\n",
    "    return models_dict_to_df(models_dict)"
   ]
  },
  {
   "source": [
    "## If you want to quickly validate some code, activate this massive subsampling \n",
    "Results will be useless but code coverage will be ok\n",
    "This is also the good place to rebalance training data (cf next experiments)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small_train_dataframe = train_dataframe.sample(20000)\n",
    "# small_models = build_model_on_all_subset_of_simple_features(small_train_dataframe,small_train_dataframe['is_duplicate'])\n",
    "# graph_all_metrics_all_models_in_grid(small_models,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build ALL models\n",
    "# Do not forget to keep the result !\n",
    "# it's a dataframe with all infos on all models\n",
    "\n",
    "# Use this line to mock up the loop\n",
    "#FAKE_build_model_on_all_subset_of_simple_features(train_dataframe,train_dataframe['is_duplicate'])\n",
    "\n",
    "model_results = build_model_on_all_subset_of_simple_features(train_dataframe,train_dataframe['is_duplicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.head().transpose()"
   ]
  },
  {
   "source": [
    "## Graph metrics on all models generated"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_all_metric_all_models(results,y_label,metric_sort,metrics):\n",
    "    palette = sns.color_palette()\n",
    "    #data = results.sort_values(metric_sort+'_test',ascending=(metric_sort!='logloss_proba'))\n",
    "    data = results.sort_values(metric_sort+'_test',ascending=(metric_sort=='logloss_proba'))\n",
    "    plot.xlim(len(data),0)\n",
    "    #data = results.sort_values('logloss_proba_test',ascending=False)\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    x = numpy.arange(len(data))\n",
    "    num_col=1\n",
    "    if len(metrics)>1:\n",
    "        kinds =['_test']\n",
    "    else:\n",
    "        kinds = ['_test','_train','_full']\n",
    "    for k in kinds:\n",
    "        for m in metrics:\n",
    "                plot.plot(x,data[m+k],color = palette[num_col],label = m+k)\n",
    "                num_col = num_col+1\n",
    "    plot.ylabel(y_label)\n",
    "    plot.xlabel('Rank of logloss_proba_test')\n",
    "    #plot.plot(x,data['nb_features'],color=palette[num_col],label='Nb features')\n",
    "    #plot.plot(x,data['time_full'],color = palette[num_col+1],label = 'time_full')\n",
    "    plot.title(EXPERIMENT+':'+ y_label,fontsize=8)\n",
    "    #plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.grid()\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "def graph_all_metrics_all_models(results):\n",
    "    for m in ['accuracy','score','logloss_proba']:\n",
    "        yield\n",
    "        graph_all_metric_all_models(results,m,'logloss_proba',[m])\n",
    "    yield\n",
    "    graph_all_metric_all_models(results,'all metrics','logloss_proba',['accuracy','score','logloss_proba'])\n",
    "\n",
    "def graph_all_metrics_all_models_in_grid(results,nb_columns=2):\n",
    "    multiplot_from_generator(graph_all_metrics_all_models(results), nb_columns)\n",
    "    \n",
    "graph_all_metrics_all_models_in_grid(model_results,2)"
   ]
  },
  {
   "source": [
    "* When submitting to kaggle, any digit may be important but visually, logloss graph on test,train, full are quite identical\n",
    "* More convenient graph is the last one"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot.figure(figsize=(15, 10))\n",
    "graph_all_metric_all_models(model_results,'all metrics','logloss_proba',['accuracy','score','logloss_proba'])"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Query our 'database' of models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euark Suppose global variable model_results is available !\n",
    "\n",
    "# kind is 'test or 'full'\n",
    "def retrieve_model(model_key,kind):\n",
    "    model = model_results['model_'+kind][model_key]\n",
    "    column_names = model_results['columns'][model_key]\n",
    "    return model,numpy.asarray(column_names)\n",
    "    \n",
    "# suppose global variable model_results is available\n",
    "def find_best_models(top,criteria,kind):\n",
    "    if 'logloss' in criteria:\n",
    "        return model_results.nsmallest(top,criteria+'_'+kind)\n",
    "    else:\n",
    "        return model_results.nlargest(top,criteria+'_'+kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_models(4,'logloss_proba','full')"
   ]
  },
  {
   "source": [
    "## Save our database of performances in a convenient Excel file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_dict_to_excel(model_results)"
   ]
  },
  {
   "source": [
    "## Do some scorings with our selection of models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to rettrieve a specific model record\n",
    "# retrieve_model('nb_common_words+nb_common_words/(nb_words_question1+nb_words_question2)','test')\n",
    "\n",
    "# find best 4 full models based on logloss_proba\n",
    "find_best_models(4,'logloss_proba','full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_docker_cp_command(absolute_file_name):\n",
    "    return 'docker cp '+ DOCKER_IMAGE_NAME+':'+zip_file_name(absolute_file_name)+ ' c:\\\\temp\\\\outputs'\n",
    "\n",
    "def show_kaggle_command(absolute_file_name):\n",
    "    return 'kaggle competitions submit quora-question-pairs -f \"' + zip_file_name(absolute_file_name) +'\" -m \"' + absolute_file_name +'\"'\n",
    "\n",
    "def show_docker_cp_commands(best_results):\n",
    "    print_section('Use these commands to transfer apply results to windows host')\n",
    "    for c in best_results['file_name'].apply(show_docker_cp_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "\n",
    "def show_kaggle_commands(best_results):\n",
    "    print_section('Use these commands to submit apply results to kaggle')\n",
    "    for c in best_results['file_name'].apply(show_kaggle_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "    \n",
    "# return a dataframe fully ready to be converted in csv and published into kaggle\n",
    "def simple_apply(model_key,input_dataframe,kind,proba=True):\n",
    "    model,column_names=retrieve_model(model_key,kind)\n",
    "    input_for_prediction=input_dataframe[column_names]\n",
    "    res = pandas.DataFrame()\n",
    "    if 'test_id' in input_dataframe.columns:\n",
    "        res['test_id']=input_dataframe['test_id']\n",
    "    if proba:\n",
    "        res['is_duplicate']=pandas.Series(model.predict_proba(input_for_prediction)[:,1],name='is_duplicate')\n",
    "    else:\n",
    "        res['is_duplicate']=pandas.Series(model.predict(input_for_prediction),name='is_duplicate')\n",
    "    return res  \n",
    "\n",
    "def submit_model(criteria,kind,input_dataframe,model_key,proba=True,show_how_to_publish=True,kaggle=False):\n",
    "    absolute_file_name_csv = apply_absolute_file_name(criteria,kind,model_key)\n",
    "    print_info('Doing apply')\n",
    "    prediction = simple_apply(model_key,input_dataframe,kind,proba)\n",
    "    print_info('Generating CSV file')\n",
    "    prediction.to_csv(absolute_file_name_csv,index=False)\n",
    "    print_info('Zipping file')\n",
    "    absolute_file_name_zip = zip_file_and_delete(absolute_file_name_csv)\n",
    "    print_info('%s is ready' % absolute_file_name_csv)\n",
    "    if show_how_to_publish:\n",
    "        if kaggle:\n",
    "            print_warning('Use this commands to submit apply results to kaggle')\n",
    "            print_warning(show_kaggle_command(absolute_file_name_zip))\n",
    "        else:\n",
    "            print_warning('Use this command to transfer apply _results to Windows host')\n",
    "            print_warning(show_docker_cp_command(absolute_file_name_csv))\n",
    "\n",
    "\n",
    "\n",
    "def submit_best_models(top,criteria,kind,input_dataframe,proba=True,kaggle=False):\n",
    "    print_section('Submit best %d %s models by %s' % (top,kind,criteria))\n",
    "    best_models = find_best_models(top,criteria,kind)\n",
    "    best_models['model_key']=numpy.asarray(best_models.index)\n",
    "    #best_models['file_name']=numpy.array([apply_file_name(criteria,kind,n) for n in best_models.index])\n",
    "    best_models['file_name']=best_models['model_key'].apply(lambda mk: apply_absolute_file_name(criteria,kind,mk))\n",
    "    best_models['model_key'].apply(lambda mk: submit_model(criteria,kind,input_dataframe,mk,proba=proba,show_how_to_publish=False,kaggle=kaggle))\n",
    "    best_models['docker']=best_models['file_name'].apply(show_docker_cp_command)\n",
    "    best_models['kaggle']=best_models['file_name'].apply(show_kaggle_command)\n",
    "    print_done('Done')\n",
    "    if kaggle:\n",
    "        show_kaggle_commands(best_models)\n",
    "    else:\n",
    "        show_docker_cp_commands(best_models)\n",
    "    return best_models"
   ]
  },
  {
   "source": [
    "## A basic apply"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_apply('nb_common_words+nb_common_words/(nb_words_question1+nb_words_question2)',challenge_dataframe,'full')\n"
   ]
  },
  {
   "source": [
    "## Full apply\n",
    "* Retrieve a set of best models\n",
    "* for each model\n",
    "    * score the model \n",
    "    * save the result in csv\n",
    "    * zip the file\n",
    "    * display the command to submit on kaggle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logloss_proba_test = submit_best_models(3,'logloss_proba','test',challenge_dataframe,kaggle=True)"
   ]
  },
  {
   "source": [
    "# What Kaggle think about our submissions ?\n",
    "## Get back all our submissions to kaggle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_submissions(all_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_submissions(all_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_submissions(all_submissions,metric='publicScore',n=5)"
   ]
  },
  {
   "source": [
    "# Graph our submissions (WIP)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_submissions(dataframe):\n",
    "    print_section( 'Show history of submissions')\n",
    "    for name in ['publicScore']: #all_numeric_columns(dataframe):\n",
    "        yield\n",
    "        display_submission(dataframe)\n",
    "    print_done('Done')\n",
    "\n",
    "def display_submissions_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(display_submissions(dataframe), nb_columns)\n",
    "\n",
    "def auto_label(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        plot.annotate('{}'.format(height),xy=(rect.get_x()+rect.get_width()/2,height),xytext=(0,3),textcoords='offset points',ha='center',va='bottom')\n",
    "\n",
    "\n",
    "# Only useful to show private/public is almost the same\n",
    "# at least now\n",
    "def display_submission_private_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    labels = dataframe['description']\n",
    "    public_scores = dataframe['publicScore']\n",
    "    private_scores = dataframe['privateScore']\n",
    "    x = numpy.arange(len(labels))\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(20, 14))\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    rects_public_score = plot.bar(x-width/2,public_scores,width,label = 'publicScore')\n",
    "    rects_private_score = plot.bar(x+width/2,private_scores,width,label = 'privateScore')\n",
    "    plot.ylabel('Scores')\n",
    "    plot.title('History of scores')\n",
    "    plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.legend()\n",
    "    #auto_label(rects_public_score)\n",
    "    fig.tight_layout()\n",
    "    plot.show()\n",
    "\n",
    "def find_description_test_and_full(dataframe):\n",
    "    list_test = list()\n",
    "    list_full = list()\n",
    "    for i,d in dataframe.iterrows():\n",
    "        #print(d)\n",
    "        if d.description.find('!test!')>=0:\n",
    "            list_test.append(d.description.replace('!test!','!x!'))\n",
    "        if d.description.find('!full!')>=0:\n",
    "            list_full.append(d.description.replace('!full!','!x!'))\n",
    "    list_test_and_full = list(set(list_test)&set(list_full))\n",
    "    list_test = [i.replace('!x!','test') for i in list_test_and_full]\n",
    "    list_full = [i.replace('!x!','full') for i in list_test_and_full]\n",
    "    return [i.replace('!x!','!test!') for i in list_test_and_full]+[i.replace('!x!','!full!') for i in list_test_and_full]\n",
    "    \n",
    "        \n",
    "\n",
    "def display_submission_full_test_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    test_and_full_scores = dataframe\n",
    "    test_scores = test_and_full_scores[test_and_full_scores['description'].str.find('!test!')>=0]['publicScore']\n",
    "    full_scores = test_and_full_scores[test_and_full_scores['description'].str.find('!full!')>=0]['publicScore']\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(15, 10))\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    x = numpy.arange(len(test_scores))\n",
    "    rects_test_score = plot.bar(x-width/2,test_scores,width,label = 'Score on test')\n",
    "    x = numpy.arange(len(full_scores))\n",
    "    rects_full_score = plot.bar(x+width/2,full_scores,width,label = 'Score on full')\n",
    "    plot.ylabel('Scores')\n",
    "    plot.title('History of scores')\n",
    "    plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.legend()\n",
    "    #auto_label(rects_public_score)\n",
    "    fig.tight_layout()\n",
    "    plot.show()\n",
    "\n",
    "# to check full and test models have been properly separated for further usage\n",
    "# res = find_description_test_and_full(all_submissions)\n",
    "\n",
    "# Only useful to show private/public is almost the same\n",
    "# at least now\n",
    "def display_submission_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    x = numpy.arange(len(dataframe['description']))\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(10, 10))\n",
    "    #ax = plot\n",
    "    #fig,ax = plot.subplots()\n",
    "    plot.plot(x,dataframe['publicScore'],label='Public score')\n",
    "    plot.plot(x,dataframe['privateScore'],label='Private score')\n",
    "    plot.ylabel('Score')\n",
    "    plot.xlabel('# Submission')\n",
    "    plot.title('History of scores')\n",
    "    plot.legend()\n",
    "    plot.show()"
   ]
  },
  {
   "source": [
    "### First scores were quite bad : logloss .... *28* !!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Worst submissions based on publicScore')\n",
    "all_submissions.nlargest(2,'publicScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Best submissions based on publicScore')\n",
    "all_submissions.nsmallest(2,'publicScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_submission_public(all_submissions)"
   ]
  }
 ]
}