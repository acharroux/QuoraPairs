{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Playing with stop words\n",
    "\n",
    "Nb common words seems to be an interesting feature.\n",
    "\n",
    "But isn'it disturbed by common words like do, not, and which may precisely be common to many pairs, this without any significance ?\n",
    "\n",
    "So, if we remove stop words, nb common feature may have more significance\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ugly incantation to make our 'framework' working\n",
    "import sys\n",
    "sys.path.insert(0, r'/SAPDevelop/QuoraPairs/BruteForce/Tools')\n",
    "\n",
    "#import all our small tools (paths, cache, print,zip,excel, pandas, progress,..)\n",
    "from Tools.all import *\n",
    "\n",
    "# setup the name of our experiment\n",
    "# it will be used to store every result in a unique place\n",
    "EXPERIMENT='clean_data'\n",
    "# Do a bit of checks before actually running code\n",
    "UNITARY_TEST = True\n",
    "print_alert('You will use environment %s' % EXPERIMENT)\n",
    "\n",
    "prepare_environnement(EXPERIMENT)\n",
    "train_dataframe=load_dataframe(CLEAN_TRAINING_DATA)\n",
    "challenge_dataframe=load_dataframe(CLEAN_CHALLENGE_DATA)\n",
    "print_section('Untouched input data has been loaded. Training: %d lines Challenge: %d lines' % (len(train_dataframe),len(challenge_dataframe)))\n"
   ]
  },
  {
   "source": [
    "## Challenge and training are not equivalent\n",
    "\n",
    "% of duplicate in training is not the same in challenge !\n",
    "We don't have the challenge's answer but by scoring a constant prediction and using logloss definition, we can guess the distribution of duplicates in challenge\n",
    "\n",
    "It's 17.46 %\n",
    "\n",
    "To fix that we can:\n",
    "\n",
    "* duplicate some negative cases in training to match challenge distribution\n",
    "* use weights : each case has a weight !=1 and all algorithms are supposed to be able to deal with that info\n",
    "I choose to use weights (less memory needed ?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the weight for 0 and weight for 1 needed to rebalance dataframe like challenge\n",
    "def balanced_weights(dataframe,expected_positive_ratio):\n",
    "    current_positive_ratio = dataframe['is_duplicate'].sum()/len(dataframe)\n",
    "    weight_for_negative = (1-expected_positive_ratio)/(1-current_positive_ratio)\n",
    "    weight_for_positive = expected_positive_ratio/current_positive_ratio\n",
    "    return weight_for_positive,weight_for_negative\n",
    "    return 1,\n",
    "\n",
    "\n",
    "CHALLENGE_DUPLICATE_PERCENT = 0.1746\n",
    "print_warning('OUPS !! %% of duplicates in train is %.3f. In challenge it is %.3f %%' % (train_dataframe['is_duplicate'].sum()/len(train_dataframe),CHALLENGE_DUPLICATE_PERCENT))\n",
    "\n",
    "# create a new 'weight' column to training dataset\n",
    "# Do not forget to remove this column from features !!!\n",
    "print_warning(\"let's add some weights to rebalance the data\")\n",
    "weight_for_1,weight_for_0 = balanced_weights(train_dataframe,CHALLENGE_DUPLICATE_PERCENT)\n",
    "print_info('Weight for positive case %.3f' % weight_for_1)\n",
    "print_info('Weight for negative case %.3f' % weight_for_0)\n",
    "train_dataframe['weight'] = train_dataframe['is_duplicate'].map( {0:weight_for_0, 1:weight_for_1})\n",
    "\n",
    "assert int(train_dataframe['weight'].sum()/len(train_dataframe)) == 1, \"training dataset has not been properly rebalanced\"\n",
    "print_info(\"Training data set has been properly rebalanced\")\n",
    "print_info('Weights distribution:')\n",
    "train_dataframe['weight'].describe()"
   ]
  },
  {
   "source": [
    "# Ready to start"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our main tool to add feature\n",
    "def add_column_from_columns(dataframe,output_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe.progress_apply(function,axis=1)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "def add_column_from_column(dataframe,output_column_name,input_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe[input_column_name].progress_apply(function)\n",
    "    return dataframe[output_column_name]"
   ]
  },
  {
   "source": [
    "### Step 1: lower case everything\n",
    "\n",
    "Note we definitely get rid of any information related to majus case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_lower_data(dataframe):\n",
    "    print_info('Lower case question1')\n",
    "    dataframe['question1'] = dataframe['question1'].str.lower()\n",
    "    print_info('Lower case question2')\n",
    "    dataframe['question2'] = dataframe['question2'].str.lower()\n",
    "    return dataframe\n",
    "\n",
    "print_section('Before')\n",
    "display(train_dataframe.head(2).transpose())\n",
    "train_dataframe = load_or_build_dataframe('Lower case everything in training','training_lower',build_all_lower_data,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Lower case everything in challenge','challenge_lower',build_all_lower_data,challenge_dataframe)\n",
    "print_section('After')\n",
    "display(train_dataframe.head(2).transpose())\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Step 2 : build our basic features\n",
    "* Nb words in question 1\n",
    "* Nb words in question 2\n",
    "* Nb common words\n",
    "* Nb common words/nb words in question 1\n",
    "* Nb common words/nb words in question 2\n",
    "* Nb non common words in question 1\n",
    "* Nb non common words in question 2\n",
    "* Nb common words/(Nb words in question1 + Nb words in question2)\n",
    "\n",
    "Note this is done on raw question content : no preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_basic_features_one_row(q1,q2):\n",
    "    q1 = set(q1.split())\n",
    "    len_q1 = len(q1)\n",
    "    q2 = set(q2.split())\n",
    "    len_q2 = len(q2)\n",
    "\n",
    "    common = q1&q2\n",
    "    len_common = len(common)\n",
    "\n",
    "    uncommon_q1 = q1-common\n",
    "    len_uncommon_q1 = len(uncommon_q1)\n",
    "\n",
    "    uncommon_q2 = q2-common\n",
    "    len_uncommon_q2 = len(uncommon_q2)\n",
    "    #       0     1           2            3         4               5               6      7      8                        9                        10\n",
    "    return common,uncommon_q1,uncommon_q2,len_common,len_uncommon_q1,len_uncommon_q2,len_q1,len_q2,len_common/max(1,len_q1),len_common/max(1,len_q2),len_common/(len_q1+len_q2)\n",
    "    \n",
    "def build_all_basic_features(dataframe):\n",
    "    print_warning('Compute all features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: build_basic_features_one_row(r.question1,r.question2))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'nb_common_words/nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'nb_common_words/nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'nb_words_question1-common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'nb_words_question2-common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'nb_common_words/(nb_words_question1+nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training data + basic features','training_basic_features',build_all_basic_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge data + basic features','challenge_basic_features',build_all_basic_features,challenge_dataframe)\n"
   ]
  },
  {
   "source": [
    "Let's take a look in our new data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.head(2).transpose()"
   ]
  },
  {
   "source": [
    "Let's take a look in common words & uncommon_words_question1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe[['common_words','uncommon_words_question1']]"
   ]
  },
  {
   "source": [
    "common words is 'polluted' by tons of word like is,with,to,...  stop words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Step 3 : remove a first set of stop words\n",
    "\n",
    "We start with stopwords coming from nltk\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def build_no_stopwords_features_one_row(q1,q2,stopwords):\n",
    "    q1 = set([w for w in q1.split() if w not in stopwords])\n",
    "    len_q1 = len(q1)\n",
    "    q2 = set([w for w in q2.split() if w not in stopwords])\n",
    "    len_q2 = len(q2)\n",
    "\n",
    "    common = q1&q2\n",
    "    len_common = len(common)\n",
    "\n",
    "    uncommon_q1 = q1-common\n",
    "    len_uncommon_q1 = len(uncommon_q1)\n",
    "\n",
    "    uncommon_q2 = q2-common\n",
    "    len_uncommon_q2 = len(uncommon_q2)\n",
    "    #       0     1           2            2         4               5               6      7      8                        9                        10\n",
    "    return common,uncommon_q1,uncommon_q2,len_common,len_uncommon_q1,len_uncommon_q2,len_q1,len_q2,len_common/max(1,len_q1),len_common/max(1,len_q2),len_common/max(1,(len_q1+len_q2))\n",
    "    \n",
    "\n",
    "def build_nltk_stop_words_features(dataframe):\n",
    "    print_warning('Remove stopwords from question 1 & question2')\n",
    "    \n",
    "    print_warning('Compute all nltk features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: build_no_stopwords_features_one_row(r.question1,r.question2,nltk_stopwords))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words/nltk_nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words/nltk_nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question1-nltk_common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question2-nltk_common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nltk_common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'nltk_uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'nltk_uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "print_info( 'There is %d stopwords in nltk' % len(nltk_stopwords))\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training: Build features aware of nltk stopwords','training_nltk_stop_words_features',build_nltk_stop_words_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge: Build features aware of nltk stopwords','challenge_nltk_stop_words_features',build_nltk_stop_words_features,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "Did we change anything ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_changed_train = int(numpy.where(train_dataframe['nltk_nb_common_words']!=train_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('We have changed %.2f %% of nb common_words in training!!' % (nb_changed_train*100./len(train_dataframe)))\n",
    "nb_changed_challenge = int(numpy.where(challenge_dataframe['nltk_nb_common_words']!=challenge_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('We have changed %.2f %% of nb common_words in challenge !!' % (nb_changed_challenge*100./len(challenge_dataframe)))\n",
    "train_dataframe[['common_words','nltk_common_words','uncommon_words_question1','nltk_uncommon_words_question1']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.visualization import hist as as_hist\n",
    "\n",
    "#plot.figure(figsize=(15, 10))\n",
    "#sns.histplot(train_dataframe['nb_common_words'],stat='count',bins=50,label='no stopwords',legend=True)\n",
    "#sns.histplot(train_dataframe['nltk_nb_common_words'],stat='count',color=\"g\",alpha=0.5,bins=50,label='nltk stopwords',legend=True)\n",
    "#plot.legend()\n",
    "\n",
    "plot.figure(figsize=(15, 10))\n",
    "as_hist(train_dataframe['nltk_nb_common_words'],label=\"Nb common words without nltk stopwords\",histtype='stepfilled',bins='blocks',density=True,color=\"r\")\n",
    "as_hist(train_dataframe['nb_common_words'],label=\"Nb common words\",histtype='step',alpha=0.5,lw=5,bins='blocks',density=True,color=\"g\")\n",
    "plot.title('Training: Distribution of Nb common words with or without nltk stopwords', fontsize=15)\n",
    "plot.xlabel('Nb common words')\n",
    "plot.ylabel('Prob')\n",
    "plot.legend()\n",
    "\n",
    "plot.figure(figsize=(15, 10))\n",
    "as_hist(challenge_dataframe['nltk_nb_common_words'],label=\"Nb common words without nltk stopwords\",histtype='stepfilled',bins='blocks',density=True,color=\"r\")\n",
    "as_hist(challenge_dataframe['nb_common_words'],label=\"Nb common words\",histtype='step',alpha=0.25,lw=5,bins='blocks',density=True,color=\"g\")\n",
    "plot.title('Challenge: Distribution of Nb common words with or without nltk stopwords', fontsize=15)\n",
    "plot.xlabel('Nb common words')\n",
    "plot.ylabel('Prob')\n",
    "plot.legend()"
   ]
  },
  {
   "source": [
    "### Step 4 : more stopwords\n",
    "we are using also stopwords from sklearn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "sk_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "print_info('There is %d words in nltk stop words' % len(nltk_stopwords))\n",
    "print_info('There is %d words in sklearn stop words' % len(sk_stopwords))\n",
    "new_stopwords = [w for w in sk_stopwords if w not in nltk_stopwords]\n",
    "print_info('There is %d new stop words in sklearn stop words' % len(new_stopwords))\n",
    "\n",
    "all_stop_words = nltk_stopwords | sk_stopwords\n",
    "print_info('There is %d words in the union of stop words' % len(all_stop_words))\n",
    "\n",
    "def build_all_stop_words_features(dataframe): \n",
    "    print_warning('Compute all features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: build_no_stopwords_features_one_row(r.question1,r.question2,all_stop_words))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "  \n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words/all_nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words/all_nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question1-all_common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question2-all_common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words/(all_nb_words_question1+all_nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'all_common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'all_uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'all_uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training: Build features aware of all stopwords','training_all_stop_words_features',build_all_stop_words_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge: Build features aware of all stopwords','challenge_all_stop_words_features',build_all_stop_words_features,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "Recap our changes : last step looks like small ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_changed_train = int(numpy.where(train_dataframe['nltk_nb_common_words'] != train_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('With nltk stop words we have changed %.2f %% of nb common_words in training!!' % (nb_changed_train*100./len(train_dataframe)))\n",
    "nb_changed_train = int(numpy.where(train_dataframe['all_nb_common_words'] != train_dataframe['nb_common_words'],1,0).sum())\n",
    "print_warning('Removing also sklearn stop words we have changed %.2f %% of nb common_words in training!!' % (nb_changed_train*100./len(train_dataframe)))\n",
    "\n",
    "nb_changed_challenge = int(numpy.where(challenge_dataframe['nltk_nb_common_words']!=challenge_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('With nltk stop words We have changed %.2f %% of nb common_words in challenge !!' % (nb_changed_challenge*100./len(challenge_dataframe)))\n",
    "nb_changed_challenge = int(numpy.where(challenge_dataframe['all_nb_common_words']!=challenge_dataframe['nb_common_words'],1,0).sum())\n",
    "print_warning('Removing also sklearn stop words we have changed %.2f %% of nb common_words in challenge !!' % (nb_changed_challenge*100./len(challenge_dataframe)))\n",
    "\n",
    "changed = train_dataframe[train_dataframe['nltk_nb_common_words']!=train_dataframe['all_nb_common_words']]\n",
    "changed[['common_words','nltk_common_words','all_common_words','uncommon_words_question1','nltk_uncommon_words_question1','all_uncommon_words_question1']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot.figure(figsize=(15, 10))\n",
    "#sns.histplot(train_dataframe['nb_common_words'],stat='count',bins=50,label='no stopwords',legend=True)\n",
    "#sns.histplot(train_dataframe['nltk_nb_common_words'],stat='count',color=\"g\",alpha=0.5,bins=50,label='nltk stopwords',legend=True)\n",
    "#plot.legend()\n",
    "\n",
    "plot.figure(figsize=(15, 10))\n",
    "as_hist(train_dataframe['all_nb_common_words'],label=\"Nb common words without nltk stopwords\",histtype='stepfilled',bins='blocks',density=True,color=\"r\")\n",
    "as_hist(train_dataframe['nltk_nb_common_words'],label=\"Nb common words without all stopwords\",histtype='step',bins='blocks',density=True,color=\"b\",lw=3)\n",
    "as_hist(train_dataframe['nb_common_words'],label=\"Nb common words\",histtype='step',alpha=0.5,lw=5,bins='blocks',density=True,color=\"g\")\n",
    "plot.title('Training: Distribution of Nb common words with or without nltk/all stopwords', fontsize=15)\n",
    "plot.xlabel('Nb common words')\n",
    "plot.ylabel('Prob')\n",
    "plot.legend()"
   ]
  },
  {
   "source": [
    "### Did you get this strange % of change in challenge ??\n",
    "\n",
    "We have changed 86.44% of common word in train data\n",
    "Applying same proces, we changed only 77.67% of challenge data\n",
    "\n",
    "Let's dig a little bit\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_dataframe[challenge_dataframe['all_nb_words_question2-all_common_words']>5].sample(1000,random_state=46)['all_uncommon_words_question1']"
   ]
  },
  {
   "source": [
    "### OK start to see the pbm\n",
    "* all question marks at end must be removed\n",
    "* some weird \" in words\n",
    "* maybe some typos like a digit or a punctuation in words\n",
    "\n",
    "Let's see how are the really unknown words ie not present in english dictionary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from english_words import english_words_lower_alpha_set\n",
    "\n",
    "def load_english_words():\n",
    "    with open(absolute_env_file_name('../words_alpha',ext='.txt')) as word_file:\n",
    "        valid_words = set(word_file.read().split())\n",
    "    print_info('size of first dictionary %d' % len(valid_words))\n",
    "    print_info('size of second dictionary %d' % len(english_words_lower_alpha_set))\n",
    "    valid_words = valid_words | english_words_lower_alpha_set\n",
    "    print_info('size of merged dictionary %d' % len(valid_words))\n",
    "    return valid_words\n",
    "\n",
    "print_info('Load a big dictionary of English words')\n",
    "english_words = load_english_words()\n",
    "print_info('Dictionary of english words contains %d words' % len(english_words))\n",
    "print_info(\"Let's find unshared words that are fully alpha and not in dictionary. Candidate for typos ?\")\n",
    "\n",
    "train_dataframe['unknown_question1'] = train_dataframe['all_uncommon_words_question1'].progress_apply(lambda x: [ w for w in x if w not in english_words and not w.isalpha() ])\n",
    "train_dataframe['nb_unknown_question1'] = train_dataframe['unknown_question1'].progress_apply(len)\n",
    "train_dataframe['unknown_question2'] = train_dataframe['all_uncommon_words_question2'].progress_apply(lambda x: [ w for w in x if w not in english_words and not w.isalpha() ])\n",
    "train_dataframe['nb_unknown_question2'] = train_dataframe['unknown_question2'].progress_apply(len)\n",
    "\n",
    "challenge_dataframe['unknown_question1'] = challenge_dataframe['all_uncommon_words_question1'].progress_apply(lambda x: [ w for w in x if w not in english_words and not w.isalpha() ])\n",
    "challenge_dataframe['nb_unknown_question1'] = challenge_dataframe['unknown_question1'].progress_apply(len)\n",
    "challenge_dataframe['unknown_question2'] = challenge_dataframe['all_uncommon_words_question2'].progress_apply(lambda x: [ w for w in x if w not in english_words and not w.isalpha() ])\n",
    "challenge_dataframe['nb_unknown_question2'] = challenge_dataframe['unknown_question2'].progress_apply(len)\n",
    "\n"
   ]
  },
  {
   "source": [
    "Do a Kolmogorov-Smirnow to see if distribution of unknown words is different in train and challenge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "print_section('Basic stats on len of unshared & unknown words')\n",
    "print_warning('Train Nb unknown words in question 1 mean %.3f std: %3.f max %d' %(train_dataframe['nb_unknown_question1'].mean(),train_dataframe['nb_unknown_question1'].std(),train_dataframe['nb_unknown_question1'].max() ))\n",
    "print_warning('%.3f %% of lines have such words' % (train_dataframe['nb_unknown_question1'].where(train_dataframe['nb_unknown_question1']>0).count()/len(train_dataframe)))\n",
    "print_warning('Train Nb unknown words in question 2 mean %.3f std: %3.f max %d' %(train_dataframe['nb_unknown_question2'].mean(),train_dataframe['nb_unknown_question2'].std(),train_dataframe['nb_unknown_question1'].max() ))\n",
    "print_warning('%.3f %% of lines have such words' % (train_dataframe['nb_unknown_question2'].where(train_dataframe['nb_unknown_question2']>0).count()/len(train_dataframe)))\n",
    "\n",
    "print_warning('Challenge Nb unknown words in question 1 mean %.3f std: %3.f max %d' %(challenge_dataframe['nb_unknown_question1'].mean(),challenge_dataframe['nb_unknown_question1'].std(),challenge_dataframe['nb_unknown_question1'].max() ))\n",
    "print_warning('%.3f %% of lines have such words' % (challenge_dataframe['nb_unknown_question1'].where(challenge_dataframe['nb_unknown_question1']>0).count()/len(challenge_dataframe)))\n",
    "print_warning('Challenge Nb unknown words in question 2 mean %.3f std: %3.f max %d' %(challenge_dataframe['nb_unknown_question2'].mean(),challenge_dataframe['nb_unknown_question2'].std(),challenge_dataframe['nb_unknown_question1'].max() ))\n",
    "print_warning('%.3f %% of lines have such words' % (challenge_dataframe['nb_unknown_question2'].where(challenge_dataframe['nb_unknown_question2']>0).count()/len(challenge_dataframe)))\n",
    "\n",
    "print()\n",
    "print_section('Using a Kolmogorov-Smirnow test')\n",
    "print_info('Null hypothesis : the 2 distributions of nb unknown words in question 1 are similar')\n",
    "print_warning(str(ks_2samp(train_dataframe['nb_unknown_question1'],challenge_dataframe['nb_unknown_question1'])))\n",
    "print_alert('stat is small but p-value is 0: we reject null hypothesis: distributions are not similar')\n",
    "print_info('Null hypothesis : the 2 distributions of nb unknown words in question 2 are similar')\n",
    "print_warning(str(ks_2samp(train_dataframe['nb_unknown_question2'],challenge_dataframe['nb_unknown_question2'])))\n",
    "print_alert('stat is small but p-value is 0: we reject null hypothesis: distributions are not similar')\n",
    "\n",
    "print_alert('challenge is not using the same vocabulary as training')\n",
    "\n"
   ]
  },
  {
   "source": [
    "Maybe it's just because challenge's question are longer ?\n",
    "\n",
    "redo it on ratio nb_unknown/nb_total\n",
    "\n",
    "Answer still no"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_warning(str(ks_2samp(train_dataframe['nb_unknown_question1']/train_dataframe['all_nb_words_question1'],challenge_dataframe['nb_unknown_question1']/challenge_dataframe['all_nb_words_question1'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to keep : most efficient way of computing a complex column with tests ?\n",
    "# train_dataframe['all_nb_words_question1_ratio'] = 0.0\n",
    "#train_dataframe.loc[(train_dataframe['nb_words_question1']>0.),'all_nb_words_question1_ratio'] = train_dataframe['all_nb_words_question1']/train_dataframe['nb_words_question1']train_dataframe['all_nb_words_question1_ratio']\n"
   ]
  },
  {
   "source": [
    "## Compute AUC of these basic features and try to figure if there is a bit of information inside each one\n",
    "\n",
    "ie is it helping to separate 1 from 0?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(y_true=dataframe['is_duplicate'],y_score=dataframe[column_name])\n",
    "\n",
    "def simple_weighted_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(y_true=dataframe['is_duplicate'],y_score=dataframe[column_name],sample_weight=dataframe['weight'])\n",
    "\n",
    "def show_AUC(dataframe,column_name):\n",
    "    if 'weight' in dataframe.columns:\n",
    "        print_bullet('Weighted AUC %s %f' % (column_name,simple_weighted_AUC(dataframe,column_name)))\n",
    "    else:\n",
    "        print_bullet('AUC %s %f' % (column_name,simple_AUC(dataframe,column_name)))\n",
    "        \n",
    "\n",
    "def display_simple_AUC(dataframe,column_name):\n",
    "    palette = sns.color_palette()\n",
    "    # Let multiplot_generator figure the size\n",
    "    #plot.figure(figsize=(10, 7))\n",
    "    #plot.hist(dataframe[column_name][dataframe['is_duplicate']==1],bins=50,color=palette[3],label='Same',histtype='step')\n",
    "    #plot.hist(dataframe[column_name][dataframe['is_duplicate']==0],bins=50,color=palette[2],label='Different',alpha = 0.75,histtype='step')\n",
    "    as_hist(dataframe[column_name][dataframe['is_duplicate']==1],bins=\"blocks\",color=palette[3],label='Same',histtype='step')\n",
    "    as_hist(dataframe[column_name][dataframe['is_duplicate']==0],bins=\"blocks\",color=palette[2],label='Different',alpha = 0.75,histtype='step')\n",
    "    plot.title('AUC %s : %f' % (column_name,simple_AUC(dataframe,column_name)) , fontsize=10)\n",
    "    plot.xlabel(column_name)\n",
    "    plot.ylabel('Nb')\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "\n",
    "def show_all_simple_AUC(dataframe):\n",
    "    all =  all_numeric_columns(dataframe)\n",
    "    print_section( 'Show AUC on %d unique features' % len(all))\n",
    "    for name in all:\n",
    "        show_AUC(dataframe,name)\n",
    "        yield\n",
    "        display_simple_AUC(dataframe,name)\n",
    "    print_done('Done')\n",
    "\n",
    "\n",
    "def show_all_simple_AUC_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(show_all_simple_AUC(dataframe), nb_columns)\n",
    "\n",
    "show_all_simple_AUC_in_grid(train_dataframe,nb_columns=2)"
   ]
  },
  {
   "source": [
    "## OK Visualising these AUC is cool\n",
    "Can we have some numbers ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def graph_relative_feature_importance(features_importances,columns):\n",
    "    # Make importances relative to max importance.\n",
    "    features_importances = 100.0 * (features_importances / features_importances.max())\n",
    "    sorted_idx = numpy.argsort(features_importances)\n",
    "    sorted_idx = sorted_idx[-20:-1:1]\n",
    "    pos = numpy.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plot.barh(pos, features_importances[sorted_idx], align='center')\n",
    "    plot.yticks(pos, columns)\n",
    "    plot.xlabel('Relative Importance')\n",
    "    plot.title('Relative Feature Importance', fontsize=30)\n",
    "    plot.tick_params(axis='x', which='major', labelsize=15)\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    plot.show()\n",
    "\n",
    "def graph_feature_importance(features_importances,columns):\n",
    "    # Make importances relative to max importance.\n",
    "    sorted_idx = numpy.argsort(features_importances)\n",
    "    sorted_idx = sorted_idx[-20:]\n",
    "    pos = numpy.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plot.barh(pos, features_importances[sorted_idx], align='center')\n",
    "    plot.yticks(pos, columns)\n",
    "    plot.xlabel('Importance')\n",
    "    plot.title('Feature Importance', fontsize=30)\n",
    "    plot.tick_params(axis='x', which='major', labelsize=15)\n",
    "    #sns.despine(left=True, bottom=True)\n",
    "    plot.show()\n",
    "\n",
    "def independant_features_analysis(name_data,input_df,target_df):\n",
    "    train_df = input_df[all_numeric_columns(input_df)]\n",
    "    target_df = target_df\n",
    "    small = int(len(target_df)*0.05)\n",
    "    small_train = train_df.sample(small,random_state=42)\n",
    "    small_target = target_df.sample(small,random_state=42)\n",
    "    small_weights = input_df[WEIGHT].sample(small,random_state=42)\n",
    "\n",
    "    # be careful all_numeric will include also the target soon\n",
    "    all_numeric = all_numeric_columns(input_df)\n",
    "    print_section('Minimal analysis of numeric features of %s' % name_data)\n",
    "    print_info('Nb features: %d' % len(all_numeric))\n",
    "    print_info(str(all_numeric))\n",
    "    print_warning('Is there any null value ? %s' % input_df.isnull().any().any())\n",
    "\n",
    "    compute_variances = feature_selection.VarianceThreshold()\n",
    "    all_numeric.append('is_duplicate')\n",
    "    variances = compute_variances.fit_transform(input_df[all_numeric])\n",
    "    print_warning('Is there any low variance feature ? %s' % str(variances.shape[1]!=len(all_numeric)))\n",
    "    print_info('Here are the correlations to the target \"is_duplicate\"')\n",
    "    correlation_matrix = input_df[all_numeric].corr().abs()\n",
    "    print(correlation_matrix['is_duplicate'].nlargest(len(all_numeric)))\n",
    "\n",
    "    correlation_matrix_without_is_duplicate = correlation_matrix['is_duplicate'][correlation_matrix['is_duplicate']<1]\n",
    "    highest_correlated_feature = correlation_matrix_without_is_duplicate.nlargest(1).index[0]\n",
    "    highest_correlation = correlation_matrix_without_is_duplicate.nlargest(1)[0]\n",
    "    if highest_correlation >0.2:\n",
    "        if highest_correlation < 0.5:\n",
    "            print_warning('%s is the most correlated with target but %f is quite weak' % (highest_correlated_feature,highest_correlation))\n",
    "        else:\n",
    "            print_info('%s is the most correlated with target but %f is very weak' % (highest_correlated_feature,highest_correlation))\n",
    "    else:\n",
    "        print_alert('%s is the most correlated with target and %f is quite big' % (highest_correlated_feature,highest_correlation))\n",
    "    plot.figure(figsize=(15, 10))\n",
    "    sns.heatmap(correlation_matrix,annot=True,cbar=True,square=True,cmap='coolwarm',mask = numpy.triu(correlation_matrix))\n",
    "    plot.show()\n",
    "\n",
    "    # we keep only numeric features and remove is_duplicate\n",
    "    all_numeric = all_numeric_columns(input_df)\n",
    "\n",
    "    print_section('Here are the features that would be selected with a simple univariate analysis')\n",
    "    start = time.time()\n",
    "    for r in tqdm(range(1,len(all_numeric))):\n",
    "        k_best = SelectKBest(score_func=f_classif,k=r)\n",
    "        fit = k_best.fit(small_train,small_target)\n",
    "        best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "        print_info('Nb features to keep %d:%s' %(r,best_columns))\n",
    "        \n",
    "        # Can we graph relative importances\n",
    "        if r>1:\n",
    "            # transform train set so we can compute importances\n",
    "            univariate_features = fit.transform(small_train)\n",
    "            rfc = RandomForestClassifier(n_estimators=100)\n",
    "            rfc_scores = cross_val_score(rfc, univariate_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "            features_importance = rfc.fit(univariate_features, small_target).feature_importances_\n",
    "            graph_relative_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "    \n",
    "    print_section('Here are the features that would be selected a priori from default Multinomial Naive Bayes')\n",
    "    start = time.time()   \n",
    "    mnb = MultinomialNB()\n",
    "    best_from_model = feature_selection.SelectFromModel(mnb)\n",
    "    fit = best_from_model.fit(small_train,small_target,sample_weight=small_weights)\n",
    "    print(fit.get_support(indices=True))\n",
    "    best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "    # transform train set so we can compute importances\n",
    "    mnb_features = fit.transform(small_train)\n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc_scores = cross_val_score(rfc, mnb_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "    features_importance = rfc.fit(mnb_features, small_target).feature_importances_\n",
    "    print(features_importance)\n",
    "    print_info('features to keep :%s logloss %.4f %.4f' %(best_columns,-rfc_scores.mean(),rfc_scores.std()))\n",
    "    graph_relative_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "    \n",
    "\n",
    "    print_info('Try to keep x \\% of variance with a PCA')\n",
    "    print_alert('bug ?')\n",
    "    start = time.time()\n",
    "    for r in tqdm([.8,.9,.95]):\n",
    "        acp = PCA(r)\n",
    "        principal_components = acp.fit_transform(small_train)\n",
    "        principal_df = pandas.DataFrame(principal_components).sample(small,random_state=42)\n",
    "        rfc = RandomForestClassifier(n_estimators=100)\n",
    "        rfc_scores = cross_val_score(rfc, principal_df, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "        features_importance = rfc.fit(principal_df, small_target).feature_importances_\n",
    "        print_info('%% of variance %f:%s' %(r,'aie'))\n",
    "        graph_relative_feature_importance(features_importance,small_train.columns)\n",
    "    print_done('Done',top=start)\n",
    "\n",
    "    print_section('Here are the features that would be selected with a recursive feature elimination')\n",
    "    start = time.time()\n",
    "    print_alert('Doomed to fail : RFE does not support to transmit ,sample_weight=small_weights')\n",
    "    print_alert('ix is waiting since 2016 ?')\n",
    "    for r in tqdm(range(1,len(all_numeric))):\n",
    "        mnb = MultinomialNB()\n",
    "        recursive_best= feature_selection.RFE(mnb, n_features_to_select=r)\n",
    "        fit = recursive_best.fit(small_train,small_target)\n",
    "        best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "        print_info('Nb features to keep %d:%s' %(r,best_columns))\n",
    "        \n",
    "        # Can we graph relative importances\n",
    "        if r>1:\n",
    "            # transform train set so we can compute importances\n",
    "            recursive_features = fit.transform(small_train)\n",
    "            rfc = RandomForestClassifier(n_estimators=100)\n",
    "            rfc_scores = cross_val_score(rfc, recursive_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "            features_importance = rfc.fit(recursive_features, small_target).feature_importances_\n",
    "            graph_relative_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "   \n",
    "\n",
    "# !!!!! a little bit long\n",
    "#independant_features_analysis('train',train_dataframe,train_dataframe['is_duplicate'])\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Previous exploration of all combination of features was not very successfull and very heavy\n",
    "### Let's try another strategy : on a given 'theme' of fields, add each field one by one to a model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Some code to wrap XGBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple XGBoost code\n",
    "# \n",
    "\n",
    "import xgboost \n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta' : 0.02,\n",
    "    'max_depth':3 \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def print_res_xgboost(infos):\n",
    "    # Glurk\n",
    "    print_info(' | '.join([('%s '+f) % (k,infos[k]) for k,f in INFO_XGBOOST_MODEL.items() if k in infos]))\n",
    "\n",
    "# 80% training 20% validation\n",
    "def build_XGBoost_model_80_20(training,target,column_names,show=True):\n",
    "    input_train,input_test,target_train,target_test = train_test_split(training,target,random_state=42,test_size=0.2)\n",
    "    final_input_train = input_train[list(column_names)]\n",
    "    final_input_test = input_test[list(column_names)]\n",
    "    train_dm = xgboost.DMatrix(final_input_train, label = target_train, weight = input_train[WEIGHT],nthread = -1)\n",
    "    validation_dm = xgboost.DMatrix(final_input_test, label = target_test,weight = input_test[WEIGHT])\n",
    "    watchlist = [(train_dm, 'train'), (validation_dm, 'valid')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(params,train_dm,400,watchlist,early_stopping_rounds=50,verbose_eval=verbose)\n",
    "    infos = compute_metrics_model_xgboost(model,final_input_test,target_test,sample_weight = input_test[WEIGHT],show=show)\n",
    "    duration = time.time()-start\n",
    "    infos.update({'time':duration})\n",
    "    if show:      \n",
    "        print_res_xgboost(infos)\n",
    "    return  infos\n",
    "\n",
    "# Pushing the limits :100 % training no validation !!\n",
    "def build_XGBoost_model_100_0(training,target,column_names,show=True):\n",
    "    final_train = training[list(column_names)]\n",
    "    final_target = target\n",
    "    final_weight = training[WEIGHT]\n",
    "\n",
    "    train_dm = xgboost.DMatrix(final_train, label = final_target, weight = final_weight,nthread = -1)\n",
    "    watchlist = [(train_dm, 'train')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(params,train_dm,400,watchlist,early_stopping_rounds=50,verbose_eval=verbose)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model_xgboost(model,final_train,final_target,sample_weight = final_weight,show=show)\n",
    "    infos.update({'time':duration})\n",
    "    if show:      \n",
    "        print_res_xgboost(infos)\n",
    "    return  infos\n",
    "\n",
    "def compute_metrics_model_xgboost(model,input_df,target_df,sample_weight = None,show = True):\n",
    "    final_input = xgboost.DMatrix(input_df)\n",
    "    prediction_proba_df = model.predict(final_input,ntree_limit=model.best_ntree_limit)\n",
    "    # Hum shouldn't we challenge this 50% threshold ?\n",
    "    prediction_df = numpy.where(prediction_proba_df>0.5,1,0)\n",
    "    res = metrics.classification_report(target_df,prediction_df,sample_weight = sample_weight,output_dict=True)\n",
    "    accuracy = res['accuracy']\n",
    "    score = res['weighted avg']['f1-score']\n",
    "    logloss_proba = metrics.log_loss(target_df,prediction_proba_df,sample_weight = sample_weight)\n",
    "    if show:\n",
    "        print_info('Classification report')\n",
    "        print(metrics.classification_report(target_df,prediction_df,sample_weight = sample_weight))\n",
    "    return {\n",
    "             'accuracy':accuracy,\n",
    "             'score':score,\n",
    "             'logloss_proba':logloss_proba,\n",
    "             'model':model\n",
    "           }\n",
    "\n",
    "INFO_XGBOOST_MODEL= {\n",
    "    'logloss_proba': '%.4f',\n",
    "    'score': '%.4f',\n",
    "    'accuracy': '%.4f',\n",
    "    'time': '%.2f'\n",
    "}\n",
    "\n",
    "print_res_xgboost({'alain':1})\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "    small_train = train_dataframe.sample(1000,random_state=42)\n",
    "    print_section('Unitary test : playbox XGBoost 100_0')\n",
    "    res = build_XGBoost_model_100_0(small_train,small_train['is_duplicate'],all_numeric_columns(small_train))\n",
    "    print(res)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "    small_train = train_dataframe.sample(1000,random_state=42)\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "    res = build_XGBoost_model_80_20(small_train,small_train['is_duplicate'],all_numeric_columns(small_train))\n",
    "    print(res)"
   ]
  },
  {
   "source": [
    "Some code to do some campaign and browse results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_result(results,metric):\n",
    "    if 'logloss' in metric:\n",
    "        return results.nsmallest(1,metric)\n",
    "    else:\n",
    "        return results.nlargest(1,metric)\n",
    "        \n",
    "def apply_xgboost(model,columns,input_df):\n",
    "    res = pandas.DataFrame()\n",
    "    res['test_id'] = input_df['test_id']\n",
    "    res['is_duplicate'] = model.predict(xgboost.DMatrix(input_df[columns]))\n",
    "    return res\n",
    "\n",
    "def submit_best_result(results,input_df,file_name,kaggle_message,metric='logloss_proba'):\n",
    "    start = time.time()\n",
    "    print_section(\"Scoring XGBoost model with best %s: %s\" % (metric,kaggle_message))\n",
    "    csv_file_name = absolute_env_file_name(file_name,ext='.csv')\n",
    "    print_info('Generating scores in %s' % csv_file_name)\n",
    "    best = find_best_result(results,metric)\n",
    "    model = best['model'][0]\n",
    "    columns = best['columns'][0]\n",
    "    prediction = apply_xgboost(model,columns,input_df)\n",
    "    prediction.to_csv(csv_file_name,index=False)\n",
    "    print_info('Zipping file')\n",
    "    absolute_file_name_zip = zip_file_and_delete(csv_file_name)\n",
    "    print_done('Done',top=start)\n",
    "    print_info('%s is ready' % absolute_file_name_zip)\n",
    "    print_warning('Use this commands to submit apply results to kaggle')\n",
    "    print_warning('kaggle competitions submit quora-question-pairs -f \"%s\" -m \"%s %s\"' % (absolute_file_name_zip,EXPERIMENT,kaggle_message))\n",
    "    return prediction\n",
    "\n",
    "def n_columns(columns,n):\n",
    "    return columns[0:n]\n",
    "\n",
    "def xgboost_all_fields_80_20(dataframe,columns_to_explore,explore=True):\n",
    "    if explore:\n",
    "        print_section(\"XGBoost (80,20) on 1 to %d fields\" % len(columns_to_explore))\n",
    "    else:\n",
    "        print_section(\"XGBoost (80,20) on %d fields\" % len(columns_to_explore))\n",
    "    start = time.time()\n",
    "    res_final = dict()\n",
    "    min_log_loss = 1000\n",
    "    best = 0\n",
    "    best_cols =''\n",
    "    # a shortcut is possible\n",
    "    if explore:\n",
    "        first_nb = 1\n",
    "    else:\n",
    "        first_nb=len(columns_to_explore)\n",
    "    for i in tqdm(range(first_nb,len(columns_to_explore)+1)):\n",
    "        columns = n_columns(columns_to_explore,i)\n",
    "        res = build_XGBoost_model_80_20(dataframe,dataframe['is_duplicate'],columns,show = False)\n",
    "        cur_logloss = res['logloss_proba']\n",
    "        if cur_logloss<min_log_loss:\n",
    "            print_info(\"%d:%.4f %s\" % (i,cur_logloss,columns))\n",
    "            min_log_loss = cur_logloss\n",
    "            best_cols = columns\n",
    "            best = i\n",
    "        else:\n",
    "            print_warning(\"%d:%.4f\" % (i,cur_logloss))\n",
    "        res.update( {'columns':columns})\n",
    "        res_final.update({str(i):res})\n",
    "    print_info('Best: %d %.4f %s' % (best,min_log_loss,best_cols))\n",
    "    print_done('Done',top=start)\n",
    "    return pandas.DataFrame.from_dict(res_final, orient='index')\n",
    "\n",
    "# try to reload from cache an exploration\n",
    "# if not available, redo it and save results in cache\n",
    "# find the best result according to a metric (default logloss_proba)\n",
    "# and submit it to kaggle\n",
    "def study_fields(message,tag,columns,train_df,challenge_df,explore=True):\n",
    "    print_section(message)\n",
    "    results = load_or_build_dataframe('Rebuild XGBoost models',tag + '_results',lambda df:xgboost_all_fields_80_20(df,columns,explore=explore),train_df)\n",
    "    save_models_dict_to_excel(results,tag=tag)\n",
    "    display(results.describe())\n",
    "    submit_best_result(results,challenge_df,'Best_'+tag,'Best ' + tag)\n",
    "    return results\n"
   ]
  },
  {
   "source": [
    "### Let's start our exploration with XGBoost (basic setup)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's define our 'domains'\n",
    "* All features without any preprocessing except lowercasing\n",
    "* All features aware of nltk list of stop words\n",
    "* All features aware of extended list of stop words\n",
    "* All 'not in english dictionary'\n",
    "* All features except dict ones\n",
    "* All features\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ALL_STOP_COLUMNS= ['no_stopword_nb_common_words', 'no_stopword_nb_common_words/(no_stopword_nb_words_question1+no_stopword_nb_words_question2)',       'no_stopword_nb_common_words/no_stopword_nb_words_question1', 'no_stopword_nb_common_words/no_stopword_nb_words_question2', 'no_stopword_nb_words_question1',       'no_stopword_nb_words_question1-no_stopword_common_words', 'no_stopword_nb_words_question2', 'no_stopword_nb_words_question2-no_stopword_common_words']\n",
    "\n",
    "# ALL_NO_STOP_COLUMNS= ['no_stopword_nb_common_words', 'no_stopword_nb_common_words/(no_stopword_nb_words_question1+no_stopword_nb_words_question2)',       'no_stopword_nb_common_words/no_stopword_nb_words_question1', 'no_stopword_nb_common_words/no_stopword_nb_words_question2', 'no_stopword_nb_words_question1',       'no_stopword_nb_words_question1-no_stopword_common_words', 'no_stopword_nb_words_question2', 'no_stopword_nb_words_question2-no_stopword_common_words']\n",
    "\n",
    "# ALL_COLUMNS =[ 'nb_words_question1', 'nb_words_question2', 'nb_common_words', 'nb_common_words/nb_words_question1','nb_common_words/nb_words_question2', 'nb_words_question1-common_words', 'nb_words_question2-common_words', 'nb_common_words/(nb_words_question1+nb_words_question2)', no_stopword_nb_words_question1', 'no_stopword_nb_words_question2', 'no_stopword_nb_common_words', 'no_stopword_nb_common_words/no_stopword_nb_words_question1',       'no_stopword_nb_common_words/no_stopword_nb_words_question2', 'no_stopword_nb_words_question1-no_stopword_common_words', no_stopword_nb_words_question2-no_stopword_common_words','no_stopword_nb_common_words/(no_stopword_nb_words_question1+no_stopword_nb_words_question2)']\n",
    "\n",
    "# To build a model with every kind of features except the 2 'not in english dict' ones\n",
    "ALL_FEATURES_EXCEPT_DICT = [c for c in all_numeric_columns(train_dataframe) if 'unknown' not in c] \n",
    "# To build a model with only features not aware of any stop words\n",
    "ALL_BASIC_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'nltk' not in c and 'all' not in c and 'unknown' not in c] \n",
    "# To build a model with only features aware of nltk stop words\n",
    "ALL_NLTK_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'nltk' in c] \n",
    "# To build a model with only features not aware of extended stop words\n",
    "ALL_EXTENDED_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'all' in c] \n",
    "# The 2 nb 'not in english dictionary' features \n",
    "ALL_DICT_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'unknown' in c] \n",
    "\n",
    "# everything\n",
    "ALL_ALL_FEATURES = all_numeric_columns(train_dataframe)\n",
    "\n",
    "print_section('All \"not in dict\" features')\n",
    "print_info('Nb features %d' % len(ALL_DICT_FEATURES))\n",
    "display(pandas.DataFrame(ALL_DICT_FEATURES,columns=['feature']))\n",
    "\n",
    "print_section('All basic features')\n",
    "print_info('Nb features %d' % len(ALL_BASIC_FEATURES))\n",
    "display(pandas.DataFrame(ALL_BASIC_FEATURES,columns=['feature']))\n",
    "\n",
    "print_section('All nltk features')\n",
    "print_info('Nb features %d' % len(ALL_NLTK_FEATURES))\n",
    "display(pandas.DataFrame(ALL_NLTK_FEATURES,columns=['feature']))\n",
    "\n",
    "print_section('All extended features')\n",
    "print_info('Nb features %d' % len(ALL_EXTENDED_FEATURES))\n",
    "display(pandas.DataFrame(ALL_EXTENDED_FEATURES,columns=['feature']))\n",
    "\n",
    "print_section('All features except \"not in dict\" ones')\n",
    "print_info('Nb features %d' % len(ALL_FEATURES_EXCEPT_DICT))\n",
    "display(pandas.DataFrame(ALL_FEATURES_EXCEPT_DICT,columns=['feature']))\n",
    "\n",
    "print_section('All features')\n",
    "print_info('Nb features %d' % len(ALL_ALL_FEATURES))\n",
    "display(pandas.DataFrame(ALL_ALL_FEATURES,columns=['feature']))\n",
    "\n"
   ]
  },
  {
   "source": [
    "Basic features : no preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_only_stop_columns_results = study_fields(\n",
    "    'Only on features not aware of any stop words',\n",
    "    'basic_features_xgboost',\n",
    "    ALL_BASIC_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe)\n",
    " "
   ]
  },
  {
   "source": [
    "Same features but nltk stop words are removed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "xgboost_only_stop_columns_results = study_fields(\n",
    "    'Only on features aware of nltk stop words',\n",
    "    'nltk_features_xgboost',\n",
    "    ALL_NLTK_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe)"
   ]
  },
  {
   "source": [
    "Same features but extended list of stop words are removed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_only_stop_columns_results = study_fields(\n",
    "    'Only on features aware of extended stop words',\n",
    "    'extended_features_xgboost',\n",
    "    ALL_EXTENDED_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe)"
   ]
  },
  {
   "source": [
    "All features except the 'not in english dictionary ones'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_only_stop_columns_results = study_fields(\n",
    "    'All features excepr not in dict ones',\n",
    "    'all_no_dict_features_xgboost',\n",
    "    ALL_FEATURES_EXCEPT_DICT,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe)  "
   ]
  },
  {
   "source": [
    "All features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# No need to explore build directly the biggest one\n",
    "xgboost_only_stop_columns_results = study_fields(\n",
    "    'All features except not in dict ones',\n",
    "    'all_features_xgboost',\n",
    "    ALL_ALL_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore=False) "
   ]
  },
  {
   "source": [
    "## What are our results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_last_submissions(all_submissions))\n",
    "display(get_best_submissions(all_submissions,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_feature_density(df, fname):\n",
    "    \n",
    "    ix_train = numpy.where(df['id'] >= 0)[0]\n",
    "    ix_test = numpy.where(df['id'] == -1)[0]\n",
    "    ix_is_dup = numpy.where(df['is_duplicate'] == 1)[0]\n",
    "    ix_not_dup = numpy.where(df['is_duplicate'] == 0)[0]\n",
    "\n",
    "    fig = plot.figure(figsize=(16, 12))\n",
    "    ax1 = plot.subplot2grid((3, 2), (0, 0), colspan=2)\n",
    "    ax2 = plot.subplot2grid((3, 2), (1, 0), colspan=2)\n",
    "    ax3 = plot.subplot2grid((3, 2), (2, 0))\n",
    "    ax4 = plot.subplot2grid((3, 2), (2, 1))\n",
    "    ax1.set_title('Distribution of %s' % fname, fontsize=20)\n",
    "    sns.distplot(df.loc[ix_train][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax1)    \n",
    "    sns.distplot(df.loc[ix_is_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='is dup')    \n",
    "    sns.distplot(df.loc[ix_not_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='not dup')\n",
    "    ax2.legend(loc='upper right', prop={'size': 18})\n",
    "    sns.boxplot(y=fname, \n",
    "                x='is_duplicate', \n",
    "                data=df.loc[ix_train], \n",
    "                ax=ax3)\n",
    "    sns.violinplot(y=fname, \n",
    "                   x='is_duplicate', \n",
    "                   data=df.loc[ix_train], \n",
    "                   ax=ax4)\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "plot_real_feature_density(train_dataframe,'all_nb_common_words/(all_nb_words_question1+all_nb_words_question2)')\n"
   ]
  },
  {
   "source": [
    "So, features around common words are quite good\n",
    "Therefore, anything that can help to better detect common words is good\n",
    "\n",
    "The notebook spacy_preprocessing.ipynb provides us a good preprocessing code to apply to questions before any work\n",
    "\n",
    "We have 2 choices with this code\n",
    "* generate a new domain : clean_xxxx, kepp previous domains and train a model\n",
    "* restart from scratch and generate all previous domains on the new clean text\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's start with way 1 : just add a new domain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# here is the code to preprocess questions\n",
    "import re\n",
    "\n",
    "# I do special stuff with $ and roupie char\n",
    "FINAL_PUNC_CLEANER = str.maketrans(dict([ (c,' ') for c in '!\"#%&\\'()*+,./:;<=>?[\\\\]^_`{|}~-@']))\n",
    "\n",
    "def clean_string(str):\n",
    "    str = re.sub('\\?',' ',str) # ?\n",
    "    # odd chars\n",
    "    # will generate more ' so do it first\n",
    "    str = re.sub(\"\", \"'\", str) # special single quote\n",
    "    str = re.sub(\"`\", \"'\", str) # special single quote\n",
    "    str = re.sub(\"\", '\"', str) # special double quote\n",
    "    str = re.sub(\"\", \"?\", str) \n",
    "    str = re.sub(\"\", \" \", str) \n",
    "    str = re.sub(\"\", \"e\", str)\n",
    "    \n",
    "    # shortcuts\n",
    "    str = re.sub('\\'s', ' is', str) \n",
    "    str = re.sub(' whats ', ' what is ', str)\n",
    "    str = re.sub('\\'ve', ' have ', str)\n",
    "    str = re.sub(\"can't\", 'can not', str)\n",
    "    # this one is tricky do it in order\n",
    "    str = re.sub(\"wouldn't\", 'would not', str)\n",
    "    str = re.sub(\"n't\", ' not ', str)\n",
    "    str = re.sub(\"i'm\", 'i am', str)\n",
    "    str = re.sub('\\'re', ' are ', str)\n",
    "    str = re.sub('\\'d', ' would ', str)\n",
    "    str = re.sub('\\'ll', ' will ', str)\n",
    "    str = re.sub('e\\.g\\.', ' eg ', str)\n",
    "    str = re.sub('b\\.g\\.', ' bg ', str)\n",
    "    str = re.sub('e-mail', ' email ', str)\n",
    "    str = re.sub('\\(s\\)', ' ', str)\n",
    "\n",
    "    # Numbers and measures are a true mess\n",
    "    # 12,000 -> 12000\n",
    "    str = re.sub('(?<=[0-9])\\,(?=[0-9])', '', str)\n",
    "\n",
    "    # Quora is very used in India so roupie (rs) is often present\n",
    "    str = re.sub(\"(?<=[0-9])rs \", \" rs \", str)\n",
    "    str = re.sub(\" rs(?=[0-9])\", \" rs \", str)\n",
    "\n",
    "    # stolen at kaggle : https://www.kaggle.com/currie32/the-importance-of-cleaning-str\n",
    "\n",
    "#    str = re.sub('[c-fC-F]\\:\\/', ' disk ', str)\n",
    "#    str = re.sub('(\\d+)(kK)', ' \\g<1>000 ', str)\n",
    "    # very weird !!! these ones decrease the hit % WTF ?\n",
    "\n",
    "    #str = re.sub(r\" (the[\\s]+|the[\\s]+)?us(a)? \", \" usa \", str)\n",
    "    #str = re.sub('(the[\\s]+|the[\\s]+)?united state(s)?', ' usa ', str)\n",
    "\n",
    "    str = re.sub(r\" uk \", \" england \", str)\n",
    "    str = re.sub(r\" imrovement \", \" improvement \", str)\n",
    "    str = re.sub(r\" intially \", \" initially \", str)\n",
    "    str = re.sub(r\" dms \", \" direct messages \", str)  \n",
    "    str = re.sub(r\" demonitization \", \" demonetization \", str) \n",
    "    str = re.sub(r\" actived \", \" active \", str)\n",
    "    str = re.sub(r\" kms \", \" kilometers \", str)\n",
    "    str = re.sub(r\" cs \", \" computer science \", str) \n",
    "    str = re.sub(r\" upvote\", \" up vote\", str)\n",
    "    str = re.sub(r\" iphone \", \" phone \", str)\n",
    "    str = re.sub(r\" \\0rs \", \" rs \", str)\n",
    "    str = re.sub(r\" calender \", \" calendar \", str)\n",
    "    str = re.sub(r\" ios \", \" operating system \", str)\n",
    "    str = re.sub(r\" programing \", \" programming \", str)\n",
    "    str = re.sub(r\" bestfriend \", \" best friend \", str)\n",
    "    str = re.sub(r\" iii \", \" 3 \", str)\n",
    "    str = re.sub(r\" banglore \", \" bangalore \", str)\n",
    "    str = re.sub(r\" j k \", \" jk \", str)\n",
    "    str = re.sub(r\" J\\.K\\. \", \" jk \", str)\n",
    "\n",
    "    \n",
    "    # some others\n",
    "    str = re.sub(r\"60k\", \" 60000 \", str)\n",
    "    str = re.sub(r\" e g \", \" eg \", str)\n",
    "    str = re.sub(r\" b g \", \" bg \", str)\n",
    "    str = re.sub(r\"\\0s\", \"0\", str)\n",
    "    str = re.sub(r\" 9 11 \", \"911\", str)\n",
    "    str = re.sub(r\"\\s{2,}\", \" \", str)\n",
    "    str = re.sub(r\" usa \", \" America \", str)\n",
    "    str = re.sub(r\" u s \", \" America \", str)\n",
    "    str = re.sub(r\"'m \", \" am \", str)\n",
    "\n",
    "    # units\n",
    "    str = re.sub(r\"(\\d+)kgs \", lambda m: m.group(1) + ' kg ', str)        # e.g. 4kgs => 4 kg\n",
    "    str = re.sub(r\"(\\d+)kg \", lambda m: m.group(1) + ' kg ', str)         # e.g. 4kg => 4 kg\n",
    "    str = re.sub(r\"(\\d+)k \", lambda m: m.group(1) + '000 ', str)          # e.g. 4k => 4000\n",
    "    str = re.sub(r\"\\$(\\d+)\", lambda m: m.group(1) + ' dollar ', str)\n",
    "    str = re.sub(r\"(\\d+)\\$\", lambda m: m.group(1) + ' dollar ', str)\n",
    "    # This one is important in 2017\n",
    "    str = re.sub(' donald trump',' trump ',str)\n",
    "    str = re.sub(' dollars',' dollar ',str)\n",
    "    str = re.sub(' quaro',' quora ',str)\n",
    "    str = re.sub(r\"googling\", \" google \", str)\n",
    "    str = re.sub(r\"googled\", \" google \", str)\n",
    "    str = re.sub(r\"googleable\", \" google \", str)\n",
    "    str = re.sub(r\"googles\", \" google \", str)\n",
    "    \n",
    "    str = re.sub(r\"\", \" rs \", str)      # \n",
    "    str = re.sub(r\"\\$\", \" dollar \", str)  \n",
    "    \n",
    "    #str = re.sub('[^\\x00-\\x7F]+', ' ', str)\n",
    "    # this will blank any of !\"#%&\\'()*+,./:;<=>?[\\\\]^_`{|}~-@\n",
    "    # Note the @ is dubious : won't we loose some emails ??\n",
    "    # and $ is replaced by dollar before\n",
    "    str = str.translate(FINAL_PUNC_CLEANER)\n",
    "\n",
    "    return str\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "time to clean up useless columns. A huge quantity of mem is wasted .."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe=train_dataframe.drop(columns=['common_words','uncommon_words_question1','uncommon_words_question2','nltk_common_words','nltk_uncommon_words_question1','nltk_uncommon_words_question2','all_common_words','all_uncommon_words_question1','all_uncommon_words_question2','unknown_question1','unknown_question2'])\n",
    "challenge_dataframe=challenge_dataframe.drop(columns=['common_words','uncommon_words_question1','uncommon_words_question2','nltk_common_words','nltk_uncommon_words_question1','nltk_uncommon_words_question2','all_common_words','all_uncommon_words_question1','all_uncommon_words_question2','unknown_question1','unknown_question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  \n",
    "def clean_build_no_stopwords_features_one_row(q1,q2,stopwords):\n",
    "    q1 = clean_string(q1)\n",
    "    q2 = clean_string(q2)\n",
    "    q1 = set([w for w in q1.split() if w not in stopwords])\n",
    "    len_q1 = len(q1)\n",
    "    q2 = set([w for w in q2.split() if w not in stopwords])\n",
    "    len_q2 = len(q2)\n",
    "\n",
    "    common = q1&q2\n",
    "    len_common = len(common)\n",
    "\n",
    "    uncommon_q1 = q1-common\n",
    "    len_uncommon_q1 = len(uncommon_q1)\n",
    "\n",
    "    uncommon_q2 = q2-common\n",
    "    len_uncommon_q2 = len(uncommon_q2)\n",
    "    #       0     1           2            2         4               5               6      7      8                        9                        10\n",
    "    return common,uncommon_q1,uncommon_q2,len_common,len_uncommon_q1,len_uncommon_q2,len_q1,len_q2,len_common/max(1,len_q1),len_common/max(1,len_q2),len_common/max(1,(len_q1+len_q2))\n",
    "\n",
    "\n",
    "def clean_build_all_stop_words_features(dataframe): \n",
    "    print_warning('Clean text and compute all features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: clean_build_no_stopwords_features_one_row(r.question1,r.question2,all_stop_words))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "  \n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words/clean_all_nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words/clean_all_nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question1-clean_all_common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question2-clean_all_common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words/(clean_all_nb_words_question1+clean_all_nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'clean_all_common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'clean_all_uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'clean_all_uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training data + clean + all stop words features','training_clean_all_stop_words_features',clean_build_all_stop_words_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge data + clean + all stop words features','challenge_clean_all_stop_words_features',clean_build_all_stop_words_features,challenge_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_dataframe=challenge_dataframe.drop(columns=['clean_all_common_words','clean_all_uncommon_words_question1','clean_all_uncommon_words_question2'])\n",
    "train_dataframe=train_dataframe.drop(columns=['clean_all_common_words','clean_all_uncommon_words_question1','clean_all_uncommon_words_question2'])\n"
   ]
  },
  {
   "source": [
    "We have generated our best features (the ones with the extended stop words) on preprocessed text.\n",
    "Let's update our domains and build models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To build a model with every kind of features except the 2 'not in english dict' ones\n",
    "ALL_FEATURES_EXCEPT_DICT = [c for c in all_numeric_columns(train_dataframe) if 'unknown' not in c] \n",
    "# To build a model with only features not aware of any stop words\n",
    "ALL_BASIC_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'nltk' not in c and 'all' not in c and 'unknown' not in c] \n",
    "# To build a model with only features aware of nltk stop words\n",
    "ALL_NLTK_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'nltk' in c] \n",
    "# To build a model with only features not aware of extended stop words\n",
    "ALL_EXTENDED_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'all' in c] \n",
    "# To build a model with only features on top of clean text and extended stop words\n",
    "ALL_CLEAN_EXTENDED_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'clean' in c] \n",
    "# The 2 nb 'not in english dictionary' features \n",
    "ALL_DICT_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'unknown' in c] \n",
    "\n",
    "# everything\n",
    "ALL_ALL_FEATURES = all_numeric_columns(train_dataframe)\n",
    "\n",
    "xgboost_only_stop_columns_results = study_fields(\n",
    "    'Only on extended features on clean text',\n",
    "    'clean_extended_features_xgboost',\n",
    "    ALL_CLEAN_EXTENDED_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_clean_all_all_features_results = study_fields(\n",
    "    'all features on clean text',\n",
    "    'clean_all_features_xgboost',\n",
    "    ALL_ALL_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()\n",
    "display(get_last_submissions(all_submissions))\n",
    "display(get_best_submissions(all_submissions,3))"
   ]
  },
  {
   "source": [
    "We have generated a bunch of other features in spacy_preprocessing.ipynb\n",
    "* all usual nb common features but on lemmatized questions, hoping it will make nb common feature more significative\n",
    "* a set of interesting entities and some ratios have been generated\n",
    "    * for each question and each kind of entity\n",
    "        * the number of entities\n",
    "        * the number of common entities\n",
    "        * ratio number of common entities/number of entities\n",
    "Idea is entities are high level information about a question. So if entities are/are not the same, in both questions, it should help "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Load these new from features from gobal repository (quite heavy to generate)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_from_pandas_store_if_missing('train_final_lemmatized_entities_features')\n",
    "train_lemmatized_entities = load_dataframe('train_final_lemmatized_entities_features')\n",
    "\n",
    "copy_from_pandas_store_if_missing('challenge_final_lemmatized_entities_features')\n",
    "challenge_lemmatized_entities = load_dataframe('challenge_final_lemmatized_entities_features')\n",
    "\n"
   ]
  },
  {
   "source": [
    "Stick our new set of features to train and challenge data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = pandas.concat([train_dataframe,train_lemmatized_entities.set_index(train_dataframe.index)],axis=1)\n",
    "challenge_dataframe = pandas.concat([challenge_dataframe,challenge_lemmatized_entities.set_index(challenge_dataframe.index)],axis=1)"
   ]
  },
  {
   "source": [
    "Let's define clearly our domains"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build a model with every kind of features except the 2 'not in english dict' ones\n",
    "ALL_FEATURES_EXCEPT_DICT = [c for c in all_numeric_columns(train_dataframe) if 'unknown' not in c] \n",
    "# To build a model with only features not aware of any stop words\n",
    "ALL_BASIC_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'nltk' not in c and 'all' not in c and 'unknown' not in c] \n",
    "# To build a model with only features aware of nltk stop words\n",
    "ALL_NLTK_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'nltk' in c] \n",
    "# To build a model with only features not aware of extended stop words\n",
    "ALL_EXTENDED_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'all' in c] \n",
    "# To build a model with only features on top of clean text and extended stop words\n",
    "ALL_CLEAN_EXTENDED_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'clean' in c] \n",
    "# The 2 nb 'not in english dictionary' features \n",
    "ALL_DICT_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'unknown' in c] \n",
    "\n",
    "ALL_LEMMATIZED_FEATURES=[c for c in all_numeric_columns(train_dataframe) if 'lemma' in c]\n",
    "ALL_ENTITIES_FEATURES=[c for c in all_numeric_columns(train_dataframe) if 'entities' in c]\n",
    "\n",
    "# everything\n",
    "ALL_ALL_FEATURES = all_numeric_columns(train_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_ALL_FEATURES"
   ]
  },
  {
   "source": [
    "Let's see if the model improved"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert WEIGHT in train_dataframe\n",
    "xgboost_everything_results = study_fields(\n",
    "    'Everything',\n",
    "    'everything_xgboost',\n",
    "    ALL_ALL_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore=False)"
   ]
  },
  {
   "source": [
    "## Let's try to fix unspelled words\n",
    "\n",
    "** Deactivated: awfully slow and not very effective**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell_checker = SpellChecker()\n",
    "\n",
    "def fix_words(words,spell_checker):\n",
    "    res = list()\n",
    "    nb = 0\n",
    "    for w in words:\n",
    "        if w in spell_checker:\n",
    "            res.append(w)\n",
    "        else:\n",
    "            res.append(spell_checker.correction(w))\n",
    "            nb += 1\n",
    "    return nb,res\n",
    "    \n",
    "if UNITARY_TEST:\n",
    "    input = 'the weather is finne'\n",
    "    nb,res = fix_words(input.split(),spell_checker)\n",
    "    assert 'fine' in res and 'finne' not in res and len(res) == len(input.split()) and nb==1\n"
   ]
  }
 ]
}