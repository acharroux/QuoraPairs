{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Usage of a classic dataset :20newsgroups\n",
    "\n",
    "1. We use this standard to build a classification model on 20 high level topics\n",
    "2. We use it to predict the topic of each questions. Maybe 2 or 3, certainly with a threshold\n",
    "3. We add this feature to the input data: \n",
    "    * if both newsgroup tags are the same, it means they talk about the same high level subject. \n",
    "    * if they are not, it means they don't talk about the same subkect, probably questions are different\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<span style=\"color:RED\">You will use environment newsgroups </span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<b>Prepare newsgroups environment in ../newsgroups</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<b>Untouched input data has been loaded. Training: 404290 lines Challenge: 2345796 lines</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "# Ugly incantation to make our 'framework' working\n",
    "import sys\n",
    "sys.path.insert(0, r'/SAPDevelop/QuoraPairs/BruteForce/Tools')\n",
    "\n",
    "#import all our small tools (paths, cache, print,zip,excel, pandas, progress,..)\n",
    "from Tools.all import *\n",
    "\n",
    "# setup the name of our experiment\n",
    "# it will be used to store every result in a unique place\n",
    "EXPERIMENT='newsgroups'\n",
    "# Do a bit of checks before actually running code\n",
    "UNITARY_TEST = True\n",
    "print_alert('You will use environment %s' % EXPERIMENT)\n",
    "\n",
    "prepare_environnement(EXPERIMENT)\n",
    "train_dataframe=load_dataframe(CLEAN_TRAINING_DATA)\n",
    "challenge_dataframe=load_dataframe(CLEAN_CHALLENGE_DATA)\n",
    "print_section('Untouched input data has been loaded. Training: %d lines Challenge: %d lines' % (len(train_dataframe),len(challenge_dataframe)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers','footers','quotes'),random_state=42)\n",
    "twenty_test =  fetch_20newsgroups(subset='test' , shuffle=True, remove=('headers','footers','quotes'),random_state=42)"
   ]
  },
  {
   "source": [
    "Here are the labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;alt.atheism&#39;,\n &#39;comp.graphics&#39;,\n &#39;comp.os.ms-windows.misc&#39;,\n &#39;comp.sys.ibm.pc.hardware&#39;,\n &#39;comp.sys.mac.hardware&#39;,\n &#39;comp.windows.x&#39;,\n &#39;misc.forsale&#39;,\n &#39;rec.autos&#39;,\n &#39;rec.motorcycles&#39;,\n &#39;rec.sport.baseball&#39;,\n &#39;rec.sport.hockey&#39;,\n &#39;sci.crypt&#39;,\n &#39;sci.electronics&#39;,\n &#39;sci.med&#39;,\n &#39;sci.space&#39;,\n &#39;soc.religion.christian&#39;,\n &#39;talk.politics.guns&#39;,\n &#39;talk.politics.mideast&#39;,\n &#39;talk.politics.misc&#39;,\n &#39;talk.religion.misc&#39;]"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "source": [
    "A bit of cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train_news = pandas.DataFrame(twenty_train.data,columns=['text'])\n",
    "train_news['text'] = train_news['text'].apply(lambda t: re.sub('[\\n]+',' ',t))\n",
    "train_news['target'] = twenty_train.target\n",
    "\n",
    "test_news = pandas.DataFrame(twenty_test.data,columns=['text'])\n",
    "test_news['text'] = test_news['text'].apply(lambda t: re.sub('[\\n]+',' ',t))\n",
    "test_news['target'] = twenty_test.target\n"
   ]
  },
  {
   "source": [
    "We merge some newsgroups to make them a little bit more generic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = {\n",
    "0:0, #'alt.atheism' -> religion\n",
    "1:1, #'comp.graphics' -> computers\n",
    "2:1, #'comp.os.ms-windows.misc' -> computers\n",
    "3:1, #'comp.sys.ibm.pc.hardware' -> computers\n",
    "4:1, #'comp.sys.mac.hardware' -> computers\n",
    "5:1, #'comp.windows.x' -> computers\n",
    "6:2, #'misc.forsale', -> forsale\n",
    "7:3, #'rec.autos' -> vehicle\n",
    "8:3, #'rec.motorcycles', -> vehicle\n",
    "9:4, #'rec.sport.baseball' -> sport\n",
    "10:4, #'rec.sport.hockey', -> sport\n",
    "11:5, #'sci.crypt', -> science\n",
    "12:5, #'sci.electronics', -> science\n",
    "13:5, #'sci.med', -> science\n",
    "14:5, #'sci.space', -> science\n",
    "15:0, #'soc.religion.christian', ->religion\n",
    "16:6, # talk.politics.guns',->politics\n",
    "17:6, #'talk.politics.mideast',->politics\n",
    "18:6, #'talk.politics.misc',->politics\n",
    "19:0, #'talk.religion.misc'-> religion\n",
    "}\n",
    "\n",
    "NEW_LABELS=[\n",
    "    'religion', #0\n",
    "    'computers', #1\n",
    "    'forsale', #2\n",
    "    'vehicles', #3\n",
    "    'sport', #4\n",
    "    'science', #5\n",
    "    'politics', #6\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news['new_target']=train_news['target'].apply(lambda k: MAPPING[k])\n",
    "test_news['new_target']=test_news['target'].apply(lambda k: MAPPING[k])\n"
   ]
  },
  {
   "source": [
    "Define a simple pipeline:\n",
    "* Count all words\n",
    "* Generate TfIdf\n",
    "* build a Multinomial Naive Bayes model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))), ('tfidf', TfidfTransformer(use_idf=True)), ('clf', MultinomialNB(alpha=0.01))])\n",
    "\n",
    "text_clf = text_clf.fit(train_news['text'], train_news['new_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8028412108337759"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_news['text'])\n",
    "np.mean(predicted == test_news['new_target'])"
   ]
  },
  {
   "source": [
    "Now, apply this model to our datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Save train_newsgroup_proba into global repository</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "train_newsgroup_proba_question1 = pandas.DataFrame(data=text_clf.predict_proba(train_dataframe['question1']),columns=['proba_'+k+'_question1' for k in NEW_LABELS])\n",
    "train_newsgroup_proba_question2 = pandas.DataFrame(data=text_clf.predict_proba(train_dataframe['question2']),columns=['proba_'+k+'_question2' for k in NEW_LABELS])\n",
    "\n",
    "# Glue the 2 proba dataset\n",
    "train_newsgroup_proba = pandas.concat([train_newsgroup_proba_question1,train_newsgroup_proba_question2.set_index(train_newsgroup_proba_question1.index)],axis=1)\n",
    "# save it in global repo\n",
    "save_global_dataframe(train_newsgroup_proba,'train_newsgroup_proba')\n",
    "del train_newsgroup_proba_question1\n",
    "del train_newsgroup_proba_question2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;IPython.core.display.HTML object&gt;",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Save challenge_newsgroup_proba into global repository</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "challenge_newsgroup_proba_question1 = pandas.DataFrame(data=text_clf.predict_proba(challenge_dataframe['question1']),columns=['proba_'+k+'_question1' for k in NEW_LABELS])\n",
    "challenge_newsgroup_proba_question2 = pandas.DataFrame(data=text_clf.predict_proba(challenge_dataframe['question2']),columns=['proba_'+k+'_question2' for k in NEW_LABELS])\n",
    "\n",
    "# Glue the 2 proba dataset\n",
    "challenge_newsgroup_proba = pandas.concat([challenge_newsgroup_proba_question1,challenge_newsgroup_proba_question2.set_index(challenge_newsgroup_proba_question1.index)],axis=1)\n",
    "# save it in global repo\n",
    "save_global_dataframe(challenge_newsgroup_proba,'challenge_newsgroup_proba')\n",
    "del challenge_newsgroup_proba_question1\n",
    "del challenge_newsgroup_proba_question2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe['is_duplicate'][train_dataframe['newsgroup1']==train_dataframe['newsgroup2']].count()\n",
    "challenge_dataframe[challenge_dataframe['newsgroup1']==challenge_dataframe['newsgroup2']].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai = pandas.DataFrame(text_clf.predict_proba(train_dataframe['question1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai"
   ]
  }
 ]
}