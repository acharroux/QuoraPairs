{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('anaconda3': conda)",
   "display_name": "Python 3.8.3 64-bit ('anaconda3': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8bc84ed563ec7490c0834918abd0859beccf8dba92b402dc7a3db729b0eff288"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Iterations on preprocessing\n",
    "\n",
    "Let's restart from scratch on lowercased questions.\n",
    "\n",
    "Note we will loose some information by lowercasing everything...\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:RED\">You will use environment kxen_nlu </span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Prepare kxen_nlu environment in ../kxen_nlu</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Save clean_training</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Save clean_challenge</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Untouched input data has been loaded. Training: 404290 lines Challenge: 2345796 lines</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "# Ugly incantation to make our 'framework' working\n",
    "import sys\n",
    "sys.path.insert(0, r'/SAPDevelop/QuoraPairs/BruteForce/Tools')\n",
    "\n",
    "#import all our small tools (paths, cache, print,zip,excel, pandas, progress,..)\n",
    "from Tools.all import *\n",
    "\n",
    "# setup the name of our experiment\n",
    "# it will be used to store every result in a unique place\n",
    "EXPERIMENT='kxen_nlu'\n",
    "# Do a bit of checks before actually running long code\n",
    "UNITARY_TEST = True\n",
    "print_alert('You will use environment %s' % EXPERIMENT)\n",
    "\n",
    "prepare_environnement(EXPERIMENT)\n",
    "\n",
    "train_dataframe = pandas.read_csv('../PandasStore/clean_train.csv',error_bad_lines=True,warn_bad_lines=True)\n",
    "challenge_dataframe = pandas.read_csv('../PandasStore/clean_challenge.csv',error_bad_lines=True,warn_bad_lines=True)\n",
    " \n",
    "save_dataframe(train_dataframe,'clean_training')\n",
    "save_dataframe(challenge_dataframe,'clean_challenge')\n",
    "\n",
    "train_dataframe=load_dataframe(CLEAN_TRAINING_DATA)\n",
    "challenge_dataframe=load_dataframe(CLEAN_CHALLENGE_DATA)\n",
    "print_section('Untouched input data has been loaded. Training: %d lines Challenge: %d lines' % (len(train_dataframe),len(challenge_dataframe)))"
   ]
  },
  {
   "source": [
    "## Challenge and training are not equivalent\n",
    "\n",
    "% of duplicate in training is not the same in challenge !\n",
    "We don't have the challenge's answer but by scoring a constant prediction and using logloss definition, we can guess the distribution of duplicates in challenge\n",
    "\n",
    "It's 17.46 %\n",
    "\n",
    "To fix that we can:\n",
    "\n",
    "* duplicate some negative cases in training to match challenge distribution\n",
    "* use weights : each case has a weight !=1 and all algorithms are supposed to be able to deal with that info\n",
    "I choose to use weights (less memory needed ?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>OUPS !! % of duplicates in train is 0.369. In challenge it is 0.175 %</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>let's add some weights to rebalance the data</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Weight for positive case 0.473</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Weight for negative case 1.308</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Training data set has been properly rebalanced</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Weights distribution:</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    404290.000000\n",
       "mean          1.000000\n",
       "std           0.403239\n",
       "min           0.472917\n",
       "25%           0.472917\n",
       "50%           1.308493\n",
       "75%           1.308493\n",
       "max           1.308493\n",
       "Name: weight, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\n",
    "CHALLENGE_DUPLICATE_PERCENT = 0.1746\n",
    "\n",
    "#return the weight for 0 and weight for 1 needed to rebalance dataframe like challenge\n",
    "def balanced_weights(dataframe,expected_positive_ratio):\n",
    "    current_positive_ratio = dataframe['is_duplicate'].sum()/len(dataframe)\n",
    "    weight_for_negative = (1-expected_positive_ratio)/(1-current_positive_ratio)\n",
    "    weight_for_positive = expected_positive_ratio/current_positive_ratio\n",
    "    return weight_for_positive,weight_for_negative\n",
    "\n",
    "print_warning('OUPS !! %% of duplicates in train is %.3f. In challenge it is %.3f %%' % (train_dataframe['is_duplicate'].sum()/len(train_dataframe),CHALLENGE_DUPLICATE_PERCENT))\n",
    "\n",
    "# create a new 'weight' column to training dataset\n",
    "# Do not forget to remove this column from features !!!\n",
    "print_warning(\"let's add some weights to rebalance the data\")\n",
    "weight_for_1,weight_for_0 = balanced_weights(train_dataframe,CHALLENGE_DUPLICATE_PERCENT)\n",
    "print_info('Weight for positive case %.3f' % weight_for_1)\n",
    "print_info('Weight for negative case %.3f' % weight_for_0)\n",
    "train_dataframe['weight'] = train_dataframe['is_duplicate'].map( {0:weight_for_0, 1:weight_for_1})\n",
    "\n",
    "assert int(train_dataframe['weight'].sum()/len(train_dataframe)) == 1, \"training dataset has not been properly rebalanced\"\n",
    "print_info(\"Training data set has been properly rebalanced\")\n",
    "print_info('Weights distribution:')\n",
    "train_dataframe['weight'].describe()"
   ]
  },
  {
   "source": [
    "### Step 1: Lower case everything"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Lower case everything in training: Load or rebuild training_lower</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/training_lower.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:training_lower contains 404290 lines in 0.2 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Lower case everything in challenge: Load or rebuild challenge_lower</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/challenge_lower.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:challenge_lower contains 2345796 lines in 1.2 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "# our main tool to add feature\n",
    "def add_column_from_columns(dataframe,output_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe.progress_apply(function,axis=1)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "def add_column_from_column(dataframe,output_column_name,input_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe[input_column_name].progress_apply(function)\n",
    "    return dataframe[output_column_name]\n",
    "    \n",
    "def build_all_lower_data(dataframe):\n",
    "    print_info('Lower case question1')\n",
    "    dataframe['question1'] = dataframe['question1'].str.lower()\n",
    "    print_info('Lower case question2')\n",
    "    dataframe['question2'] = dataframe['question2'].str.lower()\n",
    "    return dataframe\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Lower case everything in training','training_lower',build_all_lower_data,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Lower case everything in challenge','challenge_lower',build_all_lower_data,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "### Step 2 : build our basic features\n",
    "* Nb words in question 1\n",
    "* Nb words in question 2\n",
    "* Nb common words\n",
    "* Nb common words/nb words in question 1\n",
    "* Nb common words/nb words in question 2\n",
    "* Nb non common words in question 1\n",
    "* Nb non common words in question 2\n",
    "* Nb common words/(Nb words in question1 + Nb words in question2)\n",
    "\n",
    "Note this is done on raw question content : no preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Training data + basic features: Load or rebuild training_basic_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/training_basic_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:training_basic_features contains 404290 lines in 4.1 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Challenge data + basic features: Load or rebuild challenge_basic_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/challenge_basic_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:challenge_basic_features contains 2345796 lines in 24.1 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "def build_basic_features_one_row(q1,q2):\n",
    "    q1 = set(str(q1).split())\n",
    "    len_q1 = len(q1)\n",
    "    q2 = set(str(q2).split())\n",
    "    len_q2 = len(q2)\n",
    "\n",
    "    common = q1&q2\n",
    "    len_common = len(common)\n",
    "\n",
    "    uncommon_q1 = q1-common\n",
    "    len_uncommon_q1 = len(uncommon_q1)\n",
    "\n",
    "    uncommon_q2 = q2-common\n",
    "    len_uncommon_q2 = len(uncommon_q2)\n",
    "    #       0     1           2            3         4               5               6      7      8                        9                        10\n",
    "    return common,uncommon_q1,uncommon_q2,len_common,len_uncommon_q1,len_uncommon_q2,len_q1,len_q2,len_common/max(1,len_q1),len_common/max(1,len_q2),len_common/(len_q1+len_q2)\n",
    "    \n",
    "def build_all_basic_features(dataframe):\n",
    "    print_warning('Compute all features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: build_basic_features_one_row(r.question1,r.question2))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'nb_common_words/nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'nb_common_words/nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'nb_words_question1-common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'nb_words_question2-common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'nb_common_words/(nb_words_question1+nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training data + basic features','training_basic_features',build_all_basic_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge data + basic features','challenge_basic_features',build_all_basic_features,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "Let's take a look in our new data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                                          0  \\\n",
       "id                                                                                                                        0   \n",
       "qid1                                                                                                                      1   \n",
       "qid2                                                                                                                      2   \n",
       "question1                                                what is the step by step guide to invest in share market in india?   \n",
       "question2                                                         what is the step by step guide to invest in share market?   \n",
       "is_duplicate                                                                                                              0   \n",
       "weight                                                                                                              1.30849   \n",
       "nb_words_question1                                                                                                       12   \n",
       "nb_words_question2                                                                                                       11   \n",
       "nb_common_words                                                                                                          10   \n",
       "nb_common_words/nb_words_question1                                                                                 0.833333   \n",
       "nb_common_words/nb_words_question2                                                                                 0.909091   \n",
       "nb_words_question1-common_words                                                                                           2   \n",
       "nb_words_question2-common_words                                                                                           1   \n",
       "nb_common_words/(nb_words_question1+nb_words_question2)                                                            0.434783   \n",
       "common_words                                                        {guide, step, in, by, the, invest, what, share, is, to}   \n",
       "uncommon_words_question1                                                                                   {market, india?}   \n",
       "uncommon_words_question2                                                                                          {market?}   \n",
       "\n",
       "                                                                                                                                                1  \n",
       "id                                                                                                                                              1  \n",
       "qid1                                                                                                                                            3  \n",
       "qid2                                                                                                                                            4  \n",
       "question1                                                                                     what is the story of kohinoor (koh-i-noor) diamond?  \n",
       "question2                                                what would happen if the indian government stole the kohinoor (koh-i-noor) diamond back?  \n",
       "is_duplicate                                                                                                                                    0  \n",
       "weight                                                                                                                                    1.30849  \n",
       "nb_words_question1                                                                                                                              8  \n",
       "nb_words_question2                                                                                                                             12  \n",
       "nb_common_words                                                                                                                                 4  \n",
       "nb_common_words/nb_words_question1                                                                                                            0.5  \n",
       "nb_common_words/nb_words_question2                                                                                                       0.333333  \n",
       "nb_words_question1-common_words                                                                                                                 4  \n",
       "nb_words_question2-common_words                                                                                                                 8  \n",
       "nb_common_words/(nb_words_question1+nb_words_question2)                                                                                       0.2  \n",
       "common_words                                                                                                  {what, kohinoor, (koh-i-noor), the}  \n",
       "uncommon_words_question1                                                                                                {of, diamond?, is, story}  \n",
       "uncommon_words_question2                                                           {diamond, stole, government, happen, back?, indian, if, would}  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>qid1</th>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>qid2</th>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>question1</th>\n      <td>what is the step by step guide to invest in share market in india?</td>\n      <td>what is the story of kohinoor (koh-i-noor) diamond?</td>\n    </tr>\n    <tr>\n      <th>question2</th>\n      <td>what is the step by step guide to invest in share market?</td>\n      <td>what would happen if the indian government stole the kohinoor (koh-i-noor) diamond back?</td>\n    </tr>\n    <tr>\n      <th>is_duplicate</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>weight</th>\n      <td>1.30849</td>\n      <td>1.30849</td>\n    </tr>\n    <tr>\n      <th>nb_words_question1</th>\n      <td>12</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>nb_words_question2</th>\n      <td>11</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>nb_common_words</th>\n      <td>10</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>nb_common_words/nb_words_question1</th>\n      <td>0.833333</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>nb_common_words/nb_words_question2</th>\n      <td>0.909091</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>nb_words_question1-common_words</th>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>nb_words_question2-common_words</th>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>nb_common_words/(nb_words_question1+nb_words_question2)</th>\n      <td>0.434783</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>common_words</th>\n      <td>{guide, step, in, by, the, invest, what, share, is, to}</td>\n      <td>{what, kohinoor, (koh-i-noor), the}</td>\n    </tr>\n    <tr>\n      <th>uncommon_words_question1</th>\n      <td>{market, india?}</td>\n      <td>{of, diamond?, is, story}</td>\n    </tr>\n    <tr>\n      <th>uncommon_words_question2</th>\n      <td>{market?}</td>\n      <td>{diamond, stole, government, happen, back?, indian, if, would}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_dataframe.head(2).transpose()"
   ]
  },
  {
   "source": [
    "Let's take a look in common words & uncommon_words_question1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                               common_words  \\\n",
       "0                                   {guide, step, in, by, the, invest, what, share, is, to}   \n",
       "1                                                       {what, kohinoor, (koh-i-noor), the}   \n",
       "2                                                               {can, internet, speed, how}   \n",
       "3                                                                                        {}   \n",
       "4                                                                               {which, in}   \n",
       "...                                                                                     ...   \n",
       "404285  {there, version?, in, how, language, the, are, many, keywords, latest, programming}   \n",
       "404286                                                     {there, death?, life, after, is}   \n",
       "404287                                                                              {coin?}   \n",
       "404288                                                                                 {of}   \n",
       "404289                                       {cousin?, like, with, sex, have, what, is, to}   \n",
       "\n",
       "                                                                                            uncommon_words_question1  \n",
       "0                                                                                                   {market, india?}  \n",
       "1                                                                                          {of, diamond?, is, story}  \n",
       "2                                                      {a, of, the, my, while, connection, vpn?, using, i, increase}  \n",
       "3                                                        {can, why, lonely?, how, solve, very, am, mentally, i, it?}  \n",
       "4                                    {sugar,, quikly, oxide?, and, dissolve, water, carbon, di, one, salt,, methane}  \n",
       "...                                                                                                              ...  \n",
       "404285                                                                                                  {of, racket}  \n",
       "404286                                                                                            {you, do, believe}  \n",
       "404287                                                                                               {what, one, is}  \n",
       "404288  {student?, studying, uic, in, the, while, indian, chicago,, an, annual, living, cost, what, for, is, approx}  \n",
       "404289                                                                                                            {}  \n",
       "\n",
       "[404290 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>common_words</th>\n      <th>uncommon_words_question1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{guide, step, in, by, the, invest, what, share, is, to}</td>\n      <td>{market, india?}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{what, kohinoor, (koh-i-noor), the}</td>\n      <td>{of, diamond?, is, story}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{can, internet, speed, how}</td>\n      <td>{a, of, the, my, while, connection, vpn?, using, i, increase}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{}</td>\n      <td>{can, why, lonely?, how, solve, very, am, mentally, i, it?}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{which, in}</td>\n      <td>{sugar,, quikly, oxide?, and, dissolve, water, carbon, di, one, salt,, methane}</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>404285</th>\n      <td>{there, version?, in, how, language, the, are, many, keywords, latest, programming}</td>\n      <td>{of, racket}</td>\n    </tr>\n    <tr>\n      <th>404286</th>\n      <td>{there, death?, life, after, is}</td>\n      <td>{you, do, believe}</td>\n    </tr>\n    <tr>\n      <th>404287</th>\n      <td>{coin?}</td>\n      <td>{what, one, is}</td>\n    </tr>\n    <tr>\n      <th>404288</th>\n      <td>{of}</td>\n      <td>{student?, studying, uic, in, the, while, indian, chicago,, an, annual, living, cost, what, for, is, approx}</td>\n    </tr>\n    <tr>\n      <th>404289</th>\n      <td>{cousin?, like, with, sex, have, what, is, to}</td>\n      <td>{}</td>\n    </tr>\n  </tbody>\n</table>\n<p>404290 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_dataframe[['common_words','uncommon_words_question1']]"
   ]
  },
  {
   "source": [
    "OK common words and uncommon words are polluted by tons of words like: is,with,to,..  \n",
    "\n",
    "These are stop words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Step 3 : remove a first set of stop words\n",
    "We start with stopwords coming from nltk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>There is 179 stopwords in nltk</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Training: Build features aware of nltk stopwords: Load or rebuild training_nltk_stop_words_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/training_nltk_stop_words_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:training_nltk_stop_words_features contains 404290 lines in 7.9 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Challenge: Build features aware of nltk stopwords: Load or rebuild challenge_nltk_stop_words_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/challenge_nltk_stop_words_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:challenge_nltk_stop_words_features contains 2345796 lines in 58.1 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def build_no_stopwords_features_one_row(q1,q2,stopwords):\n",
    "    q1 = set([w for w in str(q1).split() if w not in stopwords])\n",
    "    len_q1 = len(q1)\n",
    "    q2 = set([w for w in str(q2).split() if w not in stopwords])\n",
    "    len_q2 = len(q2)\n",
    "\n",
    "    common = q1&q2\n",
    "    len_common = len(common)\n",
    "\n",
    "    uncommon_q1 = q1-common\n",
    "    len_uncommon_q1 = len(uncommon_q1)\n",
    "\n",
    "    uncommon_q2 = q2-common\n",
    "    len_uncommon_q2 = len(uncommon_q2)\n",
    "    #       0     1           2            2         4               5               6      7      8                        9                        10\n",
    "    return common,uncommon_q1,uncommon_q2,len_common,len_uncommon_q1,len_uncommon_q2,len_q1,len_q2,len_common/max(1,len_q1),len_common/max(1,len_q2),len_common/max(1,(len_q1+len_q2))\n",
    "    \n",
    "\n",
    "def build_nltk_stop_words_features(dataframe):\n",
    "    print_warning('Remove stopwords from question 1 & question2')\n",
    "    \n",
    "    print_warning('Compute all nltk features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: build_no_stopwords_features_one_row(r.question1,r.question2,nltk_stopwords))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words/nltk_nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words/nltk_nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question1-nltk_common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'nltk_nb_words_question2-nltk_common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nltk_common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'nltk_uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'nltk_uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "print_info( 'There is %d stopwords in nltk' % len(nltk_stopwords))\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training: Build features aware of nltk stopwords','training_nltk_stop_words_features',build_nltk_stop_words_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge: Build features aware of nltk stopwords','challenge_nltk_stop_words_features',build_nltk_stop_words_features,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "Did we change anything ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>We have changed 85.80 % of nb common_words in training!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>We have changed 76.89 % of nb common_words in challenge !!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              common_words             nltk_common_words   uncommon_words_question1 nltk_uncommon_words_question1\n",
       "0  {guide, step, in, by, the, invest, what, share, is, to}  {invest, share, step, guide}           {market, india?}              {market, india?}\n",
       "1                      {what, kohinoor, (koh-i-noor), the}      {kohinoor, (koh-i-noor)}  {of, diamond?, is, story}             {diamond?, story}"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>common_words</th>\n      <th>nltk_common_words</th>\n      <th>uncommon_words_question1</th>\n      <th>nltk_uncommon_words_question1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{guide, step, in, by, the, invest, what, share, is, to}</td>\n      <td>{invest, share, step, guide}</td>\n      <td>{market, india?}</td>\n      <td>{market, india?}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{what, kohinoor, (koh-i-noor), the}</td>\n      <td>{kohinoor, (koh-i-noor)}</td>\n      <td>{of, diamond?, is, story}</td>\n      <td>{diamond?, story}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "nb_changed_train = int(numpy.where(train_dataframe['nltk_nb_common_words']!=train_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('We have changed %.2f %% of nb common_words in training!!' % (nb_changed_train*100./len(train_dataframe)))\n",
    "nb_changed_challenge = int(numpy.where(challenge_dataframe['nltk_nb_common_words']!=challenge_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('We have changed %.2f %% of nb common_words in challenge !!' % (nb_changed_challenge*100./len(challenge_dataframe)))\n",
    "train_dataframe[['common_words','nltk_common_words','uncommon_words_question1','nltk_uncommon_words_question1']].head(2)"
   ]
  },
  {
   "source": [
    "Step 4: remove more stopwords coming from sklearn\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>There is 179 words in nltk stop words</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>There is 318 words in sklearn stop words</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>There is 199 new stop words in sklearn stop words</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>There is 378 words in the union of stop words</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Training: Build features aware of all stopwords: Load or rebuild training_all_stop_words_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/training_all_stop_words_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:training_all_stop_words_features contains 404290 lines in 13.8 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Challenge: Build features aware of all stopwords: Load or rebuild challenge_all_stop_words_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/challenge_all_stop_words_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:challenge_all_stop_words_features contains 2345796 lines in 299.2 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "sk_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "print_info('There is %d words in nltk stop words' % len(nltk_stopwords))\n",
    "print_info('There is %d words in sklearn stop words' % len(sk_stopwords))\n",
    "new_stopwords = [w for w in sk_stopwords if w not in nltk_stopwords]\n",
    "print_info('There is %d new stop words in sklearn stop words' % len(new_stopwords))\n",
    "\n",
    "all_stop_words = nltk_stopwords | sk_stopwords\n",
    "print_info('There is %d words in the union of stop words' % len(all_stop_words))\n",
    "\n",
    "def build_all_stop_words_features(dataframe): \n",
    "    print_warning('Compute all features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: build_no_stopwords_features_one_row(r.question1,r.question2,all_stop_words))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "  \n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words/all_nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words/all_nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question1-all_common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'all_nb_words_question2-all_common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'all_nb_common_words/(all_nb_words_question1+all_nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'all_common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'all_uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'all_uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training: Build features aware of all stopwords','training_all_stop_words_features',build_all_stop_words_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge: Build features aware of all stopwords','challenge_all_stop_words_features',build_all_stop_words_features,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "Try to recap the changes we made"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>With nltk stop words we have changed 85.80 % of nb common_words in training!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Removing also sklearn stop words we have changed 86.44 % of nb common_words in training!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>With nltk stop words We have changed 76.89 % of nb common_words in challenge !!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Removing also sklearn stop words we have changed 77.67 % of nb common_words in challenge !!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                         common_words                nltk_common_words          all_common_words uncommon_words_question1 nltk_uncommon_words_question1  \\\n",
       "13                                   {sexual, was, what, your, first}                  {sexual, first}                  {sexual}      {experience, like?}           {experience, like?}   \n",
       "30                             {do, better?, thing, you, one, what's}    {one, thing, better?, what's}  {thing, better?, what's}        {would, to, like}                 {would, like}   \n",
       "32  {thrones, most, of, the, be, you, would, game, what, villain, to}  {would, thrones, game, villain}  {villain, thrones, game}   {give, likely, mercy?}        {give, likely, mercy?}   \n",
       "\n",
       "   all_uncommon_words_question1  \n",
       "13          {experience, like?}  \n",
       "30                       {like}  \n",
       "32             {likely, mercy?}  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>common_words</th>\n      <th>nltk_common_words</th>\n      <th>all_common_words</th>\n      <th>uncommon_words_question1</th>\n      <th>nltk_uncommon_words_question1</th>\n      <th>all_uncommon_words_question1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>{sexual, was, what, your, first}</td>\n      <td>{sexual, first}</td>\n      <td>{sexual}</td>\n      <td>{experience, like?}</td>\n      <td>{experience, like?}</td>\n      <td>{experience, like?}</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>{do, better?, thing, you, one, what's}</td>\n      <td>{one, thing, better?, what's}</td>\n      <td>{thing, better?, what's}</td>\n      <td>{would, to, like}</td>\n      <td>{would, like}</td>\n      <td>{like}</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>{thrones, most, of, the, be, you, would, game, what, villain, to}</td>\n      <td>{would, thrones, game, villain}</td>\n      <td>{villain, thrones, game}</td>\n      <td>{give, likely, mercy?}</td>\n      <td>{give, likely, mercy?}</td>\n      <td>{likely, mercy?}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "nb_changed_train = int(numpy.where(train_dataframe['nltk_nb_common_words'] != train_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('With nltk stop words we have changed %.2f %% of nb common_words in training!!' % (nb_changed_train*100./len(train_dataframe)))\n",
    "nb_changed_train = int(numpy.where(train_dataframe['all_nb_common_words'] != train_dataframe['nb_common_words'],1,0).sum())\n",
    "print_warning('Removing also sklearn stop words we have changed %.2f %% of nb common_words in training!!' % (nb_changed_train*100./len(train_dataframe)))\n",
    "\n",
    "nb_changed_challenge = int(numpy.where(challenge_dataframe['nltk_nb_common_words']!=challenge_dataframe['nb_common_words'],1,0).sum())\n",
    "print_info('With nltk stop words We have changed %.2f %% of nb common_words in challenge !!' % (nb_changed_challenge*100./len(challenge_dataframe)))\n",
    "nb_changed_challenge = int(numpy.where(challenge_dataframe['all_nb_common_words']!=challenge_dataframe['nb_common_words'],1,0).sum())\n",
    "print_warning('Removing also sklearn stop words we have changed %.2f %% of nb common_words in challenge !!' % (nb_changed_challenge*100./len(challenge_dataframe)))\n",
    "\n",
    "changed = train_dataframe[train_dataframe['nltk_nb_common_words']!=train_dataframe['all_nb_common_words']]\n",
    "changed[['common_words','nltk_common_words','all_common_words','uncommon_words_question1','nltk_uncommon_words_question1','all_uncommon_words_question1']].head(3)"
   ]
  },
  {
   "source": [
    "We have tons of useless and costly data in memory. Let's get rid of it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe=train_dataframe.drop(columns=['common_words','uncommon_words_question1','uncommon_words_question2','nltk_common_words','nltk_uncommon_words_question1','nltk_uncommon_words_question2','all_common_words','all_uncommon_words_question1','all_uncommon_words_question2'])\n",
    "challenge_dataframe=challenge_dataframe.drop(columns=['common_words','uncommon_words_question1','uncommon_words_question2','nltk_common_words','nltk_uncommon_words_question1','nltk_uncommon_words_question2','all_common_words','all_uncommon_words_question1','all_uncommon_words_question2'])"
   ]
  },
  {
   "source": [
    "## Clean questions\n",
    "\n",
    "Questions are polluted by many small issues tha impact also the quality of our share/unshared words:\n",
    "* typos : initilly for initially\n",
    "* units: 100,000rs for 100000 roupies\n",
    "* punctaion : each question ends with a ? glued to last word\n",
    "* ...\n",
    "\n",
    "Notebook find_preprocessing allowed us to defaint around 50 cleaning rules we are apply here\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the code to preprocess questions\n",
    "import re\n",
    "\n",
    "# I do special stuff with $ and roupie char\n",
    "FINAL_PUNC_CLEANER = str.maketrans(dict([ (c,' ') for c in '!\"#%&\\'()*+,./:;<=>?[\\\\]^_`{|}~-@']))\n",
    "\n",
    "def clean_string(str):\n",
    "    str = re.sub('\\?',' ',str) # ?\n",
    "    # odd chars\n",
    "    # will generate more ' so do it first\n",
    "    str = re.sub(\"â€™\", \"'\", str) # special single quote\n",
    "    str = re.sub(\"`\", \"'\", str) # special single quote\n",
    "    str = re.sub(\"â€œ\", '\"', str) # special double quote\n",
    "    str = re.sub(\"ï¼Ÿ\", \"?\", str) \n",
    "    str = re.sub(\"â€¦\", \" \", str) \n",
    "    str = re.sub(\"Ã©\", \"e\", str)\n",
    "    \n",
    "    # shortcuts\n",
    "    str = re.sub('\\'s', ' is', str) \n",
    "    str = re.sub(' whats ', ' what is ', str)\n",
    "    str = re.sub('\\'ve', ' have ', str)\n",
    "    str = re.sub(\"can't\", 'can not', str)\n",
    "    # this one is tricky do it in order\n",
    "    str = re.sub(\"wouldn't\", 'would not', str)\n",
    "    str = re.sub(\"n't\", ' not ', str)\n",
    "    str = re.sub(\"i'm\", 'i am', str)\n",
    "    str = re.sub('\\'re', ' are ', str)\n",
    "    str = re.sub('\\'d', ' would ', str)\n",
    "    str = re.sub('\\'ll', ' will ', str)\n",
    "    str = re.sub('e\\.g\\.', ' eg ', str)\n",
    "    str = re.sub('b\\.g\\.', ' bg ', str)\n",
    "    str = re.sub('e-mail', ' email ', str)\n",
    "    str = re.sub('\\(s\\)', ' ', str)\n",
    "\n",
    "    # Numbers and measures are a true mess\n",
    "    # 12,000 -> 12000\n",
    "    str = re.sub('(?<=[0-9])\\,(?=[0-9])', '', str)\n",
    "\n",
    "    # Quora is very used in India so roupie (rs) is often present\n",
    "    str = re.sub(\"(?<=[0-9])rs \", \" rs \", str)\n",
    "    str = re.sub(\" rs(?=[0-9])\", \" rs \", str)\n",
    "\n",
    "    # stolen at kaggle : https://www.kaggle.com/currie32/the-importance-of-cleaning-str\n",
    "\n",
    "#    str = re.sub('[c-fC-F]\\:\\/', ' disk ', str)\n",
    "#    str = re.sub('(\\d+)(kK)', ' \\g<1>000 ', str)\n",
    "    # very weird !!! these ones decrease the hit % WTF ?\n",
    "\n",
    "    #str = re.sub(r\" (the[\\s]+|the[\\s]+)?us(a)? \", \" usa \", str)\n",
    "    #str = re.sub('(the[\\s]+|the[\\s]+)?united state(s)?', ' usa ', str)\n",
    "\n",
    "    str = re.sub(r\" uk \", \" england \", str)\n",
    "    str = re.sub(r\" imrovement \", \" improvement \", str)\n",
    "    str = re.sub(r\" intially \", \" initially \", str)\n",
    "    str = re.sub(r\" dms \", \" direct messages \", str)  \n",
    "    str = re.sub(r\" demonitization \", \" demonetization \", str) \n",
    "    str = re.sub(r\" actived \", \" active \", str)\n",
    "    str = re.sub(r\" kms \", \" kilometers \", str)\n",
    "    str = re.sub(r\" cs \", \" computer science \", str) \n",
    "    str = re.sub(r\" upvote\", \" up vote\", str)\n",
    "    str = re.sub(r\" iphone \", \" phone \", str)\n",
    "    str = re.sub(r\" \\0rs \", \" rs \", str)\n",
    "    str = re.sub(r\" calender \", \" calendar \", str)\n",
    "    str = re.sub(r\" ios \", \" operating system \", str)\n",
    "    str = re.sub(r\" programing \", \" programming \", str)\n",
    "    str = re.sub(r\" bestfriend \", \" best friend \", str)\n",
    "    str = re.sub(r\" iii \", \" 3 \", str)\n",
    "    str = re.sub(r\" banglore \", \" bangalore \", str)\n",
    "    str = re.sub(r\" j k \", \" jk \", str)\n",
    "    str = re.sub(r\" J\\.K\\. \", \" jk \", str)\n",
    "\n",
    "    \n",
    "    # some others\n",
    "    str = re.sub(r\"60k\", \" 60000 \", str)\n",
    "    str = re.sub(r\" e g \", \" eg \", str)\n",
    "    str = re.sub(r\" b g \", \" bg \", str)\n",
    "    str = re.sub(r\"\\0s\", \"0\", str)\n",
    "    str = re.sub(r\" 9 11 \", \"911\", str)\n",
    "    str = re.sub(r\"\\s{2,}\", \" \", str)\n",
    "    str = re.sub(r\" usa \", \" America \", str)\n",
    "    str = re.sub(r\" u s \", \" America \", str)\n",
    "    str = re.sub(r\"'m \", \" am \", str)\n",
    "\n",
    "    # units\n",
    "    str = re.sub(r\"(\\d+)kgs \", lambda m: m.group(1) + ' kg ', str)        # e.g. 4kgs => 4 kg\n",
    "    str = re.sub(r\"(\\d+)kg \", lambda m: m.group(1) + ' kg ', str)         # e.g. 4kg => 4 kg\n",
    "    str = re.sub(r\"(\\d+)k \", lambda m: m.group(1) + '000 ', str)          # e.g. 4k => 4000\n",
    "    str = re.sub(r\"\\$(\\d+)\", lambda m: m.group(1) + ' dollar ', str)\n",
    "    str = re.sub(r\"(\\d+)\\$\", lambda m: m.group(1) + ' dollar ', str)\n",
    "    # This one is important in 2017\n",
    "    str = re.sub(' donald trump',' trump ',str)\n",
    "    str = re.sub(' dollars',' dollar ',str)\n",
    "    str = re.sub(' quaro',' quora ',str)\n",
    "    str = re.sub(r\"googling\", \" google \", str)\n",
    "    str = re.sub(r\"googled\", \" google \", str)\n",
    "    str = re.sub(r\"googleable\", \" google \", str)\n",
    "    str = re.sub(r\"googles\", \" google \", str)\n",
    "    \n",
    "    str = re.sub(r\"â‚¹\", \" rs \", str)      # æµ‹è¯•ï¼\n",
    "    str = re.sub(r\"\\$\", \" dollar \", str)  \n",
    "    \n",
    "    #str = re.sub('[^\\x00-\\x7F]+', ' ', str)\n",
    "    # this will blank any of !\"#%&\\'()*+,./:;<=>?[\\\\]^_`{|}~-@\n",
    "    # Note the @ is dubious : won't we loose some emails ??\n",
    "    # and $ is replaced by dollar before\n",
    "    str = str.translate(FINAL_PUNC_CLEANER)\n",
    "\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Training data + clean + all stop words features: Load or rebuild training_clean_all_stop_words_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/training_clean_all_stop_words_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:training_clean_all_stop_words_features contains 404290 lines in 1.8 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Challenge data + clean + all stop words features: Load or rebuild challenge_clean_all_stop_words_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>!!!!! ../kxen_nlu/challenge_clean_all_stop_words_features.pkl is cached!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done:challenge_clean_all_stop_words_features contains 2345796 lines in 26.3 s</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "def clean_build_no_stopwords_features_one_row(q1,q2,stopwords):\n",
    "    q1 = clean_string(str(q1))\n",
    "    q2 = clean_string(str(q2))\n",
    "    q1 = set([w for w in q1.split() if w not in stopwords])\n",
    "    len_q1 = len(q1)\n",
    "    q2 = set([w for w in q2.split() if w not in stopwords])\n",
    "    len_q2 = len(q2)\n",
    "\n",
    "    common = q1&q2\n",
    "    len_common = len(common)\n",
    "\n",
    "    uncommon_q1 = q1-common\n",
    "    len_uncommon_q1 = len(uncommon_q1)\n",
    "\n",
    "    uncommon_q2 = q2-common\n",
    "    len_uncommon_q2 = len(uncommon_q2)\n",
    "    #       0     1           2            2         4               5               6      7      8                        9                        10\n",
    "    return common,uncommon_q1,uncommon_q2,len_common,len_uncommon_q1,len_uncommon_q2,len_q1,len_q2,len_common/max(1,len_q1),len_common/max(1,len_q2),len_common/max(1,(len_q1+len_q2))\n",
    "\n",
    "\n",
    "def clean_build_all_stop_words_features(dataframe): \n",
    "    print_warning('Clean text and compute all features in one shot')\n",
    "    add_column_from_columns(dataframe,'temp',lambda r: clean_build_no_stopwords_features_one_row(r.question1,r.question2,all_stop_words))\n",
    "    \n",
    "    print_warning('Extract nb_words_question1')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question1','temp',lambda x: x[6])\n",
    "    print_warning('Extract nb_words_question2')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question2','temp',lambda x: x[7])\n",
    "\n",
    "  \n",
    "    print_warning('Extract Nb common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words','temp',lambda x: x[3])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question1')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words/clean_all_nb_words_question1','temp',lambda x: x[8])\n",
    "\n",
    "    print_warning('Extract Nb common words/nb words in question2')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words/clean_all_nb_words_question2','temp',lambda x: x[9])\n",
    "\n",
    "    print_warning('Extract Nb words in question1 not in common words')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question1-clean_all_common_words','temp',lambda x: x[4])\n",
    "\n",
    "    print_warning('Extract Nb words in question2 not in common words')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_words_question2-clean_all_common_words','temp',lambda x: x[5])\n",
    "\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_column(dataframe,'clean_all_nb_common_words/(clean_all_nb_words_question1+clean_all_nb_words_question2)','temp',lambda x: x[10])\n",
    "    \n",
    "    print_warning('Extract common words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'clean_all_common_words','temp',lambda x: x[0])\n",
    "    \n",
    "    print_warning('Extract uncommon words in question1')\n",
    "    add_column_from_column(dataframe,'clean_all_uncommon_words_question1','temp',lambda x: x[1])\n",
    "\n",
    "    print_warning('Extract uncommon words in question2')\n",
    "    add_column_from_column(dataframe,'clean_all_uncommon_words_question2','temp',lambda x: x[2])\n",
    "    dataframe = dataframe.drop (columns='temp')  \n",
    "    return dataframe\n",
    "\n",
    "train_dataframe = load_or_build_dataframe('Training data + clean + all stop words features','training_clean_all_stop_words_features',clean_build_all_stop_words_features,train_dataframe)\n",
    "challenge_dataframe = load_or_build_dataframe('Challenge data + clean + all stop words features','challenge_clean_all_stop_words_features',clean_build_all_stop_words_features,challenge_dataframe)"
   ]
  },
  {
   "source": [
    "### Lemmatized questions\n",
    "\n",
    "Lemmatization allows to retrieve the root word (the lemme) of any word : trainings->train, will->be,...\n",
    "This another way to cleanup lists of shared/unshared words.\n",
    "\n",
    "Lemmatisation has been done in notebod spacy_preprocessing and is quite heavy.\n",
    "We just get back the results of this notebook in cache and glue them to our datasets\n",
    "\n",
    "### Entities\n",
    "Named entities are high level entities like a person, a place, an organisation,... detected in a text. Spacy can detect such objects (quit heavy operation). \n",
    "We detect these entities for each questions and see if there are some common ones. Idea is if questions talk about the same things with the same words, there are better chances they are duplicated. \n",
    "We have computed all these numbers (and ratios to get rid of scale issues with long questions) in the notebook spacy_preprocessing and just glue the results to our datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>train</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Glue lemmes & entities</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Glue newsgroups</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Glue spacy similarities</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>challenge</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Glue lemmes & entities</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Glue newsgroups</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Glue spacy similarities</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    }
   ],
   "source": [
    "def glue_lemmes_entities_newsgroups_similarities(dataframe,dataset_name):\n",
    "    print_warning(dataset_name)\n",
    "    lemmatized_entities = load_global_dataframe(dataset_name+'_final_lemmatized_entities_features')\n",
    "   # ugly but easy : I prefer this order of fields\n",
    "    lemmatized_entities = lemmatized_entities.reindex(\n",
    "        columns=[\n",
    "        'lemmatized_nb_words_question1',\n",
    "        'lemmatized_nb_words_question2',\n",
    "        'lemmatized_nb_common_words',\n",
    "        'lemmatized_nb_common_words/lemmatized_nb_words_question1',\n",
    "        'lemmatized_nb_common_words/lemmatized_nb_words_question2',\n",
    "        'lemmatized_nb_words_question1-lemmatized_common_words',\n",
    "        'lemmatized_nb_words_question2-lemmatized_common_words',\n",
    "        'lemmatized_nb_common_words/(lemmatized_nb_words_question1+lemmatized_nb_words_question2)',\n",
    "        'nb_entities_GPE_question1', \n",
    "        'nb_entities_PERSON_question1',\n",
    "        'nb_entities_PRODUCT_question1',\n",
    "        'nb_entities_ORG_question1',\n",
    "        'nb_entities_DATE_question1',\n",
    "        'nb_entities_NORP_question1',\n",
    "        'nb_entities_WORK_OF_ART_question1',\n",
    "        'nb_entities_LANGUAGE_question1',\n",
    "        'nb_entities_EVENT_question1',\n",
    "        'nb_entities_FAC_question1',\n",
    "        'nb_entities_LAW_question1',\n",
    "        'nb_entities_LOC_question1',\n",
    "        'nb_entities_GPE_question2',\n",
    "        'nb_entities_PERSON_question2',\n",
    "        'nb_entities_PRODUCT_question2',\n",
    "        'nb_entities_ORG_question2',\n",
    "        'nb_entities_DATE_question2',\n",
    "        'nb_entities_NORP_question2',\n",
    "        'nb_entities_WORK_OF_ART_question2',\n",
    "        'nb_entities_LANGUAGE_question2',\n",
    "        'nb_entities_EVENT_question2',\n",
    "        'nb_entities_FAC_question2',\n",
    "        'nb_entities_LAW_question2',\n",
    "        'nb_entities_LOC_question2',\n",
    "        'nb_entities_common_GPE',\n",
    "        'nb_entities_common_PERSON',\n",
    "        'nb_entities_common_PRODUCT',\n",
    "        'nb_entities_common_ORG',\n",
    "        'nb_entities_common_DATE',\n",
    "        'nb_entities_common_NORP',\n",
    "        'nb_entities_common_WORK_OF_ART',\n",
    "        'nb_entities_common_LANGUAGE',\n",
    "        'nb_entities_common_EVENT',\n",
    "        'nb_entities_common_FAC',\n",
    "        'nb_entities_common_LAW',\n",
    "        'nb_entities_common_LOC',\n",
    "        'ratio_nb_entities_common_GPE',\n",
    "        'ratio_nb_entities_common_PERSON',\n",
    "        'ratio_nb_entities_common_PRODUCT',\n",
    "        'ratio_nb_entities_common_ORG',\n",
    "        'ratio_nb_entities_common_DATE',\n",
    "        'ratio_nb_entities_common_NORP',\n",
    "        'ratio_nb_entities_common_WORK_OF_ART',\n",
    "        'ratio_nb_entities_common_LANGUAGE',\n",
    "        'ratio_nb_entities_common_EVENT',\n",
    "        'ratio_nb_entities_common_FAC',\n",
    "        'ratio_nb_entities_common_LAW',\n",
    "        'ratio_nb_entities_common_LOC',\n",
    "        ])\n",
    "    print_warning('Glue lemmes & entities')\n",
    "    # The set_index option was specially tricky. Made me mad for a while\n",
    "    res = pandas.concat([dataframe,lemmatized_entities.set_index(dataframe.index)],axis=1)\n",
    "    del lemmatized_entities\n",
    "\n",
    "    #copy_from_pandas_store_if_missing(dataset_name+'_newsgroup_proba')\n",
    "    print_warning('Glue newsgroups')\n",
    "    newsgroup_proba = load_global_dataframe(dataset_name+'_newsgroup_proba')\n",
    "    res = pandas.concat([res,newsgroup_proba.set_index(dataframe.index)],axis=1)\n",
    "    del newsgroup_proba\n",
    "\n",
    "    #copy_from_pandas_store_if_missing(dataset_name+'_spacy_similarity')\n",
    "    print_warning('Glue spacy similarities')\n",
    "    spacy_similarity = load_global_dataframe(dataset_name+'_spacy_similarity')\n",
    "    # Glue similarities to dataframe\n",
    "    # We have only a serie so it is much easier than with a full dataframe\n",
    "    res['spacy_similarity'] = spacy_similarity\n",
    "    del spacy_similarity\n",
    "    # Need to have proper order of fields : Quite brutal but so easy\n",
    "    return res\n",
    "\n",
    "    \n",
    "train_dataframe = glue_lemmes_entities_newsgroups_similarities(train_dataframe,'train')\n",
    "challenge_dataframe = glue_lemmes_entities_newsgroups_similarities(challenge_dataframe,'challenge')"
   ]
  },
  {
   "source": [
    "Time to check all the columns we have"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>404290 lines of 113 columns of train:</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate', 'weight', 'nb_words_question1', 'nb_words_question2', 'nb_common_words', 'nb_common_words/nb_words_question1', 'nb_common_words/nb_words_question2', 'nb_words_question1-common_words', 'nb_words_question2-common_words', 'nb_common_words/(nb_words_question1+nb_words_question2)', 'nltk_nb_words_question1', 'nltk_nb_words_question2', 'nltk_nb_common_words', 'nltk_nb_common_words/nltk_nb_words_question1', 'nltk_nb_common_words/nltk_nb_words_question2', 'nltk_nb_words_question1-nltk_common_words', 'nltk_nb_words_question2-nltk_common_words', 'nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)', 'all_nb_words_question1', 'all_nb_words_question2', 'all_nb_common_words', 'all_nb_common_words/all_nb_words_question1', 'all_nb_common_words/all_nb_words_question2', 'all_nb_words_question1-all_common_words', 'all_nb_words_question2-all_common_words', 'all_nb_common_words/(all_nb_words_question1+all_nb_words_question2)', 'clean_all_nb_words_question1', 'clean_all_nb_words_question2', 'clean_all_nb_common_words', 'clean_all_nb_common_words/clean_all_nb_words_question1', 'clean_all_nb_common_words/clean_all_nb_words_question2', 'clean_all_nb_words_question1-clean_all_common_words', 'clean_all_nb_words_question2-clean_all_common_words', 'clean_all_nb_common_words/(clean_all_nb_words_question1+clean_all_nb_words_question2)', 'clean_all_common_words', 'clean_all_uncommon_words_question1', 'clean_all_uncommon_words_question2', 'lemmatized_nb_words_question1', 'lemmatized_nb_words_question2', 'lemmatized_nb_common_words', 'lemmatized_nb_common_words/lemmatized_nb_words_question1', 'lemmatized_nb_common_words/lemmatized_nb_words_question2', 'lemmatized_nb_words_question1-lemmatized_common_words', 'lemmatized_nb_words_question2-lemmatized_common_words', 'lemmatized_nb_common_words/(lemmatized_nb_words_question1+lemmatized_nb_words_question2)', 'nb_entities_GPE_question1', 'nb_entities_PERSON_question1', 'nb_entities_PRODUCT_question1', 'nb_entities_ORG_question1', 'nb_entities_DATE_question1', 'nb_entities_NORP_question1', 'nb_entities_WORK_OF_ART_question1', 'nb_entities_LANGUAGE_question1', 'nb_entities_EVENT_question1', 'nb_entities_FAC_question1', 'nb_entities_LAW_question1', 'nb_entities_LOC_question1', 'nb_entities_GPE_question2', 'nb_entities_PERSON_question2', 'nb_entities_PRODUCT_question2', 'nb_entities_ORG_question2', 'nb_entities_DATE_question2', 'nb_entities_NORP_question2', 'nb_entities_WORK_OF_ART_question2', 'nb_entities_LANGUAGE_question2', 'nb_entities_EVENT_question2', 'nb_entities_FAC_question2', 'nb_entities_LAW_question2', 'nb_entities_LOC_question2', 'nb_entities_common_GPE', 'nb_entities_common_PERSON', 'nb_entities_common_PRODUCT', 'nb_entities_common_ORG', 'nb_entities_common_DATE', 'nb_entities_common_NORP', 'nb_entities_common_WORK_OF_ART', 'nb_entities_common_LANGUAGE', 'nb_entities_common_EVENT', 'nb_entities_common_FAC', 'nb_entities_common_LAW', 'nb_entities_common_LOC', 'ratio_nb_entities_common_GPE', 'ratio_nb_entities_common_PERSON', 'ratio_nb_entities_common_PRODUCT', 'ratio_nb_entities_common_ORG', 'ratio_nb_entities_common_DATE', 'ratio_nb_entities_common_NORP', 'ratio_nb_entities_common_WORK_OF_ART', 'ratio_nb_entities_common_LANGUAGE', 'ratio_nb_entities_common_EVENT', 'ratio_nb_entities_common_FAC', 'ratio_nb_entities_common_LAW', 'ratio_nb_entities_common_LOC', 'proba_religion_question1', 'proba_computers_question1', 'proba_forsale_question1', 'proba_vehicles_question1', 'proba_sport_question1', 'proba_science_question1', 'proba_politics_question1', 'proba_religion_question2', 'proba_computers_question2', 'proba_forsale_question2', 'proba_vehicles_question2', 'proba_sport_question2', 'proba_science_question2', 'proba_politics_question2', 'spacy_similarity']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>2345796 lines of 109 columns of challenge:</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['test_id', 'question1', 'question2', 'nb_words_question1', 'nb_words_question2', 'nb_common_words', 'nb_common_words/nb_words_question1', 'nb_common_words/nb_words_question2', 'nb_words_question1-common_words', 'nb_words_question2-common_words', 'nb_common_words/(nb_words_question1+nb_words_question2)', 'nltk_nb_words_question1', 'nltk_nb_words_question2', 'nltk_nb_common_words', 'nltk_nb_common_words/nltk_nb_words_question1', 'nltk_nb_common_words/nltk_nb_words_question2', 'nltk_nb_words_question1-nltk_common_words', 'nltk_nb_words_question2-nltk_common_words', 'nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)', 'all_nb_words_question1', 'all_nb_words_question2', 'all_nb_common_words', 'all_nb_common_words/all_nb_words_question1', 'all_nb_common_words/all_nb_words_question2', 'all_nb_words_question1-all_common_words', 'all_nb_words_question2-all_common_words', 'all_nb_common_words/(all_nb_words_question1+all_nb_words_question2)', 'clean_all_nb_words_question1', 'clean_all_nb_words_question2', 'clean_all_nb_common_words', 'clean_all_nb_common_words/clean_all_nb_words_question1', 'clean_all_nb_common_words/clean_all_nb_words_question2', 'clean_all_nb_words_question1-clean_all_common_words', 'clean_all_nb_words_question2-clean_all_common_words', 'clean_all_nb_common_words/(clean_all_nb_words_question1+clean_all_nb_words_question2)', 'clean_all_common_words', 'clean_all_uncommon_words_question1', 'clean_all_uncommon_words_question2', 'lemmatized_nb_words_question1', 'lemmatized_nb_words_question2', 'lemmatized_nb_common_words', 'lemmatized_nb_common_words/lemmatized_nb_words_question1', 'lemmatized_nb_common_words/lemmatized_nb_words_question2', 'lemmatized_nb_words_question1-lemmatized_common_words', 'lemmatized_nb_words_question2-lemmatized_common_words', 'lemmatized_nb_common_words/(lemmatized_nb_words_question1+lemmatized_nb_words_question2)', 'nb_entities_GPE_question1', 'nb_entities_PERSON_question1', 'nb_entities_PRODUCT_question1', 'nb_entities_ORG_question1', 'nb_entities_DATE_question1', 'nb_entities_NORP_question1', 'nb_entities_WORK_OF_ART_question1', 'nb_entities_LANGUAGE_question1', 'nb_entities_EVENT_question1', 'nb_entities_FAC_question1', 'nb_entities_LAW_question1', 'nb_entities_LOC_question1', 'nb_entities_GPE_question2', 'nb_entities_PERSON_question2', 'nb_entities_PRODUCT_question2', 'nb_entities_ORG_question2', 'nb_entities_DATE_question2', 'nb_entities_NORP_question2', 'nb_entities_WORK_OF_ART_question2', 'nb_entities_LANGUAGE_question2', 'nb_entities_EVENT_question2', 'nb_entities_FAC_question2', 'nb_entities_LAW_question2', 'nb_entities_LOC_question2', 'nb_entities_common_GPE', 'nb_entities_common_PERSON', 'nb_entities_common_PRODUCT', 'nb_entities_common_ORG', 'nb_entities_common_DATE', 'nb_entities_common_NORP', 'nb_entities_common_WORK_OF_ART', 'nb_entities_common_LANGUAGE', 'nb_entities_common_EVENT', 'nb_entities_common_FAC', 'nb_entities_common_LAW', 'nb_entities_common_LOC', 'ratio_nb_entities_common_GPE', 'ratio_nb_entities_common_PERSON', 'ratio_nb_entities_common_PRODUCT', 'ratio_nb_entities_common_ORG', 'ratio_nb_entities_common_DATE', 'ratio_nb_entities_common_NORP', 'ratio_nb_entities_common_WORK_OF_ART', 'ratio_nb_entities_common_LANGUAGE', 'ratio_nb_entities_common_EVENT', 'ratio_nb_entities_common_FAC', 'ratio_nb_entities_common_LAW', 'ratio_nb_entities_common_LOC', 'proba_religion_question1', 'proba_computers_question1', 'proba_forsale_question1', 'proba_vehicles_question1', 'proba_sport_question1', 'proba_science_question1', 'proba_politics_question1', 'proba_religion_question2', 'proba_computers_question2', 'proba_forsale_question2', 'proba_vehicles_question2', 'proba_sport_question2', 'proba_science_question2', 'proba_politics_question2', 'spacy_similarity']\n"
     ]
    }
   ],
   "source": [
    "def show_columns(dataframe,dataset_name):\n",
    "    print_section('%d lines of %d columns of %s:' % (len(dataframe),len(dataframe.columns),dataset_name))\n",
    "    print(dataframe.columns.tolist())\n",
    "\n",
    "show_columns(train_dataframe,'train')\n",
    "show_columns(challenge_dataframe,'challenge')\n"
   ]
  },
  {
   "source": [
    "Let's play a little bit with nlu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "How much these adjusted features are better than basic ones ?\n",
    "\n",
    "Let's check AUCs "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(y_true=dataframe['is_duplicate'],y_score=dataframe[column_name])\n",
    "\n",
    "def simple_weighted_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(y_true=dataframe['is_duplicate'],y_score=dataframe[column_name],sample_weight=dataframe['weight'])\n",
    "\n",
    "def show_AUC(dataframe,column_name):\n",
    "    if 'weight' in dataframe.columns:\n",
    "        print_bullet('Weighted AUC %s %f' % (column_name,simple_weighted_AUC(dataframe,column_name)))\n",
    "    else:\n",
    "        print_bullet('AUC %s %f' % (column_name,simple_AUC(dataframe,column_name)))\n",
    "        \n",
    "\n",
    "def display_simple_AUC(dataframe,column_name):\n",
    "    palette = sns.color_palette()\n",
    "    as_hist(dataframe[column_name][dataframe['is_duplicate']==1],bins=\"blocks\",color=palette[3],label='Same',histtype='step')\n",
    "    as_hist(dataframe[column_name][dataframe['is_duplicate']==0],bins=\"blocks\",color=palette[2],label='Different',alpha = 0.75,histtype='step')\n",
    "    plot.title('AUC %s : %f' % (column_name,simple_AUC(dataframe,column_name)) , fontsize=10)\n",
    "    plot.xlabel(column_name)\n",
    "    plot.ylabel('Nb')\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "\n",
    "def show_all_simple_AUC(dataframe):\n",
    "    all =  all_numeric_columns(dataframe)\n",
    "    print_section( 'Show AUC on %d unique features' % len(all))\n",
    "    for name in all:\n",
    "        show_AUC(dataframe,name)\n",
    "\n",
    "\n",
    "show_all_simple_AUC(train_dataframe)\n"
   ]
  },
  {
   "source": [
    "Just a couple of graphs to see how much adjusted features are different than basic ones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure(figsize=(15,10))\n",
    "sns.distplot(train_dataframe[train_dataframe['is_duplicate']==1]['nb_common_words/(nb_words_question1+nb_words_question2)'], bins=30, label='duplicate', kde=True)  \n",
    "sns.distplot(train_dataframe[train_dataframe['is_duplicate']==0]['nb_common_words/(nb_words_question1+nb_words_question2)'], bins=30,   label='not duplicate', kde=True)\n",
    "plot.title('Distribution of nb_common_words/(nb_words_question1+nb_words_question2) - AUC %.3f' %simple_weighted_AUC(train_dataframe,'nb_common_words/(nb_words_question1+nb_words_question2)'))\n",
    "plot.grid(True)\n",
    "plot.legend()\n",
    "plot_save('distribution_ratio_no_stopwords')\n",
    "\n",
    "plot.figure(figsize=(15,10))\n",
    "sns.distplot(train_dataframe[train_dataframe['is_duplicate']==1]['nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)'], bins=30, label='duplicate', kde=True)    \n",
    "sns.distplot(train_dataframe[train_dataframe['is_duplicate']==0]['nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)'], bins=30,   label='not duplicate', kde=True)\n",
    "plot.title('Distribution of nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)) - AUC %.3f' %simple_weighted_AUC(train_dataframe,'nltk_nb_common_words/(nltk_nb_words_question1+nltk_nb_words_question2)'))\n",
    "plot.grid(True)\n",
    "plot.legend()\n",
    "plot_save('distribution_ratio_nltk_stopwords')\n"
   ]
  },
  {
   "source": [
    "Just focus on the correlation matrix of non semantic features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def show_correlation_matrix(input_df,columns):\n",
    "        correlation_matrix = input_df[columns].corr().abs()\n",
    "        print(correlation_matrix['is_duplicate'].nlargest(len(columns)))\n",
    "\n",
    "        correlation_matrix_without_is_duplicate = correlation_matrix['is_duplicate'][correlation_matrix['is_duplicate']<1]\n",
    "        highest_correlated_feature = correlation_matrix_without_is_duplicate.nlargest(1).index[0]\n",
    "        highest_correlation = correlation_matrix_without_is_duplicate.nlargest(1)[0]\n",
    "        if highest_correlation >0.2:\n",
    "            if highest_correlation < 0.5:\n",
    "                print_warning('%s is the most correlated with target but %f is quite weak' % (highest_correlated_feature,highest_correlation))\n",
    "            else:\n",
    "                print_info('%s is the most correlated with target but %f is very weak' % (highest_correlated_feature,highest_correlation))\n",
    "        else:\n",
    "            print_alert('%s is the most correlated with target and %f is quite big' % (highest_correlated_feature,highest_correlation))\n",
    "        plot.figure(figsize=(30, 30))\n",
    "        plot.title('Correlation matrix: non semantic features')\n",
    "        sns.heatmap(correlation_matrix,annot=True,cbar=True,square=True,cmap='coolwarm',mask = numpy.triu(correlation_matrix),xticklabels=False)\n",
    "        \n",
    "    NON_SEMANTIC_FEATURES = [c for c in all_numeric_columns(train_dataframe) if 'entities' not in c and 'news' not in c and 'proba' not in c and 'sim' not in c] \n",
    "    NON_SEMANTIC_FEATURES.append('is_duplicate')\n",
    "\n",
    "    show_correlation_matrix(train_dataframe,NON_SEMANTIC_FEATURES)\n",
    "    plot_save('correlation_matrix_non_semantic_features')"
   ]
  },
  {
   "source": [
    "Let's recap our available features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES=all_numeric_columns(train_dataframe)\n",
    "print_info('We have %d features' % len(ALL_FEATURES))\n",
    "display(pandas.DataFrame(ALL_FEATURES,columns=['feature']))"
   ]
  },
  {
   "source": [
    "Exploring each combination of these 103 features (like we did in a first experiment with basic features) is not practical. We will just add one field at a time, build a model, keep metrics and figure what is happening."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A bunch of code to build xgboost models with basic set of parameters\n",
    "The criteria to stop learning is aligned to kaggle's one: *logloss*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple XGBoost code\n",
    "# \n",
    "\n",
    "import xgboost \n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta' : 0.02,\n",
    "    'max_depth':3 \n",
    "    }\n",
    "\n",
    "def print_res_xgboost(infos):\n",
    "    # Glurk\n",
    "    print_info(' | '.join([('%s '+f) % (k,infos[k]) for k,f in INFO_XGBOOST_MODEL.items() if k in infos]))\n",
    "\n",
    "# 80% training 20% validation\n",
    "def build_XGBoost_model_80_20(training,target,column_names,show=True):\n",
    "    input_train,input_test,target_train,target_test = train_test_split(training,target,random_state=42,test_size=0.2)\n",
    "    final_input_train = input_train[list(column_names)]\n",
    "    final_input_test = input_test[list(column_names)]\n",
    "    train_dm = xgboost.DMatrix(final_input_train, label = target_train, weight = input_train[WEIGHT],nthread = -1)\n",
    "    validation_dm = xgboost.DMatrix(final_input_test, label = target_test,weight = input_test[WEIGHT])\n",
    "    watchlist = [(train_dm, 'train'), (validation_dm, 'valid')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(params,train_dm,400,watchlist,early_stopping_rounds=50,verbose_eval=verbose)\n",
    "    infos = compute_metrics_model_xgboost(model,final_input_test,target_test,sample_weight = input_test[WEIGHT],show=show)\n",
    "    duration = time.time()-start\n",
    "    infos.update({'time':duration})\n",
    "    if show:      \n",
    "        print_res_xgboost(infos)\n",
    "    return  infos\n",
    "\n",
    "# Pushing the limits :100 % training no validation !!\n",
    "# Just in case it allows to gain small digits in kaggle score...\n",
    "# Actually not used ...\n",
    "def build_XGBoost_model_100_0(training,target,column_names,show=True):\n",
    "    final_train = training[list(column_names)]\n",
    "    final_target = target\n",
    "    final_weight = training[WEIGHT]\n",
    "\n",
    "    train_dm = xgboost.DMatrix(final_train, label = final_target, weight = final_weight,nthread = -1)\n",
    "    watchlist = [(train_dm, 'train')]\n",
    "\n",
    "    start = time.time()\n",
    "    if show:\n",
    "        verbose = 10\n",
    "    else:\n",
    "        verbose = 0\n",
    "    model = xgboost.train(params,train_dm,400,watchlist,early_stopping_rounds=50,verbose_eval=verbose)\n",
    "    duration = time.time()-start\n",
    "    infos = compute_metrics_model_xgboost(model,final_train,final_target,sample_weight = final_weight,show=show)\n",
    "    infos.update({'time':duration})\n",
    "    if show:      \n",
    "        print_res_xgboost(infos)\n",
    "    return  infos\n",
    "\n",
    "def compute_metrics_model_xgboost(model,input_df,target_df,sample_weight = None,show = True):\n",
    "    final_input = xgboost.DMatrix(input_df)\n",
    "    prediction_proba_df = model.predict(final_input,ntree_limit=model.best_ntree_limit)\n",
    "    # Hum shouldn't we challenge this 50% threshold ?\n",
    "    prediction_df = numpy.where(prediction_proba_df>0.5,1,0)\n",
    "    res = metrics.classification_report(target_df,prediction_df,sample_weight = sample_weight,output_dict=True)\n",
    "    accuracy = res['accuracy']\n",
    "    score = res['weighted avg']['f1-score']\n",
    "    logloss_proba = metrics.log_loss(target_df,prediction_proba_df,sample_weight = sample_weight)\n",
    "    if show:\n",
    "        print_info('Classification report')\n",
    "        print(metrics.classification_report(target_df,prediction_df,sample_weight = sample_weight))\n",
    "    return {\n",
    "             'accuracy':accuracy,\n",
    "             'score':score,\n",
    "             'logloss_proba':logloss_proba,\n",
    "             'model':model\n",
    "           }\n",
    "\n",
    "INFO_XGBOOST_MODEL= {\n",
    "    'logloss_proba': '%.4f',\n",
    "    'score': '%.4f',\n",
    "    'accuracy': '%.4f',\n",
    "    'time': '%.2f'\n",
    "}\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "    small_train = train_dataframe.sample(1000,random_state=42)\n",
    "    print_section('Unitary test : playbox XGBoost 100_0')\n",
    "    res = build_XGBoost_model_100_0(small_train,small_train['is_duplicate'],all_numeric_columns(small_train))\n",
    "    print(res)\n",
    "\n",
    "if UNITARY_TEST:\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "    small_train = train_dataframe.sample(1000,random_state=42)\n",
    "    print_section('Unitary test : playbox XGBoost 80_20')\n",
    "    res = build_XGBoost_model_80_20(small_train,small_train['is_duplicate'],all_numeric_columns(small_train))\n",
    "    print(res)"
   ]
  },
  {
   "source": [
    "Before launching bigh models, let's check again what we have.\n",
    "Here are the numeric columns (ids and target have been removed) in train dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_numeric_columns(train_dataframe) == all_numeric_columns(challenge_dataframe), 'Oups train and challenge do not have the same numeric columns'\n",
    "print_info('train and challenge datasets have the same numeric columns ')\n",
    "i = 1\n",
    "for c in all_numeric_columns(train_dataframe):\n",
    "    print(i,c)\n",
    "    i +=1\n",
    "\n"
   ]
  },
  {
   "source": [
    "Let's check again the impact of entities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NB_ENTITIES=[c for c in all_numeric_columns(train_dataframe) if 'enti' in c and 'ratio' not in c]\n",
    "print(ALL_NB_ENTITIES)\n",
    "def sniff_entities(r):\n",
    "    for c in ALL_NB_ENTITIES:\n",
    "        if r[c]>0:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "res = train_dataframe.progress_apply(sniff_entities,axis=1)\n",
    "print_info('%.2f %% of pairs have some detected entities' % (res.mean()*100.))\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Another bunch of code. Just wrappers so we do in one shot:\n",
    "* generate a set of models given a set of columns\n",
    "* keep their results in memory\n",
    "* save all models to excel\n",
    "* dump everything (including the models) in a cache so all previous steps can be done in a sec\n",
    "* find the best model (related to logloss)\n",
    "* generate a file of predictions suitable for kaggle\n",
    "* prepare everything for a kaggle submission\n",
    "* show the statement to do the submission to kaggle\n",
    "\n",
    "\n",
    "Current exploration methods available, given n columns\n",
    "* generate models using from 1 to n columns (24 columns will generate 24 models)\n",
    "* only one model with all the columns provided as parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_result(results,metric):\n",
    "    if 'logloss' in metric:\n",
    "        return results.nsmallest(1,metric)\n",
    "    else:\n",
    "        return results.nlargest(1,metric)\n",
    "        \n",
    "def apply_xgboost(model,columns,input_df):\n",
    "    res = pandas.DataFrame()\n",
    "    res['test_id'] = input_df['test_id']\n",
    "    res['is_duplicate'] = model.predict(xgboost.DMatrix(input_df[columns]))\n",
    "    return res\n",
    "\n",
    "def submit_best_result(results,input_df,file_name,kaggle_message,metric='logloss_proba'):\n",
    "    start = time.time()\n",
    "    print_section(\"Scoring XGBoost model with best %s: %s\" % (metric,kaggle_message))\n",
    "    csv_file_name = absolute_env_file_name(file_name,ext='.csv')\n",
    "    print_info('Generating scores in %s' % csv_file_name)\n",
    "    best = find_best_result(results,metric)\n",
    "    model = best['model'][0]\n",
    "    columns = best['columns'][0]\n",
    "    prediction = apply_xgboost(model,columns,input_df)\n",
    "    prediction.to_csv(csv_file_name,index=False)\n",
    "    print_info('Zipping file')\n",
    "    absolute_file_name_zip = zip_file_and_delete(csv_file_name)\n",
    "    print_done('Done',top=start)\n",
    "    print_info('%s is ready' % absolute_file_name_zip)\n",
    "    print_warning('Use this commands to submit apply results to kaggle')\n",
    "    print_warning('kaggle competitions submit quora-question-pairs -f \"%s\" -m \"%s %s\"' % (absolute_file_name_zip,EXPERIMENT,kaggle_message))\n",
    "    return prediction\n",
    "\n",
    "def n_columns(columns,n):\n",
    "    return columns[0:n]\n",
    "\n",
    "def xgboost_80_20_one_more_field_at_a_time(dataframe,columns_to_explore,explore=True,show=False):\n",
    "    if explore:\n",
    "        print_section(\"XGBoost (80,20) on 1 to %d fields\" % len(columns_to_explore))\n",
    "    else:\n",
    "        print_section(\"XGBoost (80,20) on %d fields\" % len(columns_to_explore))\n",
    "    start = time.time()\n",
    "    res_final = dict()\n",
    "    min_log_loss = 1000\n",
    "    best = 0\n",
    "    best_cols =''\n",
    "    # a shortcut is possible\n",
    "    if explore:\n",
    "        first_nb = 1\n",
    "    else:\n",
    "        first_nb=len(columns_to_explore)\n",
    "    for i in tqdm(range(first_nb,len(columns_to_explore)+1)):\n",
    "        columns = n_columns(columns_to_explore,i)\n",
    "        res = build_XGBoost_model_80_20(dataframe,dataframe['is_duplicate'],columns,show = show)\n",
    "        cur_logloss = res['logloss_proba']\n",
    "        cur_accuracy = res['accuracy']\n",
    "        cur_score = res['score']\n",
    "        if cur_logloss<min_log_loss:\n",
    "            print_info(\"%d:%.4f %.4f %.4f %s\" % (i,cur_logloss,cur_accuracy,cur_score,columns))\n",
    "            min_log_loss = cur_logloss\n",
    "            best_cols = columns\n",
    "            best = i\n",
    "        else:\n",
    "            print_warning(\"%d:%.4f %.4f %.4f %s\" % (i,cur_logloss,cur_accuracy,cur_score,columns))\n",
    "        res.update( {'columns':columns})\n",
    "        res_final.update({str(i):res})\n",
    "    print_info('Best: %d %.4f %s' % (best,min_log_loss,best_cols))\n",
    "    print_done('Done',top=start)\n",
    "    return pandas.DataFrame.from_dict(res_final, orient='index')\n",
    "\n",
    "# try to reload from cache an exploration\n",
    "# if not available, redo it and save results in cache\n",
    "# find the best result according to a metric (default logloss_proba)\n",
    "# and submit it to kaggle\n",
    "def study_models_one_more_field_at_a_time(message,tag,columns,train_df,challenge_df,explore=True,show=False):\n",
    "    print_section(message)\n",
    "    results = load_or_build_dataframe('Rebuild XGBoost models',tag + '_results',lambda df:xgboost_80_20_one_more_field_at_a_time(df,columns,explore=explore,show=show),train_df)\n",
    "    save_models_dict_to_excel(results,tag=tag)\n",
    "    display(results.describe())\n",
    "    submit_best_result(results,challenge_df,'Best_'+tag,'Best ' + tag)\n",
    "    return results"
   ]
  },
  {
   "source": [
    "A little bit long but results are saved in a cache so next time will be fast"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_everything_results = study_models_one_more_field_at_a_time(\n",
    "    'all features_not_cleaned',\n",
    "    'all_features_not_cleaned',\n",
    "    ALL_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore = True,\n",
    "    show = False)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We graph the logloss obtained adding one field at a time from:\n",
    "* basic features on non processed data\n",
    "* same features with a first set of stop words removed\n",
    "* same features with more stop words removed\n",
    "* same features with more stop words after cleaning\n",
    "* same features with more stop words after cleaning and lemmatisation\n",
    "* Entities \n",
    "* newsgroups\n",
    "* similarity computed by spacy lib\n",
    "\n",
    "Obviously, this graph does not prove anything as another order of fields would generate a totally different graph but we can still say:\n",
    "* all preprocessing informations provide some enhancement\n",
    "* expanding list of stop words provides only marginal enhancement. **Again**, this does not mean features built with full list of stop words does not provide informations. It does provide a little bit more information to the ones already available\n",
    "* lemmatisation is useful in our context\n",
    "* entities do not provide much more informations\n",
    "* spacy similarity does enhance the model but not dramatically\n",
    "* optimising hyperparameters is quite efficient\n",
    "\n",
    "Challenging the need of a feature on the final model is done later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's start with all non semantic features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure(figsize=(10,10))\n",
    "\n",
    "pal=sns.color_palette()\n",
    "plot.ylim(0.3,0.5)\n",
    "plot.plot(numpy.arange(1,9,1),xgboost_everything_results['logloss_proba'][1:9],color=pal[1],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(1,9,1),xgboost_everything_results['logloss_proba'][1:9],label='basic',color=pal[1],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(8,17,1),xgboost_everything_results['logloss_proba'][8:17],color=pal[2],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(8,17,1),xgboost_everything_results['logloss_proba'][8:17],label='nltk stopwords',color=pal[2],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(16,25,1),xgboost_everything_results['logloss_proba'][16:25],color=pal[3],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(16,25,1),xgboost_everything_results['logloss_proba'][16:25],label='nltk +sklearn stopwords',color=pal[3],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(24,33,1),xgboost_everything_results['logloss_proba'][24:33],color=pal[4],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(24,33,1),xgboost_everything_results['logloss_proba'][24:33],label='nltk +sklearn stopwords+clean',color=pal[4],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(32,41,1),xgboost_everything_results['logloss_proba'][32:41],color=pal[5],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(32,41,1),xgboost_everything_results['logloss_proba'][32:41],label='nltk +sklearn stopwords+clean+lemme',color=pal[5],alpha=0.3)\n",
    "\n",
    "plot.title('xgboost on non semantic features: logloss/Nb fields')\n",
    "plot.xlabel('Nb fields')\n",
    "plot.grid(True)\n",
    "plot.legend()\n",
    "plot_save('xgboost_all_features_all_preprocess_non_semantic')"
   ]
  },
  {
   "source": [
    "Now, let's plot the model with semantic features : entities+newsgroup+similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot.figure(figsize=(10,10))\n",
    "\n",
    "pal=sns.color_palette()\n",
    "plot.ylim(0.3,0.5)\n",
    "plot.plot(numpy.arange(1,9,1),xgboost_everything_results['logloss_proba'][1:9],color=pal[1],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(1,9,1),xgboost_everything_results['logloss_proba'][1:9],label='basic',color=pal[1],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(8,17,1),xgboost_everything_results['logloss_proba'][8:17],color=pal[2],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(8,17,1),xgboost_everything_results['logloss_proba'][8:17],label='nltk stopwords',color=pal[2],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(16,25,1),xgboost_everything_results['logloss_proba'][16:25],color=pal[3],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(16,25,1),xgboost_everything_results['logloss_proba'][16:25],label='nltk +sklearn stopwords',color=pal[3],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(24,33,1),xgboost_everything_results['logloss_proba'][24:33],color=pal[4],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(24,33,1),xgboost_everything_results['logloss_proba'][24:33],label='nltk +sklearn stopwords+clean',color=pal[4],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(32,41,1),xgboost_everything_results['logloss_proba'][32:41],color=pal[5],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(32,41,1),xgboost_everything_results['logloss_proba'][32:41],label='nltk +sklearn stopwords+clean+lemme',color=pal[5],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(40,89,1),xgboost_everything_results['logloss_proba'][40:89],color=pal[6],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(40,89,1),xgboost_everything_results['logloss_proba'][40:89],label='nltk +sklearn stopwords+clean+entities',color=pal[6],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(88,103,1),xgboost_everything_results['logloss_proba'][88:103],color=pal[7],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(88,103,1),xgboost_everything_results['logloss_proba'][88:103],label='nltk +sklearn stopwords+clean+entities+newsgroups',color=pal[7],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(101,103,1),xgboost_everything_results['logloss_proba'][101:103],color=pal[8],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(101,103,1),xgboost_everything_results['logloss_proba'][101:103],label='nltk +sklearn stopwords+clean+entities+newsgroups+similarity',color=pal[8],alpha=0.3)\n",
    "plot.title('xgboost: logloss/Nb fields')\n",
    "plot.xlabel('Nb fields')\n",
    "plot.grid(True)\n",
    "plot.legend()\n",
    "plot_save('all_features_all_preprocess')"
   ]
  },
  {
   "source": [
    "We have now a model using all features (even if we know some features provide only small information)\n",
    "\n",
    "Let's optimize this model by searching bets set of hyperparameters.\n",
    "\n",
    "We have use hyperopt library to search the best model between 500 models exploring a vast set of parameters.\n",
    "\n",
    "Each model is evaluated on its logloss : we keep following the kaggle challenge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "def hyperopt_optimize(training,target,column_names,trials,show=False):\n",
    "\n",
    "    parameters_space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n",
    "        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "        # A problem with max_depth casted to float instead of int with\n",
    "        # the hp.quniform method.\n",
    "        'max_depth':  hp.choice('max_depth', numpy.arange(1, 14, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        'nthread': 8,\n",
    "        #'booster': 'gbtree',\n",
    "        #'tree_method': 'exact',\n",
    "        'seed': 42,\n",
    "        \"seed_per_iteration\": True,\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "\n",
    "    input_train,input_test,target_train,target_test = train_test_split(training,target,random_state=42,test_size=0.2)\n",
    "    final_input_train = input_train[list(column_names)]\n",
    "    final_input_test = input_test[list(column_names)]\n",
    "    train_dm = xgboost.DMatrix(final_input_train, label = target_train, weight = input_train[WEIGHT],nthread = -1)\n",
    "    validation_dm = xgboost.DMatrix(final_input_test, label = target_test,weight = input_test[WEIGHT])\n",
    "    watchlist = [(train_dm, 'train'), (validation_dm, 'valid')]\n",
    "\n",
    "    def eval_model(params):\n",
    "        num_round = int(params['n_estimators'])\n",
    "        del params['n_estimators']\n",
    "        model = xgboost.train(params,train_dm,num_round,watchlist,early_stopping_rounds=50,verbose_eval=show)\n",
    "        prediction_proba_df = model.predict(validation_dm,ntree_limit=model.best_iteration + 1)\n",
    "        logloss = metrics.log_loss(target_test,prediction_proba_df,sample_weight = input_test[WEIGHT])\n",
    "        return {'loss': logloss, 'status': STATUS_OK, 'model' : model, 'logloss':logloss}\n",
    "\n",
    "    # Use the hyperopt's fmin\n",
    "    start = time.time()\n",
    "    print_section('Finding hyperparameters')\n",
    "    best = fmin(eval_model, parameters_space, algo=tpe.suggest, \n",
    "                trials=trials, \n",
    "                max_evals=500)\n",
    "    print_done('Done',top=start)\n",
    "    return best"
   ]
  },
  {
   "source": [
    "Do not do it : it takes 10 hours on a 8 cores server"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trials = Trials()\n",
    "#best_xgboost_parameters = hyperopt_optimize(train_dataframe,train_dataframe['is_duplicate'],ALL_FEATURES,trials,show=False)"
   ]
  },
  {
   "source": [
    "Here are the best parameters:\n",
    "* colsample_bytree: 0.7000000000000001,\n",
    "* eta: 0.05,\n",
    "* gamma: 0.8500000000000001,\n",
    "* max_depth: 11,\n",
    "* min_child_weight: 3.0,\n",
    "* n_estimators: 868.0,\n",
    "* subsample: 0.9500000000000001}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgboost_parameters"
   ]
  },
  {
   "source": [
    "Here is the history of the hyperopt work"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hyperopt's sources ....\n",
    "plot.figure(figsize=(10,10))\n",
    "plot.scatter(range(len(trials.losses())), trials.losses())\n",
    "plot.xlabel(\"time\")\n",
    "plot.ylabel(\"logloss\")\n",
    "plot.grid(True)\n",
    "print(\"avg best error:\", best_err)\n",
    "plot.axhline(best_err, c=\"g\",label='avg best error')\n",
    "plot.legend()\n",
    "plot.title('Hyperopt on final model: logloss history')\n",
    "plot_save('hyperopt_history')\n",
    "\n",
    "#hyperopt.plotting.main_plot_histogram(trials)\n",
    "#hyperopt.plotting.main_plot_vars(trials)"
   ]
  },
  {
   "source": [
    "Let's redo this best model with our 'framework'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'colsample_bytree': 0.7000000000000001,\n",
    "    'eta': 0.05,\n",
    "    'gamma': 0.8500000000000001,\n",
    "    'max_depth': 11,\n",
    "    'min_child_weight': 3.0,\n",
    "    'n_estimators': 868.0,\n",
    "    'subsample': 0.9500000000000001}\n",
    "\n",
    "xgboost_best_optim = study_models_one_more_field_at_a_time(\n",
    "    'all features all_preprocess optim',\n",
    "    'all_features_all_preprocess_optim',\n",
    "    ALL_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore = False,\n",
    "    show = False)"
   ]
  },
  {
   "source": [
    "Let's add this optimized model to the previous plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "plot.figure(figsize=(10,10))\n",
    "\n",
    "pal=sns.color_palette()\n",
    "plot.ylim(0.28,0.5)\n",
    "plot.plot(numpy.arange(1,9,1),xgboost_everything_results['logloss_proba'][1:9],color=pal[1],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(1,9,1),xgboost_everything_results['logloss_proba'][1:9],label='basic',color=pal[1],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(8,17,1),xgboost_everything_results['logloss_proba'][8:17],color=pal[2],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(8,17,1),xgboost_everything_results['logloss_proba'][8:17],label='nltk stopwords',color=pal[2],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(16,25,1),xgboost_everything_results['logloss_proba'][16:25],color=pal[3],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(16,25,1),xgboost_everything_results['logloss_proba'][16:25],label='nltk +sklearn stopwords',color=pal[3],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(24,33,1),xgboost_everything_results['logloss_proba'][24:33],color=pal[4],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(24,33,1),xgboost_everything_results['logloss_proba'][24:33],label='nltk +sklearn stopwords+clean',color=pal[4],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(32,41,1),xgboost_everything_results['logloss_proba'][32:41],color=pal[5],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(32,41,1),xgboost_everything_results['logloss_proba'][32:41],label='nltk +sklearn stopwords+clean+lemme',color=pal[5],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(40,89,1),xgboost_everything_results['logloss_proba'][40:89],color=pal[6],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(40,89,1),xgboost_everything_results['logloss_proba'][40:89],label='nltk +sklearn stopwords+clean+entities',color=pal[6],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(88,103,1),xgboost_everything_results['logloss_proba'][88:103],color=pal[7],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(88,103,1),xgboost_everything_results['logloss_proba'][88:103],label='nltk +sklearn stopwords+clean+entities+newsgroups',color=pal[7],alpha=0.3)\n",
    "\n",
    "plot.plot(numpy.arange(101,103,1),xgboost_everything_results['logloss_proba'][101:103],color=pal[8],alpha=0.3)\n",
    "plot.fill_between(numpy.arange(101,103,1),xgboost_everything_results['logloss_proba'][101:103],label='nltk +sklearn stopwords+clean+entities+newsgroups+similarity',color=pal[8],alpha=0.3)\n",
    "\n",
    "plot.scatter(102,0.2867,color=\"red\",label='optimized model')\n",
    "plot.title('xgboost: logloss/Nb fields')\n",
    "plot.xlabel('Nb fields')\n",
    "plot.grid(True)\n",
    "plot.legend()\n",
    "plot_save('all_features_all_preprocess_optimized')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "What does Kaggle think about that model ?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_submissions(all_submissions,10)"
   ]
  },
  {
   "source": [
    "Ok We have a model quite acceptable for kaggle (in the first 33%)\n",
    "\n",
    "Is it really a good model ? Kaggle score does not imply it is a good model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost_best_optim['model'][0]\n",
    "# do the usual split (80,20)\n",
    "input_train,input_test,target_train,target_test = train_test_split(train_dataframe,train_dataframe['is_duplicate'],random_state=42,test_size=0.2)\n",
    "test_probabilities = model.predict(xgboost.DMatrix(input_test[ALL_FEATURES]),ntree_limit=model.best_ntree_limit)\n",
    "train_probabilities = model.predict(xgboost.DMatrix(input_train[ALL_FEATURES]),ntree_limit=model.best_ntree_limit)\n",
    "challenge_probabilities = model.predict(xgboost.DMatrix(challenge_dataframe[ALL_FEATURES]),ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "# Apply the standard threshold\n",
    "test_predictions = numpy.where(test_probabilities>0.5,1,0)\n",
    "train_predictions = numpy.where(train_probabilities>0.5,1,0)\n",
    "challenge_predictions = numpy.where(challenge_probabilities>0.5,1,0)\n",
    "\n",
    "# What do we have in predictions\n",
    "print_info('On test %% of true duplicates is %.3f' % (target_test.mean()*100.))\n",
    "print_warning('On test %% of predicted duplicates is %.3f !!' % (test_predictions.mean()*100.))\n",
    "\n",
    "print_info('On train %% of true duplicates is %.3f' % (target_train.mean()*100.))\n",
    "print_warning('On train %% of predicted duplicates is %.3f !!' % (train_predictions.mean()*100.))\n",
    "\n",
    "print_warning('On challenge %% of predicted duplicates is %.3f !!' % (challenge_predictions.mean()*100.))\n",
    "\n",
    "plot.figure(figsize=(15,10))\n",
    "sns.distplot(test_probabilities,label='Probability', kde=True)\n",
    "plot.axvline(x=0.5,color='r',label='default threshold')\n",
    "plot.legend()\n",
    "plot.grid(True)\n",
    "plot.title(\"Distributions of probabilities predicted by xgboost\")\n",
    "plot_save('distribution_uncalibrated_probabilities_xgboost')"
   ]
  },
  {
   "source": [
    "What about the confusion matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def show_confusion_matrix(model,columns,input,target):\n",
    "    input_train,input_test,target_train,target_test = train_test_split(input,target,random_state=42,test_size=0.2)\n",
    "    input_train_weight = input_train['weight']\n",
    "    input_test_weight = input_test['weight']\n",
    "\n",
    "    test_probabilities = model.predict(xgboost.DMatrix(input_test[columns]),ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    test_predictions = numpy.where(test_probabilities>0.5,1,0)\n",
    "    # Last minute check\n",
    "    nb_0 = numpy.where(test_predictions==0,1,0).sum()\n",
    "    nb_1 = numpy.where(test_predictions==1,1,0).sum()\n",
    "    assert (nb_1+nb_0) == len(test_predictions)\n",
    "\n",
    "    print_info('Performances')\n",
    "    print(metrics.classification_report(target_test,test_predictions,sample_weight=input_test_weight))\n",
    "    print_warning('%% prediction of is_duplicate=1 in test !!! : %.3f' % (test_predictions.mean()*100.))\n",
    "    logloss_proba = metrics.log_loss(target_test,test_probabilities,sample_weight=input_test_weight,normalize=True)\n",
    "    print_warning('logloss in test %.3f' % logloss_proba)\n",
    "\n",
    "    # display cool graphs\n",
    "    #plot.figure(figsize=(10, 10))\n",
    "    #fig,ax = plot.subplots()\n",
    "    #uncalibrated_confusion_matrix = confusion_matrix(target_test,test_predictions,sample_weight=input_test_weight)\n",
    "    #ConfusionMatrixDisplay(uncalibrated_confusion_matrix).plot(cmap=plot.cm.Blues,values_format=\".0f\",ax=ax)\n",
    "    #plot.title('Default XGBoost on all features: Confusion Matrix with default threshold')\n",
    "    #plot_save('uncalibrated_xgboost_all_features_confusion_matrix1')\n",
    "\n",
    "    plot.figure(figsize=(10, 10))\n",
    "    fig,ax = plot.subplots()\n",
    "    uncalibrated_confusion_matrix = confusion_matrix(target_test,test_predictions,sample_weight=input_test_weight,normalize=\"all\")\n",
    "    ConfusionMatrixDisplay(uncalibrated_confusion_matrix).plot(cmap=plot.cm.Blues,ax=ax)\n",
    "    plot.title('Uncalibrated XGBoost on all features: Confusion Matrix with default threshold')\n",
    "    plot_save('uncalibrated_xgboost_all_features_confusion_matrix2')\n",
    "    \n",
    "\n",
    "\n",
    "show_confusion_matrix(model,ALL_FEATURES,train_dataframe,train_dataframe['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "plot.figure(figsize=(10,10))\n",
    "plot_test_truth, plot_test_predictions = calibration_curve(target_test, test_probabilities,strategy='quantile', n_bins=10)\n",
    "plot.plot(plot_test_predictions, plot_test_truth,label='test')\n",
    "# Useless to draw train curve : almost identical to test and therefore, invisible\n",
    "# plot_train_truth, plot_train_predictions = calibration_curve(target_train, train_probabilities,strategy='quantile', n_bins=10)\n",
    "# plot.plot(plot_train_predictions, plot_train_truth,label='train')\n",
    "\n",
    "xmin = min(plot_test_predictions.min(), plot_test_truth.min())\n",
    "xmax = max(plot_test_predictions.max(), plot_test_truth.max())\n",
    "#plot.plot([xmin,xmax],[xmin,xmax], '--',label='perfect')\n",
    "plot.plot([0, 1], [0, 1], '--', label='perfect')\n",
    "plot.xlabel('Fraction of positives (Predicted)')\n",
    "plot.ylabel('Fraction of positives (Actual)')\n",
    "plot.ylim([-0.05, 1.05])\n",
    "plot.legend() # loc='upper left', ncol=2)\n",
    "plot.title('Calibration Plots (Reliability Curve)')\n",
    "plot.grid(True)\n",
    "plot_save('xgboost_calibration_issue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.calibration import IsotonicRegression\n",
    "\n",
    "\n",
    "class MyPlattScaling(BaseEstimator):\n",
    "    def __init__(self, log_odds: bool=True):\n",
    "        self.log_odds = log_odds\n",
    "    \n",
    "    def fit(self, y_prob: numpy.ndarray, y_true: numpy.ndarray):\n",
    "        self.fit_predict(y_prob, y_true)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_log_odds(y_prob: numpy.ndarray):\n",
    "        eps = 1e-12\n",
    "        y_prob = numpy.clip(y_prob, eps, 1 - eps)\n",
    "        y_prob = numpy.log(y_prob / (1 - y_prob))\n",
    "        return y_prob\n",
    "\n",
    "    def predict(self, y_prob: numpy.ndarray):\n",
    "        if self.log_odds:\n",
    "            y_prob = self._convert_to_log_odds(y_prob)\n",
    "\n",
    "        output = self._transform(y_prob)\n",
    "        return output\n",
    "\n",
    "    def _transform(self, y_prob: numpy.ndarray):\n",
    "        output = y_prob * self.coef_[0] + self.intercept_\n",
    "        output = 1 / (1 + numpy.exp(-output))\n",
    "        return output\n",
    "\n",
    "    def fit_predict(self, y_prob: numpy.ndarray, y_true: numpy.ndarray) -> numpy.ndarray:\n",
    "        if self.log_odds:\n",
    "            y_prob = self._convert_to_log_odds(y_prob)\n",
    "\n",
    "        # the class expects 2d ndarray as input features\n",
    "        logistic = LogisticRegression(C=1e10, solver='lbfgs')\n",
    "        logistic.fit(y_prob.reshape(-1, 1), y_true)\n",
    "        self.coef_ = logistic.coef_[0]\n",
    "        self.intercept_ = logistic.intercept_\n",
    "\n",
    "        y_calibrated_prob = self._transform(y_prob)\n",
    "        return y_calibrated_prob\n",
    "\n",
    "\n",
    "isotonic = IsotonicRegression(out_of_bounds='clip',\n",
    "                              y_min=train_probabilities.min(),\n",
    "                              y_max=train_probabilities.max())\n",
    "# still valid: learn on train, predict on test\n",
    "isotonic.fit(train_probabilities,target_train)\n",
    "isotonic_test_probabilities = isotonic.predict(test_probabilities)\n",
    "isotonic_challenge_probabilities = isotonic.predict(challenge_probabilities)\n",
    "\n",
    "platt = MyPlattScaling(log_odds=False)\n",
    "# still valid: learn on train, predict on test\n",
    "platt.fit(train_probabilities,target_train)\n",
    "platt_test_probabilities = platt.predict(test_probabilities)\n",
    "#platt_train_probabilities = platt.predict(train_probabilities)\n",
    "platt_challenge_probabilities = platt.predict(challenge_probabilities)\n",
    "\n",
    "# fig,ax = plot.subplots()\n",
    "plot.figure(figsize=(10, 10))\n",
    "plot_test_truth, plot_test_predictions = calibration_curve(target_test, test_probabilities,strategy='quantile', n_bins=10)\n",
    "plot.plot(plot_test_predictions, plot_test_truth,label='test uncalibrated')\n",
    "\n",
    "plot_test_truth, plot_test_predictions = calibration_curve(target_test, isotonic_test_probabilities,strategy='quantile', n_bins=10)\n",
    "plot.plot(plot_test_predictions, plot_test_truth,label='test Isotonic Regression')\n",
    "\n",
    "plot_test_truth, plot_test_predictions = calibration_curve(target_test, platt_test_probabilities,strategy='quantile', n_bins=10)\n",
    "plot.plot(plot_test_predictions, plot_test_truth,label='test Platt Scaling')\n",
    "plot.plot([xmin,xmax],[xmin,xmax], '--',label='perfect')\n",
    "plot.xlabel('Fraction of positives (Predicted)')\n",
    "plot.ylabel('Fraction of positives (Actual)')\n",
    "plot.ylim([-0.05, 1.05])\n",
    "plot.legend(loc='upper left', ncol=2)\n",
    "plot.title('Calibration Plots (Reliability Curve)')\n",
    "plot_save('reliability_curve')\n",
    "\n",
    "isotonic_test_predictions = numpy.where(isotonic_test_probabilities>0.5,1,0)\n",
    "isotonic_challenge_predictions = numpy.where(isotonic_challenge_probabilities>0.5,1,0)\n",
    "print_warning('Now, on test we have %.3f %% of 1 using Isotonic\"s recalibrated probabilities' % (100.*isotonic_test_predictions.mean()))\n",
    "print_warning('And, on challenge we have %.3f %% of 1 using Isotonic\"s recalibrated probabilities' % (100.*isotonic_challenge_predictions.mean()))\n",
    "\n",
    "platt_test_predictions = numpy.where(platt_test_probabilities>0.5,1,0)\n",
    "platt_challenge_predictions = numpy.where(platt_challenge_probabilities>0.5,1,0)\n",
    "print_info('Now, we have %.3f %% of 1 using Platt scaling\"s recalibrated probabilities' % (100.*platt_test_predictions.mean()))\n",
    "print_info('And, on challenge we have %.3f %% of 1 using Platt scaling\"s recalibrated probabilities' % (100.*platt_challenge_predictions.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot.figure(figsize=(15,10))\n",
    " sns.distplot(test_probabilities,label='uncalibrated probability', kde=True)\n",
    " sns.distplot(platt_test_probabilities,label='calibrated (platt) probability', kde=True)\n",
    " #sns.distplot(isotonic_test_probabilities,label='calibrated (platt) probability', kde=True)\n",
    " plot.legend()\n",
    " plot.title(\"Distributions  of probabilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info('Test has %d rows' %len(target_test))\n",
    "\n",
    "print_warning('Uncalibrated')\n",
    "print_warning(\"logloss % .3f\" % metrics.log_loss(target_test,test_probabilities,sample_weight = input_test['weight']))\n",
    "print(metrics.classification_report(target_test,test_predictions,sample_weight = input_test['weight']))\n",
    "\n",
    "print_info('Calibrated')\n",
    "print_info(\"logloss % .3f\" % metrics.log_loss(target_test,platt_test_probabilities,sample_weight = input_test['weight']))\n",
    "print(metrics.classification_report(target_test,platt_test_predictions,sample_weight = input_test['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info('Calibrated with Plott Scaling')\n",
    "#calibrated_confusion_matrix = confusion_matrix(target_test,platt_test_predictions,sample_weight=input_test['weight'])\n",
    "#ConfusionMatrixDisplay(calibrated_confusion_matrix).plot(cmap=plot.cm.Blues,values_format=\".0f\")\n",
    "calibrated_confusion_matrix = confusion_matrix(target_test,platt_test_predictions,sample_weight=input_test['weight'],normalize=\"all\")\n",
    "ConfusionMatrixDisplay(calibrated_confusion_matrix).plot(cmap=plot.cm.Blues)\n",
    "plot.title('Calibrated XGBoost on all features: Confusion Matrix with default threshold')\n",
    "plot_save('calibrated_xgboost_all_features_confusion_matrix2')"
   ]
  },
  {
   "source": [
    "We have a model. Let's dig:\n",
    "\n",
    "* what are the important features\n",
    "* can we get rid of some features (hopefully the heavy ones)\n",
    "\n",
    "\n",
    "BE CAREFUL : except univariables, all that does not work ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def graph_relative_feature_importance(features_importances,columns):\n",
    "    # Make importances relative to max importance.\n",
    "    features_importances = 100.0 * (features_importances / features_importances.max())\n",
    "    sorted_idx = numpy.argsort(features_importances)\n",
    "    sorted_idx = sorted_idx[-20:-1:1]\n",
    "    pos = numpy.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plot.barh(pos, features_importances[sorted_idx], align='center')\n",
    "    plot.yticks(pos, columns)\n",
    "    plot.xlabel('Relative Importance')\n",
    "    plot.title('Relative Feature Importance', fontsize=30)\n",
    "    plot.tick_params(axis='x', which='major', labelsize=15)\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    plot.show()\n",
    "\n",
    "def graph_feature_importance(features_importances,columns):\n",
    "    # Make importances relative to max importance.\n",
    "    sorted_idx = numpy.argsort(features_importances)\n",
    "    sorted_idx = sorted_idx[-20:]\n",
    "    pos = numpy.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plot.barh(pos, features_importances[sorted_idx], align='center')\n",
    "    plot.yticks(pos, columns)\n",
    "    plot.xlabel('Importance')\n",
    "    plot.title('Feature Importance', fontsize=30)\n",
    "    plot.tick_params(axis='x', which='major', labelsize=15)\n",
    "    #sns.despine(left=True, bottom=True)\n",
    "    plot.show()\n",
    "\n",
    "def independant_features_analysis(name_data,input_df,target_df):\n",
    "    train_df = input_df[all_numeric_columns(input_df)]\n",
    "    target_df = target_df\n",
    "    small = int(len(target_df)*0.05)\n",
    "    small_train = train_df.sample(small,random_state=42)\n",
    "    small_target = target_df.sample(small,random_state=42)\n",
    "    small_weights = input_df[WEIGHT].sample(small,random_state=42)\n",
    "\n",
    "    # be careful all_numeric will include also the target soon\n",
    "    all_numeric = all_numeric_columns(input_df)\n",
    "    print_section('Minimal analysis of numeric features of %s' % name_data)\n",
    "    print_info('Nb features: %d' % len(all_numeric))\n",
    "    print_info(str(all_numeric))\n",
    "    print_warning('Is there any null value ? %s' % input_df.isnull().any().any())\n",
    "\n",
    "    compute_variances = feature_selection.VarianceThreshold()\n",
    "    all_numeric.append('is_duplicate')\n",
    "    variances = compute_variances.fit_transform(input_df[all_numeric])\n",
    "    print_warning('Is there any low variance feature ? %s' % str(variances.shape[1]!=len(all_numeric)))\n",
    "    print_info('Here are the correlations to the target \"is_duplicate\"')\n",
    "    correlation_matrix = input_df[all_numeric].corr().abs()\n",
    "    print(correlation_matrix['is_duplicate'].nlargest(len(all_numeric)))\n",
    "\n",
    "    correlation_matrix_without_is_duplicate = correlation_matrix['is_duplicate'][correlation_matrix['is_duplicate']<1]\n",
    "    highest_correlated_feature = correlation_matrix_without_is_duplicate.nlargest(1).index[0]\n",
    "    highest_correlation = correlation_matrix_without_is_duplicate.nlargest(1)[0]\n",
    "    if highest_correlation >0.2:\n",
    "        if highest_correlation < 0.5:\n",
    "            print_warning('%s is the most correlated with target but %f is quite weak' % (highest_correlated_feature,highest_correlation))\n",
    "        else:\n",
    "            print_info('%s is the most correlated with target but %f is very weak' % (highest_correlated_feature,highest_correlation))\n",
    "    else:\n",
    "        print_alert('%s is the most correlated with target and %f is quite big' % (highest_correlated_feature,highest_correlation))\n",
    "    plot.figure(figsize=(15, 10))\n",
    "    sns.heatmap(correlation_matrix,annot=False,cbar=True,square=True,cmap='coolwarm',mask = numpy.triu(correlation_matrix))\n",
    "    plot.show()\n",
    "\n",
    "    # we keep only numeric features and remove is_duplicate\n",
    "    all_numeric = all_numeric_columns(input_df)\n",
    "\n",
    "    print_section('Here are the features that would be selected with a simple univariate analysis')\n",
    "    start = time.time()\n",
    "    #for r in tqdm(range(1,len(all_numeric))):\n",
    "    for r in [103]:\n",
    "        k_best = SelectKBest(score_func=f_classif,k=r)\n",
    "        fit = k_best.fit(small_train,small_target)\n",
    "        best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "        print_info('Nb features to keep %d:%s' %(r,best_columns))\n",
    "        \n",
    "        # Can we graph relative importances\n",
    "        if r>1:\n",
    "            # transform train set so we can compute importances\n",
    "            univariate_features = fit.transform(small_train)\n",
    "            rfc = RandomForestClassifier(n_estimators=100)\n",
    "            rfc_scores = cross_val_score(rfc, univariate_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "            features_importance = rfc.fit(univariate_features, small_target).feature_importances_\n",
    "            graph_relative_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "    \n",
    "    \n",
    "\n",
    "    print_info('Try to keep x \\% of variance with a PCA')\n",
    "    print_alert('bug ?')\n",
    "    start = time.time()\n",
    "    for r in tqdm([.8,.9,.95]):\n",
    "        acp = PCA(r)\n",
    "        principal_components = acp.fit_transform(small_train)\n",
    "        principal_df = pandas.DataFrame(principal_components).sample(small,random_state=42)\n",
    "        rfc = RandomForestClassifier(n_estimators=100)\n",
    "        rfc_scores = cross_val_score(rfc, principal_df, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "        features_importance = rfc.fit(principal_df, small_target).feature_importances_\n",
    "        print_info('%% of variance %f:%s' %(r,'aie'))\n",
    "        graph_relative_feature_importance(features_importance,small_train.columns)\n",
    "    print_done('Done',top=start)\n",
    "\n",
    "    print_section('Here are the features that would be selected with a recursive feature elimination')\n",
    "    start = time.time()\n",
    "    print_alert('Doomed to fail : RFE does not support to transmit ,sample_weight=small_weights')\n",
    "    print_alert('ix is waiting since 2016 ?')\n",
    "    for r in tqdm(range(1,len(all_numeric))):\n",
    "        mnb = MultinomialNB()\n",
    "        recursive_best= feature_selection.RFE(mnb, n_features_to_select=r)\n",
    "        fit = recursive_best.fit(small_train,small_target)\n",
    "        best_columns = [small_train.columns[c] for c in fit.get_support(indices=True)]\n",
    "        print_info('Nb features to keep %d:%s' %(r,best_columns))\n",
    "        \n",
    "        # Can we graph relative importances\n",
    "        if r>1:\n",
    "            # transform train set so we can compute importances\n",
    "            recursive_features = fit.transform(small_train)\n",
    "            rfc = RandomForestClassifier(n_estimators=100)\n",
    "            rfc_scores = cross_val_score(rfc, recursive_features, small_target, cv=5, scoring='neg_log_loss',n_jobs=os.cpu_count())\n",
    "            features_importance = rfc.fit(recursive_features, small_target).feature_importances_\n",
    "            graph_relative_feature_importance(features_importance,best_columns)\n",
    "    print_done('Done',top=start)\n",
    "\n",
    "independant_features_analysis('train',train_dataframe,train_dataframe['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "#ax, height, xlim, ylim, title, xlabel, ylabel, fmap, importance_type, max_num_features, grid, show_values, **kwargs\n",
    "def show_topn_features(model,importance_type,top=10):\n",
    "    plot.figure(figsize=(15,10))\n",
    "    fig,ax = plot.subplots()\n",
    "    ax = xgboost.plot_importance(model,max_num_features=top,ax = ax ,title='%d top features by %s' % (top,importance_type),grid=True,show_values=False,importance_type=importance_type)\n",
    "    yticklabels = ax.get_yticklabels()[::-1]\n",
    "    if top == -1:\n",
    "        top = len(yticklabels)\n",
    "    else:\n",
    "        top = min(top, len(yticklabels))\n",
    "        print_section(\"importance by %s\" % importance_type)\n",
    "    features = [yticklabels[i].get_text() for i in range(top)]\n",
    "    print_info(str(features))\n",
    "    return features\n",
    "        \n",
    "#plot.figure(figsize=(30, 20))\n",
    "#xgboost.plot_importance(model,max_num_features=18)#\n",
    "#plot.show()\n",
    "\n",
    "top_by_weight = show_topn_features(model,importance_type='weight',top=20)\n",
    "top_by_gain = show_topn_features(model,importance_type='gain',top=20)\n",
    "features_to_keep_39 = list(set.union(set(top_by_weight),set(top_by_gain)))\n",
    "print_section('Top %d features to keep' % len(features_to_keep_39))\n",
    "print_info(str(features_to_keep_39))\n",
    "\n",
    "\n",
    "#  I don't understand what this is \n",
    "# show_topn_features(model,importance_type='cover',top=20)"
   ]
  },
  {
   "source": [
    "We have selected 39 variables that looked like important for xgboost\n",
    "What is happening if we recompute a model with only these variables ?\n",
    "\n",
    "**My decision** I keep the parameters from optimisation on all features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params= {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'colsample_bytree': 0.7000000000000001,\n",
    "    'eta': 0.05,\n",
    "    'gamma': 0.8500000000000001,\n",
    "    'max_depth': 11,\n",
    "    'min_child_weight': 3.0,\n",
    "    'n_estimators': 868.0,\n",
    "    'subsample': 0.9500000000000001}\n",
    "    \n",
    "    \n",
    "xgboost_39 = study_models_one_more_field_at_a_time(\n",
    "    '39 features all_preprocess optim',\n",
    "    '39_features_all_preprocess_optim',\n",
    "    features_to_keep_39,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore = False,\n",
    "    show = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()\n",
    "get_last_submissions(all_submissions,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_by_weight = show_topn_features(model,importance_type='weight',top=10)\n",
    "top_by_gain = show_topn_features(model,importance_type='gain',top=10)\n",
    "features_to_keep_20 = list(set.union(set(top_by_weight),set(top_by_gain)))\n",
    "print_section('Top %d features to keep' % len(features_to_keep_20))\n",
    "print_info(str(features_to_keep_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'colsample_bytree': 0.7000000000000001,\n",
    "    'eta': 0.05,\n",
    "    'gamma': 0.8500000000000001,\n",
    "    'max_depth': 11,\n",
    "    'min_child_weight': 3.0,\n",
    "    'n_estimators': 868.0,\n",
    "    'subsample': 0.9500000000000001}\n",
    "    \n",
    "    \n",
    "xgboost_20 = study_models_one_more_field_at_a_time(\n",
    "    '20 features all_preprocess optim',\n",
    "    '20_features_all_preprocess_optim',\n",
    "    features_to_keep_20,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore = False,\n",
    "    show = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()\n",
    "get_last_submissions(all_submissions,10)"
   ]
  },
  {
   "source": [
    "Strategy : semantic features only\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SEMANTIC_FEATURES=[c for c in all_numeric_columns(train_dataframe) if 'entities' in c or 'proba' in c or 'sim' in c]\n",
    "print(ALL_SEMANTIC_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_semantic = study_models_one_more_field_at_a_time(\n",
    "    'semantic features  optim',\n",
    "    'semantic_features_optim',\n",
    "    ALL_SEMANTIC_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore = False,\n",
    "    show = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()\n",
    "get_last_submissions(all_submissions,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_EASY_FEATURES=[c for c in all_numeric_columns(train_dataframe) if 'entities' not in c and 'entities' not in c and 'sim' not in c and 'lemm' not in c]\n",
    "print(ALL_EASY_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_easy = study_models_one_more_field_at_a_time(\n",
    "    'easy features  optim',\n",
    "    'easy_features_optim',\n",
    "    ALL_EASY_FEATURES,\n",
    "    train_dataframe,\n",
    "    challenge_dataframe,\n",
    "    explore = False,\n",
    "    show = False)"
   ]
  }
 ]
}