{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599769007457",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some setup and os tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'all_submissions' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9e3d6991e95f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mshow_last_submissions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubmissions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_submissions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_submissions' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import os\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sns\n",
    "from shutil import copyfile\n",
    "from IPython.display import IFrame, display, HTML\n",
    "%matplotlib inline\n",
    "\n",
    "# This is supposed to enhance default pandas display\n",
    "pandas.set_option('display.width',200)\n",
    "pandas.set_option('display.max_colwidth',200)\n",
    "\n",
    "# the folder where this experiment will be stored\n",
    "EXPERIMENT = 'multinomial_naivebayes_unbalanced'\n",
    "\n",
    "# The name of the docker image (used to display docker command to copy apply files to windows host)\n",
    "DOCKER_IMAGE_NAME = 'dev_ds_1'\n",
    "\n",
    "# Some common resources\n",
    "PANDAS_STORE = '../PandasStore'\n",
    "KAGGLE_EXE = '/root/anaconda3/bin/kaggle'\n",
    "PICKLE_EXTENSION = '.pkl'\n",
    "CLEAN_TRAINING_DATA = 'clean_training'\n",
    "CLEAN_CHALLENGE_DATA = 'clean_challenge'\n",
    "\n",
    "# Other useful constants\n",
    "ZIP_EXTENSION = '.zip'\n",
    "SEP_IN_FILE_NAME = '!'\n",
    "KAGGLE_GET_SUBMISSIONS_COMMAND = KAGGLE_EXE +' competitions submissions quora-question-pairs --csv'\n",
    "EXCEL_PRECISION='%.6f'\n",
    "\n",
    "# Try to have a minimal decoration in notebook output\n",
    "def print_html(s):\n",
    "    display(HTML(s, metadata=dict(isolated=True)))\n",
    "\n",
    "def start_small():\n",
    "    display(HTML('<span><small>', metadata=dict(isolated=True)))\n",
    "\n",
    "def print_small(s):\n",
    "    print_html('<small>',s,'</small>')\n",
    "\n",
    "def end_small():\n",
    "    display(HTML('</small></span>', metadata=dict(isolated=True)))\n",
    "\n",
    "def start_italic():\n",
    "    display(HTML('<i>', metadata=dict(isolated=True)))\n",
    "\n",
    "def print_italic(s):\n",
    "    print_html('<i>'+s+'</i>')\n",
    "\n",
    "def end_italic():\n",
    "    display(HTML('</i>', metadata=dict(isolated=True)))\n",
    "\n",
    "def start_bold():\n",
    "    display(HTML('<b>', metadata=dict(isolated=True)))\n",
    "\n",
    "def print_bold(s):\n",
    "    print_html('<b>'+s+'</b>')\n",
    "\n",
    "def end_bold():\n",
    "    display(HTML('</b>', metadata=dict(isolated=True)))\n",
    "\n",
    "def print_bullet(s):\n",
    "    print_html('<li>'+s)\n",
    "\n",
    "def print_section(s):\n",
    "    print_bold(s)\n",
    "    print_html('<HR>')\n",
    "\n",
    "def print_done(s):\n",
    "    print_html('<span style=\"color:LIMEGREEN\"><small><b><i>'+s+'</i></b><p></p></small></span>')\n",
    "\n",
    "def print_info(s):\n",
    "    print_html('<span style=\"color:LIMEGREEN\"><small>'+s+'</small></span>')\n",
    "\n",
    "def print_warning(s):\n",
    "    print_html('<span style=\"color:LIGHTSALMON\"><small>'+s+'</small></span>')\n",
    "\n",
    "## Plenty of small tools to help not creating a huge mess\n",
    "\n",
    "def global_pandas_store_file_name(file_name,ext='.pkl'):\n",
    "    return PANDAS_STORE +'/'+file_name+ext\n",
    "\n",
    "def local_pandas_store_file_name(file_name):\n",
    "    return env_file_name(file_name,PICKLE_EXTENSION)\n",
    "\n",
    "def env_path():\n",
    "    return '../'+EXPERIMENT\n",
    "\n",
    "def env_file_name(file_name,ext=''):\n",
    "    return env_path()+'/'+file_name+ext\n",
    "\n",
    "def absolute_env_file_name(file_name,ext=''):\n",
    "    return str(os.path.abspath(env_path()+'/'+file_name+ext))\n",
    "\n",
    "def copy_from_pandas_store_if_missing(file_name):\n",
    "    if not os.path.exists(env_file_name(file_name)):\n",
    "        print_info('Make local copy of %s' % global_pandas_store_file_name(file_name))\n",
    "        copyfile(global_pandas_store_file_name(file_name),local_pandas_store_file_name(file_name))\n",
    "\n",
    "def prepare_environnement():\n",
    "    print_section('Prepare %s environment' % EXPERIMENT)\n",
    "    if not os.path.exists(env_path()):\n",
    "        os.mkdir(env_path())\n",
    "    copy_from_pandas_store_if_missing(CLEAN_TRAINING_DATA)\n",
    "    copy_from_pandas_store_if_missing(CLEAN_CHALLENGE_DATA)\n",
    "    print_done('Done')\n",
    "    print\n",
    "\n",
    "## Tools to cache important and costly data\n",
    "\n",
    "def load_dataframe(file_name):\n",
    "    df = pandas.read_pickle(local_pandas_store_file_name(file_name))\n",
    "    return df\n",
    "\n",
    "def save_dataframe(df,file_name):\n",
    "    print_info('Save %s' % file_name )\n",
    "    df.to_pickle(local_pandas_store_file_name(file_name))\n",
    "\n",
    "def load_or_build_dataframe(dataframe_name,file_name,builder,dataframe):\n",
    "    print_section('%s: Load or rebuild %s' % (dataframe_name,file_name))\n",
    "    if os.path.exists(local_pandas_store_file_name(file_name)):\n",
    "        print_info(\"!!!!! %s is cached!!!\" % local_pandas_store_file_name(file_name))\n",
    "        df = load_dataframe(file_name)\n",
    "    else:\n",
    "        print_warning(\"!!!!! %s does not exists!!!\" % local_pandas_store_file_name(file_name))\n",
    "        print_warning('Rebuild and save it')\n",
    "        df = builder(dataframe)\n",
    "        save_dataframe(df,file_name)\n",
    "    print_done('Done:%s contains %d lines' % (file_name,len(df)))\n",
    "    return df\n",
    "\n",
    "# Code to generate all combination of numeric features\n",
    "from itertools import compress, product\n",
    "from itertools import chain, combinations\n",
    "\n",
    "def all_numeric_columns(dataframe):\n",
    "    l = list()\n",
    "    for name in dataframe.columns:\n",
    "        if dataframe.dtypes[name] in ['int64','float64'] and name not in ['test_id','id','qid1','qid2','is_duplicate']:\n",
    "            l.append( name)\n",
    "    return l\n",
    "\n",
    "def all_subsets(ss):\n",
    "    return list(chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1))))\n",
    "\n",
    "def clean_combination_name(c):\n",
    "    return str(numpy.asarray(c)).replace(\"' '\",\"+\").replace(\"['\",\"\").replace(\"']\",\"\").replace(\"'\",\"\").replace(\" \",\"\").replace('\\n','+')\n",
    "\n",
    "def clean_all_combination_names(dataframe):\n",
    "    for c in all_subsets(all_numeric_columns(dataframe)):\n",
    "        if len(c)>0:\n",
    "            print('|%s|'% clean_combination_name(c))\n",
    "\n",
    "#print( 'All numeric columns\\n%s' % all_numeric_columns(train_dataframe))\n",
    "#print('All combinations of numeric columns\\n %d combinations\\n' % len(all_subsets(all_numeric_columns(train_dataframe))))\n",
    "#for c in all_subsets(all_numeric_columns(train_dataframe)):\n",
    "#    print(c)\n",
    "#clean_all_combination_names(train_dataframe)\n",
    "#print( '%d combination of features will be studied' % (len(list(all_subsets(all_numeric_columns(train_dataframe))))-1))\n",
    "\n",
    "# code to put plots in a grid\n",
    "\n",
    "def multiplot_from_generator(g, num_columns, figsize_for_one_row=None):\n",
    "    # call 'next(g)' to get past the first 'yield'\n",
    "    next(g)\n",
    "    # default to 15-inch rows, with square subplots\n",
    "    if figsize_for_one_row is None:\n",
    "        figsize_for_one_row = (15, 15/num_columns)\n",
    "    try:\n",
    "        while True:\n",
    "            # call plt.figure once per row\n",
    "            plot.figure(figsize=figsize_for_one_row)\n",
    "            for col in range(num_columns):\n",
    "                ax = plot.subplot(1, num_columns, col+1)\n",
    "                next(g)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "# apply tools\n",
    "def apply_file_name(criteria,kind,model_key,ext='.csv'):\n",
    "    return env_file_name(SEP_IN_FILE_NAME.join((EXPERIMENT,criteria,kind,clean_combination_name(model_key).replace('/','_div_'))),ext=ext)\n",
    "\n",
    "def apply_absolute_file_name(criteria,kind,model_key,ext='.csv'):\n",
    "    return str(os.path.abspath(apply_file_name(criteria,kind,model_key,ext=ext)))\n",
    "\n",
    "# Zip tools\n",
    "from zipfile import ZipFile,ZIP_DEFLATED \n",
    "import zlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def zip_file_name(original_file_name):\n",
    "    return str(Path(original_file_name).with_suffix('.zip'))\n",
    "\n",
    "def zip_file_and_delete(original_file_name):\n",
    "    zip_name = zip_file_name(original_file_name) \n",
    "    zip = ZipFile(str(zip_name), 'w',compression=ZIP_DEFLATED)\n",
    "    zip.write(original_file_name)\n",
    "    zip.close()\n",
    "    os.unlink(original_file_name)\n",
    "    return zip_name\n",
    "\n",
    "# Excel tools\n",
    "def excel_file_name(file_name):\n",
    "    return env_file_name(file_name,ext='.xlsx')\n",
    "\n",
    "def pandas_to_excel(dataframe,file_name):\n",
    "    dataframe.to_excel(excel_file_name(SEP_IN_FILE_NAME.join([EXPERIMENT,file_name])),float_format=EXCEL_PRECISION)\n",
    "\n",
    "def save_models_dict_to_excel(models_dict,file_name='all_models'):\n",
    "    results = models_dict_to_models_results(models_dict)\n",
    "    results.to_excel(excel_file_name('_'.join([EXPERIMENT,file_name])),float_format=\"%.4f\")\n",
    "\n",
    "# Kaggle submissions tool\n",
    "import subprocess\n",
    "\n",
    "def load_kaggle_submissions():\n",
    "    print_section('Load all Kaggle submissions')\n",
    "    generic_submissions_name = EXPERIMENT+'_submissions'\n",
    "    file_name_csv = absolute_env_file_name(generic_submissions_name,ext='.csv')\n",
    "    file_name_excel = absolute_env_file_name(excel_file_name(generic_submissions_name))\n",
    "    csv_output = open(file_name_csv,\"w\")\n",
    "    proc = subprocess.Popen(KAGGLE_GET_SUBMISSIONS_COMMAND.split(),stdout=csv_output)\n",
    "    proc.wait()\n",
    "    csv_output.close()\n",
    "    print_info('All submissions are available in .csv&nbsp;&nbsp;format with %s' % file_name_csv)\n",
    "    submissions = pandas.read_csv(file_name_csv,error_bad_lines=True,warn_bad_lines=True)\n",
    "    # fix dataframe so it is more convenient to use\n",
    "    #submissions = submissions[submissions['status']=='complete' & submissions['description']!='first xgboost']\n",
    "    submissions = submissions[submissions['status']=='complete']\n",
    "    submissions = submissions[['date','publicScore','privateScore','description','fileName']]\n",
    "    submissions['date'] = pandas.to_datetime(submissions['date'])\n",
    "    submissions['publicScore'] = submissions['publicScore'].astype('float64')\n",
    "    submissions['privateScore'] = submissions['privateScore'].astype('float64')\n",
    "    submissions.to_excel(file_name_excel,float_format=\"%.4f\")\n",
    "    print_info('All submissions are available in .xlsx format with %s' % file_name_excel)\n",
    "    return submissions\n",
    "\n",
    "def show_last_submissions(top=3,submissions=all_submissions):\n",
    "    return submissions.nlargest(top,'date')\n",
    "\n",
    "def put_first(l,e):\n",
    "    l.insert(0,e)\n",
    "    return list(dict.fromkeys(l))\n",
    "\n",
    "\n",
    "def show_best_submissions(submissions=all_submissions,n=3,metric='publicScore',):\n",
    "    return submissions.nsmallest(n,metric)[put_first(submissions.columns.values.tolist(),metric)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_column_from_columns(dataframe,output_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe.apply(function,axis=1)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "def add_column_from_column(dataframe,output_column_name,input_column_name,function):\n",
    "    dataframe[output_column_name]=dataframe[input_column_name].apply(function)\n",
    "    return dataframe[output_column_name]\n",
    "\n",
    "\n",
    "def build_all_simple_features(dataframe):\n",
    "    start_small()\n",
    "    print_warning('Compute common_words between question1 & question2')\n",
    "    add_column_from_column(dataframe,'nb_words_question1','question1',lambda x: len(x.split()))\n",
    "    add_column_from_column(dataframe,'nb_words_question2','question2',lambda x: len(x.split()))\n",
    "    print_warning('Compute Nb common_words between question1 & question2')\n",
    "    add_column_from_columns(dataframe,'common_words',lambda r: list(set(r.question1.split())&set(r.question2.split())))\n",
    "    add_column_from_column(dataframe,'nb_common_words','common_words',len)\n",
    "\n",
    "    print_warning('Compute Nb common words/nb words in question1')\n",
    "    add_column_from_columns(dataframe,'nb_common_words/nb_words_question1',lambda r: r.nb_common_words/max(1,r.nb_words_question1))\n",
    "    print_warning('Compute Nb common words/nb words in question2')\n",
    "    add_column_from_columns(dataframe,'nb_common_words/nb_words_question2',lambda r: r.nb_common_words/max(1,r.nb_words_question2))\n",
    "\n",
    "    print_warning('Compute Nb words in question1,question2 not in common words')\n",
    "    add_column_from_columns(dataframe,'nb_words_question1-common_words',lambda r: len(list(set(r.question1.split())-set(r.common_words))))\n",
    "    add_column_from_columns(dataframe,'nb_words_question2-common_words',lambda r: len(list(set(r.question2.split())-set(r.common_words))))\n",
    "    print_warning('Compute (nb common words)/(nb words in question1+nb word in question2)')\n",
    "    add_column_from_columns(dataframe,'nb_common_words/(nb_words_question1+nb_words_question2)',lambda r: r.nb_common_words/(r.nb_words_question1+r.nb_words_question2))\n",
    "    end_small()\n",
    "    dataframe=dataframe.drop(columns='common_words')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps\n",
    "* Train & Challenge: Read original data (minimal low level cleaning already done)\n",
    "* Train & Challenge: Compute basic features\n",
    "    * Nb common words between question1 & question2\n",
    "    * Nb words in question1 not in common words\n",
    "    * Nb words in question2 not in common words\n",
    "    * Nb common words/(nb words in question1 + nb words in question2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Prepare multinomial_naivebayes_unbalanced environment</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Make local copy of ../PandasStore/clean_training.pkl</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small>Make local copy of ../PandasStore/clean_challenge.pkl</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIMEGREEN\"><small><b><i>Done</i></b><p></p></small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<b>Training basic features: Load or rebuild training_basic_features</b>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<HR>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>!!!!! ../multinomial_naivebayes_unbalanced/training_basic_features.pkl does not exists!!!</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Rebuild and save it</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span><small>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Compute common_words between question1 & question2</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Compute Nb common_words between question1 & question2</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:LIGHTSALMON\"><small>Compute Nb common words/nb words in question1</small></span>"
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     }
    },
    {
     "output_type": "error",
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cb545b65ce21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchallenge_dataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLEAN_CHALLENGE_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_dataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_or_build_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training basic features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'training_basic_features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuild_all_simple_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mchallenge_dataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_or_build_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Challenge basic features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'challenge_basic_features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuild_all_simple_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchallenge_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9e3d6991e95f>\u001b[0m in \u001b[0;36mload_or_build_dataframe\u001b[0;34m(dataframe_name, file_name, builder, dataframe)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!!!!! %s does not exists!!!\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlocal_pandas_store_file_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprint_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rebuild and save it'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0msave_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mprint_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done:%s contains %d lines'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-617a0825b055>\u001b[0m in \u001b[0;36mbuild_all_simple_features\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compute Nb common words/nb words in question1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0madd_column_from_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nb_common_words/nb_words_question1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_common_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_words_question1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compute Nb common words/nb words in question2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0madd_column_from_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nb_common_words/nb_words_question2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_common_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_words_question2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-617a0825b055>\u001b[0m in \u001b[0;36madd_column_from_columns\u001b[0;34m(dataframe, output_column_name, function)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_column_from_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_column_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_column_from_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_column_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_column_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-617a0825b055>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compute Nb common words/nb words in question1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0madd_column_from_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nb_common_words/nb_words_question1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_common_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_words_question1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compute Nb common words/nb words in question2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0madd_column_from_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nb_common_words/nb_words_question2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_common_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_words_question2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "prepare_environnement()\n",
    "train_dataframe=load_dataframe(CLEAN_TRAINING_DATA)\n",
    "challenge_dataframe=load_dataframe(CLEAN_CHALLENGE_DATA)\n",
    "\n",
    "train_dataframe=load_or_build_dataframe('Training basic features','training_basic_features',build_all_simple_features,train_dataframe)\n",
    "challenge_dataframe=load_or_build_dataframe('Challenge basic features','challenge_basic_features',build_all_simple_features,challenge_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some code to easily compute AUC of these basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_AUC(dataframe,column_name):\n",
    "    return roc_auc_score(dataframe['is_duplicate'],dataframe[column_name])\n",
    "\n",
    "def show_AUC(dataframe,column_name):\n",
    "    print_bullet('AUC %s-is_duplicate: %f' % (column_name,simple_AUC(dataframe,column_name)))\n",
    "\n",
    "def display_simple_AUC(dataframe,column_name):\n",
    "    palette = sns.color_palette()\n",
    "    # Let multiplot_generator figure the size\n",
    "    #plot.figure(figsize=(10, 7))\n",
    "    plot.hist(dataframe[column_name][dataframe['is_duplicate']==1],bins=50,color=palette[3],label='Same',histtype='step')\n",
    "    plot.hist(train_dataframe[column_name][dataframe['is_duplicate']==0],bins=50,color=palette[2],label='Different',alpha = 0.75,histtype='step')\n",
    "    plot.title('AUC is_duplicate/%s : %f' % (column_name,simple_AUC(dataframe,column_name)) , fontsize=10)\n",
    "    plot.xlabel(column_name)\n",
    "    plot.ylabel('Nb')\n",
    "    plot.legend()\n",
    "\n",
    "\n",
    "def show_all_simple_AUC(dataframe):\n",
    "    all =  all_numeric_columns(dataframe)\n",
    "    print_section( 'Show AUC on %d unique features' % len(all))\n",
    "    for name in all:\n",
    "        show_AUC(dataframe,name)\n",
    "        yield\n",
    "        display_simple_AUC(dataframe,name)\n",
    "    print_done('Done')\n",
    "\n",
    "\n",
    "def show_all_simple_AUC_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(show_all_simple_AUC(dataframe), nb_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a bit of information in our features ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_all_simple_AUC_in_grid(train_dataframe,nb_columns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to build simple naive bayes model on these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "def build_naivebayes_model_with_test(input,column_names,target,show=True):\n",
    "    # print_bullet('Multinomial Naive Bayes with test on %s' % clean_combination_name(column_names))\n",
    "    input_train = pandas.DataFrame()\n",
    "    for column_name in column_names:\n",
    "        input_train[column_name]=input[column_name]\n",
    "    target_train = target\n",
    "    input_train,input_test,target_train,target_test = train_test_split(input_train,target_train,random_state=42,test_size=0.2)\n",
    "    if show:\n",
    "        print_info( 'Validation %d Test %d' % (len(input_train),len(input_test)))\n",
    "    model = MultinomialNB()\n",
    "    #naive_bayes_classifier_with_test=ComplementNB()\n",
    "    model.fit(input_train,target_train)\n",
    "    prediction=model.predict(input_test)\n",
    "    prediction_proba=model.predict_proba(input_test)\n",
    "    res = metrics.classification_report(target_test,prediction,output_dict=True)\n",
    "    accuracy = res['accuracy']\n",
    "    score = res['0']['f1-score']*(1-0.17)+res['1']['f1-score']*.17\n",
    "    logloss_classif = metrics.log_loss(target_test,prediction)\n",
    "    logloss_proba = metrics.log_loss(target_test,prediction_proba)\n",
    "    if show:\n",
    "        print(metrics.classification_report(target_test,prediction))\n",
    "        print_info( 'Test logloss %.4f accuracy %.4f score %.4f %s' % (logloss_proba,accuracy,score,clean_combination_name(column_names)))\n",
    "    #print(metrics.accuracy_score(target_test,prediction))\n",
    "    #print(metrics.confusion_matrix(target_test,prediction))\n",
    "    return model,accuracy,score,logloss_classif,logloss_proba\n",
    "\n",
    "def build_naivebayes_model_full(input,column_names,target,show=True):\n",
    "    #print_bullet('Multinomial Naive Bayes full on %s' % clean_combination_name(column_names))\n",
    "    input_train = pandas.DataFrame()\n",
    "    for column_name in column_names:\n",
    "        input_train[column_name]=input[column_name]\n",
    "    target_train = target\n",
    "    model=MultinomialNB()\n",
    "    model.fit(input_train,target_train)\n",
    "    prediction=model.predict(input_train)\n",
    "    prediction_proba=model.predict_proba(input_train)\n",
    "    res = metrics.classification_report(target,prediction,output_dict=True)\n",
    "    accuracy = res['accuracy']\n",
    "    score = res['0']['f1-score']*(1-0.17)+res['1']['f1-score']*.17\n",
    "    logloss_classif = metrics.log_loss(target,prediction)\n",
    "    logloss_proba = metrics.log_loss(target,prediction_proba)\n",
    "    if show:\n",
    "        print_info( 'Full logloss %.4f accuracy %.4f score %.4f %s' % (logloss_proba,accuracy,score,clean_combination_name(column_names)))\n",
    "        print(metrics.classification_report(target,prediction))\n",
    "    return model,accuracy,score,logloss_classif,logloss_proba\n",
    "\n",
    "# just in case model_results has been lost in action\n",
    "def build_model_results(models_dict):\n",
    "    results = pandas.DataFrame(models_dict).transpose()\n",
    "    results = results.drop(columns=['model_test','model_full','column_names'])\n",
    "    results['accuracy_test'] = results['accuracy_test'].astype('float64')\n",
    "    results['score_test'] = results['score_test'].astype('float64')\n",
    "    results['accuracy_full'] = results['accuracy_full'].astype('float64')\n",
    "    results['score_full'] = results['score_full'].astype('float64')\n",
    "    results['logloss_classif_test'] = results['logloss_classif_test'].astype('float64')\n",
    "    results['logloss_proba_test'] = results['logloss_proba_test'].astype('float64')\n",
    "    results['logloss_classif_full'] = results['logloss_classif_full'].astype('float64')\n",
    "    results['logloss_proba_full'] = results['logloss_proba_full'].astype('float64')\n",
    "    return results\n",
    "\n",
    "# bad design choice : a DataFrame can be more convenient than a dict\n",
    "# But then, it is convenient to suppress all non numeric/string columns\n",
    "def models_dict_to_models_results(models_dict):\n",
    "    results = pandas.DataFrame(models_dict).transpose()\n",
    "    results = results.drop(columns=['model_test','model_full','column_names'])\n",
    "    results['accuracy_test'] = results['accuracy_test'].astype('float64')\n",
    "    results['score_test'] = results['score_test'].astype('float64')\n",
    "    results['accuracy_full'] = results['accuracy_full'].astype('float64')\n",
    "    results['score_full'] = results['score_full'].astype('float64')\n",
    "    results['logloss_classif_test'] = results['logloss_classif_test'].astype('float64')\n",
    "    results['logloss_proba_test'] = results['logloss_proba_test'].astype('float64')\n",
    "    results['logloss_classif_full'] = results['logloss_classif_full'].astype('float64')\n",
    "    results['logloss_proba_full'] = results['logloss_proba_full'].astype('float64')\n",
    "    return results\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def build_model(train_dataframe,column_names,target,show=True):\n",
    "    m_t,a_t,s_t,l_c_t,l_p_t = build_naivebayes_model_with_test(train_dataframe,column_names,target,show=show)\n",
    "    m_f,a_f,s_f,l_c_f,l_p_f = build_naivebayes_model_full(train_dataframe,column_names,target,show=show)\n",
    "    print_info( '%.4f %.4f|%.4f %.4f|%.4f %.4f|%s' % (l_p_t,l_p_f,a_t,a_f,s_t,s_f,clean_combination_name(column_names)))\n",
    "    return {'column_names':column_names,'model_test':m_t,'accuracy_test':a_t,'score_test':s_t,'logloss_classif_test':l_c_t,'logloss_proba_test':l_p_t,'model_full':m_f,'accuracy_full':a_f,'score_full':s_f,'logloss_classif_full':l_c_f,'logloss_proba_full':l_p_f}\n",
    "\n",
    "def build_model_on_all_subset_of_simple_features(dataframe,target):\n",
    "    print_section('%s : Build all models (with test+full) on every combination of simple features' % EXPERIMENT)\n",
    "    models_dict = dict()\n",
    "    print_info('logloss&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accuracy&nbsp;&nbsp&nbsp;&nbsp;&nbsp;&nbsp;score&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features')\n",
    "    print_info('test&nbsp;&nbsp;&nbsp;full&nbsp;&nbsp;|test&nbsp;&nbsp;&nbsp;full&nbsp;&nbsp;|test&nbsp;&nbsp;&nbsp;full&nbsp;&nbsp;|')\n",
    "    for c in tqdm(all_subsets(all_numeric_columns(dataframe))):\n",
    "        if (len(c)) >0:\n",
    "            models_dict[clean_combination_name(c)] = build_model(dataframe,c,target,show=False)\n",
    "    print_done('Done')\n",
    "    # Design mistake : need to convert dict to dataframe :(\n",
    "    return models_dict,models_dict_to_models_results(models_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# build one model \n",
    "model = build_model(train_dataframe,['nb_words_question1'],train_dataframe['is_duplicate'])\n",
    "# build ALL models\n",
    "# 2 very important global variables: all_models_dict & model_results\n",
    "all_models_dict,model_results = build_model_on_all_subset_of_simple_features(train_dataframe,train_dataframe['is_duplicate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code for an easy apply (display kaggle command for subscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppose global variable all_models_dict is available !\n",
    "\n",
    "# kind is 'test or 'full'\n",
    "def retrieve_model(model_key,kind):\n",
    "    combination_results = all_models_dict[model_key]\n",
    "    column_names = combination_results['column_names']\n",
    "    model = combination_results['model_'+kind]\n",
    "    return model,numpy.asarray(column_names)\n",
    "    \n",
    "# return a dataframe fully ready to be converted in csv and published into kaggle\n",
    "def simple_apply(model_key,input_dataframe,kind,proba=True):\n",
    "    model,column_names=retrieve_model(model_key,kind)\n",
    "    input_for_prediction=input_dataframe[column_names]\n",
    "    res = pandas.DataFrame()\n",
    "    if 'test_id' in input_dataframe.columns:\n",
    "        res['test_id']=input_dataframe['test_id']\n",
    "    if proba:\n",
    "        res['is_duplicate']=pandas.Series(model.predict_proba(input_for_prediction)[:,1],name='is_duplicate')\n",
    "    else:\n",
    "        res['is_duplicate']=pandas.Series(model.predict(input_for_prediction),name='is_duplicate')\n",
    "    return res\n",
    "\n",
    "\n",
    "def show_docker_cp_command(absolute_file_name):\n",
    "    return 'docker cp '+ DOCKER_IMAGE_NAME+':'+zip_file_name(absolute_file_name)+ ' c:\\\\temp\\\\outputs'\n",
    "\n",
    "def show_kaggle_command(absolute_file_name):\n",
    "    return 'kaggle competitions submit quora-question-pairs -f \"' + zip_file_name(absolute_file_name) +'\" -m \"' + absolute_file_name +'\"'\n",
    "\n",
    "def show_docker_cp_commands(best_results):\n",
    "    print_section('Use these commands to transfer apply results to windows host')\n",
    "    for c in best_results['file_name'].apply(show_docker_cp_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "\n",
    "def show_kaggle_commands(best_results):\n",
    "    print_section('Use these commands to submit apply results to kaggle')\n",
    "    for c in best_results['file_name'].apply(show_kaggle_command):\n",
    "        print_warning(c)\n",
    "    print_done(\"\")\n",
    "   \n",
    "\n",
    "def submit_model(criteria,kind,input_dataframe,model_key,proba=True,show_how_to_publish=True,kaggle=False):\n",
    "    absolute_file_name_csv = apply_absolute_file_name(criteria,kind,model_key)\n",
    "    print_info('Doing apply')\n",
    "    prediction = simple_apply(model_key,input_dataframe,kind,proba)\n",
    "    print_info('Generating CSV file')\n",
    "    prediction.to_csv(absolute_file_name_csv,index=False)\n",
    "    print_info('Zipping file')\n",
    "    absolute_file_name_zip = zip_file_and_delete(absolute_file_name_csv)\n",
    "    print_info('%s is ready' % absolute_file_name_csv)\n",
    "    if show_how_to_publish:\n",
    "        if kaggle:\n",
    "            print_warning('Use this commands to submit apply results to kaggle')\n",
    "            print_warning(show_kaggle_command(absolute_file_name_zip))\n",
    "        else:\n",
    "            print_warning('Use this command to transfer apply _results to Windows host')\n",
    "            print_warning(show_docker_cp_command(absolute_file_name_csv))\n",
    "\n",
    "# suppose global variable model_results is available\n",
    "def find_best_models(top,criteria,kind):\n",
    "    if 'logloss' in criteria:\n",
    "        return model_results.nsmallest(top,criteria+'_'+kind)\n",
    "    else:\n",
    "        return model_results.nlargest(top,criteria+'_'+kind)\n",
    "\n",
    "def submit_best_models(top,criteria,kind,input_dataframe,proba=True,kaggle=False):\n",
    "    print_section('Submit best %d %s models by %s' % (top,kind,criteria))\n",
    "    best_models = find_best_models(top,criteria,kind)\n",
    "    best_models['model_key']=numpy.asarray(best_models.index)\n",
    "    #best_models['file_name']=numpy.array([apply_file_name(criteria,kind,n) for n in best_models.index])\n",
    "    best_models['file_name']=best_models['model_key'].apply(lambda mk: apply_absolute_file_name(criteria,kind,mk))\n",
    "    best_models['model_key'].apply(lambda mk: submit_model(criteria,kind,input_dataframe,mk,proba=proba,show_how_to_publish=False,kaggle=kaggle))\n",
    "    best_models['docker']=best_models['file_name'].apply(show_docker_cp_command)\n",
    "    best_models['kaggle']=best_models['file_name'].apply(show_kaggle_command)\n",
    "    print_done('Done')\n",
    "    if kaggle:\n",
    "        show_kaggle_commands(best_models)\n",
    "    else:\n",
    "        show_docker_cp_commands(best_models)\n",
    "    return best_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_model('nb_common_words+nb_common_words/(nb_words_question1+nb_words_question2)','full')\n",
    "simple_apply('nb_common_words+nb_common_words/(nb_words_question1+nb_words_question2)',challenge_dataframe,'full')\n",
    "find_best_models(2,'logloss_proba','full')\n",
    "#submit_best_models('accuracy','full',train_dataframe)\n",
    "#submit_best_models('score','full',challenge_dataframe)\n",
    "# best_accuracy_full_models = submit_best_models(3,'accuracy','full',challenge_dataframe)\n",
    "# show_docker_cp_commands(best_accuracy_full_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best models for all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('%d models have been trained' %len(all_models_dict))\n",
    "print('\\nmax metrix\\n',model_results.idxmax())\n",
    "print('\\nmin metrix\\n',model_results.idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply full models with best logloss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#best_accuracy_full_models = submit_best_models(3,'accuracy','full',challenge_dataframe)\n",
    "#show_docker_cp_commands(best_accuracy_full_models)\n",
    "#best_score_full_models = submit_best_models(3,'score','full',challenge_dataframe)\n",
    "#show_docker_cp_commands(best_score_full_models)\n",
    "#best_accuracy_test_models = submit_best_models(3,'accuracy','test',challenge_dataframe)\n",
    "#show_docker_cp_commands(best_accuracy_test_models)\n",
    "#best_score_test_models = submit_best_models(3,'score','test',challenge_dataframe)\n",
    "#show_docker_cp_commands(best_score_test_models)\n",
    "best_logloss_proba_test = submit_best_models(3,'logloss_proba','test',challenge_dataframe,kaggle=True)\n",
    "best_logloss_proba_full = submit_best_models(3,'logloss_proba','full',challenge_dataframe,kaggle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_docker_cp_commands(best_logloss_proba_full)\n",
    "show_kaggle_commands(best_logloss_proba_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_model('score','full',challenge_dataframe,'nb_common_words/(nb_words_question1+nb_words_question2)',kaggle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def submit_all_bests(criterias,input_dataframe,kaggle=False):\n",
    "    kinds = ['full','test']\n",
    "    print_section('Submit best models by %sx%s' % (criterias,kinds))\n",
    "    all_file_names = list()\n",
    "    for c in tqdm(criterias):\n",
    "        for k in kinds:\n",
    "            print_info('Best %s_%s' %(c,k))\n",
    "            best_model = find_best_models(1,c,k)\n",
    "            generated_file = apply_absolute_file_name(c,k,best_model.index)\n",
    "            best_model['model_key']=numpy.asarray(best_model.index)\n",
    "            best_model['model_key'].apply(lambda mk: submit_model(c,k,input_dataframe,mk,proba=True,show_how_to_publish=False,kaggle=kaggle))\n",
    "            all_file_names.append(generated_file)\n",
    "    print_done('Done')\n",
    "    commands = pandas.DataFrame()\n",
    "    commands['file_name']=all_file_names\n",
    "    if kaggle:\n",
    "        commands['kaggle'] = commands['file_name'].apply(show_kaggle_command)\n",
    "        show_kaggle_commands(commands)\n",
    "    else:\n",
    "        commands['docker'] = commands['file_name'].apply(show_docker_cp_command)\n",
    "        show_docker_cp_commands(commands)\n",
    "    return commands\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "submit_all_bests(['accuracy','score','logloss_classif','logloss_proba'],challenge_dataframe,kaggle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_models_dict_to_excel(all_models_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_submissions = load_kaggle_submissions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_last_submissions(submissions=all_submissions)\n",
    "#show_best_submissions(all_submissions,metric='publicScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_best_submissions(metric='privateScore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_submissions(dataframe):\n",
    "    print_section( 'Show history of submissions')\n",
    "    for name in ['publicScore']: #all_numeric_columns(dataframe):\n",
    "        yield\n",
    "        display_submission(dataframe)\n",
    "    print_done('Done')\n",
    "\n",
    "def display_submissions_in_grid(dataframe,nb_columns=2):\n",
    "    multiplot_from_generator(display_submissions(dataframe), nb_columns)\n",
    "\n",
    "def auto_label(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        plot.annotate('{}'.format(height),xy=(rect.get_x()+rect.get_width()/2,height),xytext=(0,3),textcoords='offset points',ha='center',va='bottom')\n",
    "\n",
    "\n",
    "# Only useful to show private/public is almost the same\n",
    "# at least now\n",
    "def display_submission_private_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    labels = dataframe['description']\n",
    "    public_scores = dataframe['publicScore']\n",
    "    private_scores = dataframe['privateScore']\n",
    "    x = numpy.arange(len(labels))\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(20, 14))\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    rects_public_score = plot.bar(x-width/2,public_scores,width,label = 'publicScore')\n",
    "    rects_private_score = plot.bar(x+width/2,private_scores,width,label = 'privateScore')\n",
    "    plot.ylabel('Scores')\n",
    "    plot.title('History of scores')\n",
    "    plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.legend()\n",
    "    #auto_label(rects_public_score)\n",
    "    fig.tight_layout()\n",
    "    plot.show()\n",
    "\n",
    "def find_description_test_and_full(dataframe):\n",
    "    list_test = list()\n",
    "    list_full = list()\n",
    "    for i,d in dataframe.iterrows():\n",
    "        #print(d)\n",
    "        if d.description.find('!test!')>=0:\n",
    "            list_test.append(d.description.replace('!test!','!x!'))\n",
    "        if d.description.find('!full!')>=0:\n",
    "            list_full.append(d.description.replace('!full!','!x!'))\n",
    "    list_test_and_full = list(set(list_test)&set(list_full))\n",
    "    list_test = [i.replace('!x!','test') for i in list_test_and_full]\n",
    "    list_full = [i.replace('!x!','full') for i in list_test_and_full]\n",
    "    return [i.replace('!x!','!test!') for i in list_test_and_full]+[i.replace('!x!','!full!') for i in list_test_and_full]\n",
    "    \n",
    "        \n",
    "\n",
    "def display_submission_full_test_public(dataframe):\n",
    "    #dataframe = dataframe[dataframe['publicScore']>0.4][dataframe['publicScore']<1]\n",
    "    dataframe = dataframe.sort_values('date')\n",
    "    test_and_full = find_description_test_and_full(dataframe)\n",
    "    test_and_full_scores = dataframe[dataframe['description'].isin(test_and_full)]\n",
    "    test_scores = test_and_full_scores[test_and_full_scores['description'].str.find('!test!')>=0]['publicScore']\n",
    "    full_scores = test_and_full_scores[test_and_full_scores['description'].str.find('!full!')>=0]['publicScore']\n",
    "    width = 0.35\n",
    "    fig = plot.figure(figsize=(15, 10))\n",
    "    #ax = plot\n",
    "    # fig,ax = plot.subplots()\n",
    "    x = numpy.arange(len(test_scores))\n",
    "    rects_test_score = plot.bar(x-width/2,test_scores,width,label = 'Score on test')\n",
    "    x = numpy.arange(len(full_scores))\n",
    "    rects_full_score = plot.bar(x+width/2,full_scores,width,label = 'Score on full')\n",
    "    plot.ylabel('Scores')\n",
    "    plot.title('History of scores')\n",
    "    plot.xticks(x)\n",
    "    #ax.set_xticklabels(labels)\n",
    "    plot.legend()\n",
    "    #auto_label(rects_public_score)\n",
    "    fig.tight_layout()\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "\n",
    "#all_submissions['description']\n",
    "res = find_description_test_and_full(all_submissions)\n",
    "display_submission_private_public(all_submissions)\n",
    "#display_submission_private_public(all_submissions)\n"
   ]
  }
 ]
}